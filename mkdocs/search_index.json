{
    "docs": [
        {
            "location": "/", 
            "text": "DataHub Documentation\n\n\nWelcome to the DataHub documentation. Choose the appropriate section and dive right in!\n\n\nSections\n\n\n\n\nFor developers\n: \n3 Python, JavaScript and data pipelines? Start here!\n\n\nFor publishers\n: Want to store your data on DataHub? Start here!", 
            "title": "Home"
        }, 
        {
            "location": "/#datahub-documentation", 
            "text": "Welcome to the DataHub documentation. Choose the appropriate section and dive right in!", 
            "title": "DataHub Documentation"
        }, 
        {
            "location": "/#sections", 
            "text": "For developers :  3 Python, JavaScript and data pipelines? Start here!  For publishers : Want to store your data on DataHub? Start here!", 
            "title": "Sections"
        }, 
        {
            "location": "/developers/", 
            "text": "Developers\n\n\nThis section of the DataHub documentation is for developers. Here you can learn about the design of the platform and how to get DataHub running locally or on your own servers, and the process for contributing enhancements and bug fixes to the code.\n\n\n\n\nWe use following GitHub repositories for DataHub platform:\n\n\n\n\nDEPLOY\n - Automated deployment\n\n\nFRONTEND\n - Frontend application in node.js\n\n\nASSEMBLER\n - Data assembly line\n\n\nAUTH\n - A generic OAuth2 authentication service and user permission manager.\n\n\nSPECSTORE\n - API server for managing a Source Spec Registry\n\n\nBITSTORE\n - A microservice for storing blobs i.e. files.\n\n\nDOCS\n - Documentations\n\n\n\n\n\ngraph TD\n\nsubgraph Repos\n  frontend[Frontend]\n  assembler[Assembler]\n  auth[Auth]\n  specstore[Specstore]\n  bitstore[Bitstore]\n  docs[Docs]\nend\n\nsubgraph Sites\n  dhio[datahub.io]\n  dhdocs[docs.datahub.io]\n  docs --> dhdocs\nend\n\ndeploy((DEPLOY))\ndeploy --> dhio\nfrontend --> deploy\nassembler --> deploy\nauth --> deploy\nspecstore --> deploy\nbitstore --> deploy\n\n\n\n\n\nInstall\n\n\nWe use several different services to run our platform, please follow the installation instructions here:\n\n\n\n\n\n\nInstall Assembler\n\n\n\n\n\n\nInstall Auth\n\n\n\n\n\n\nInstall Specstore\n\n\n\n\n\n\nInstall Bitstore\n\n\n\n\n\n\nInstall DataHub-CLI\n\n\n\n\n\n\nDeploy\n\n\nFor deployment of the application in a production environment, please see \nthe deploy page\n.\n\n\nAuthorization\n\n\nThe authorization set up enables system to restricts user permission to execute.\n\n\nAuthorization docs\n\n\nAuthentication\n\n\nSome DataHub API methods require client to provide user identity. API Client can use JWT token to perform authenticated requests.\n\n\nAuthentication docs\n\n\nDataHub CLI\n\n\nThe DataHub CLI is a Node JS lib and command line interface to interact with an DataHub instance.\n\n\nCLI code", 
            "title": "Getting started"
        }, 
        {
            "location": "/developers/#developers", 
            "text": "This section of the DataHub documentation is for developers. Here you can learn about the design of the platform and how to get DataHub running locally or on your own servers, and the process for contributing enhancements and bug fixes to the code.   We use following GitHub repositories for DataHub platform:   DEPLOY  - Automated deployment  FRONTEND  - Frontend application in node.js  ASSEMBLER  - Data assembly line  AUTH  - A generic OAuth2 authentication service and user permission manager.  SPECSTORE  - API server for managing a Source Spec Registry  BITSTORE  - A microservice for storing blobs i.e. files.  DOCS  - Documentations   \ngraph TD\n\nsubgraph Repos\n  frontend[Frontend]\n  assembler[Assembler]\n  auth[Auth]\n  specstore[Specstore]\n  bitstore[Bitstore]\n  docs[Docs]\nend\n\nsubgraph Sites\n  dhio[datahub.io]\n  dhdocs[docs.datahub.io]\n  docs --> dhdocs\nend\n\ndeploy((DEPLOY))\ndeploy --> dhio\nfrontend --> deploy\nassembler --> deploy\nauth --> deploy\nspecstore --> deploy\nbitstore --> deploy", 
            "title": "Developers"
        }, 
        {
            "location": "/developers/#install", 
            "text": "We use several different services to run our platform, please follow the installation instructions here:    Install Assembler    Install Auth    Install Specstore    Install Bitstore    Install DataHub-CLI", 
            "title": "Install"
        }, 
        {
            "location": "/developers/#deploy", 
            "text": "For deployment of the application in a production environment, please see  the deploy page .", 
            "title": "Deploy"
        }, 
        {
            "location": "/developers/#authorization", 
            "text": "The authorization set up enables system to restricts user permission to execute.  Authorization docs", 
            "title": "Authorization"
        }, 
        {
            "location": "/developers/#authentication", 
            "text": "Some DataHub API methods require client to provide user identity. API Client can use JWT token to perform authenticated requests.  Authentication docs", 
            "title": "Authentication"
        }, 
        {
            "location": "/developers/#datahub-cli", 
            "text": "The DataHub CLI is a Node JS lib and command line interface to interact with an DataHub instance.  CLI code", 
            "title": "DataHub CLI"
        }, 
        {
            "location": "/developers/platform/", 
            "text": "Platform\n\n\nThe DataHub platform has been designed as a set of loosely coupled components, each performing distinct functions related to the platform as a whole.\n\n\n\n\nArchitecture\n\n\nDomain Model\n\n\nProfile\n\n\nPackage\n\n\n\n\n\n\n\n\nArchitecture\n\n\n\n\ngraph TD\n\n\n  cli((CLI fa:fa-user))\n  auth[Auth Service]\n  cli --login--> auth\n\n\n    cli --store--> raw[Raw Store API\n+ Storage]  \n\n    cli --package-info--> pipeline-store\n  raw --data resource--> pipeline-runner\n\n  pipeline-store -.generate.-> pipeline-runner\n\n  pipeline-runner --> package[Package Storage]\n    package --api--> frontend[Frontend]\n  frontend --> user[User fa:fa-user]\n\n\n\n  package -.publish.->metastore[MetaStore]\n  pipeline-store -.publish.-> metastore[MetaStore]\n  metastore[MetaStore] --api--> frontend\n\n\n\n\n\n\n\nDataHub-CLI\n - Command Line Interface for publishing \nData Packages\n\n\nFront-end Web Application\n - Core part of platform - API, Login \n Sign-Up and Browse \n Search (page not yet implemented)\n\n\nViews and Renderer\n - JS Library responsible for visualization and views on platform\n\n\n\n\nRaw Storage\n\n\nWe first save all raw files before sending to pipeline-runner.\n\nPipeline-runner\n is a service that runs the data package pipelines. It is used to normalise and modify the data before it is displayed publicly\n\n\n\n\nWe use AWS S3 instance for storing data\n\n\n\n\nPackage Storage\n\n\nWe store files after passing pipeline-runner\n\n\n\n\nWe use AWS S3 instance for storing data\n\n\n\n\nBitStore\n\n\nWe are preserving the data byte by byte.\n\n\n\n\nWe use AWS S3 instance for storing data\n\n\n\n\nMetaStore\n\n\nThe MetaStore stores Data Package meta-data along with other management information like publishers, users and permissions.\n\n\nWe use AWS RDS Postgresql database for storing meta-data.\n\n\nUsers and Permissions\n\n\n\n\nWe are using GitHub auth API for authenticating users on our platform. See more information on \nauthentication page\n\n\nWe have a standard access control matrix with 3 axes for authorization. See more information on\n\nauthorization page\n\n\n\n\nDomain model\n\n\nThere are two main concepts to understand in DataHub domain model - \nProfile\n and \nPackage\n\n\n\ngraph TD\n\npkg[Data Package]\nresource[Resource]\nfile[File]\nversion[Version]\nuser[User]\npublisher[Publisher]\n\nsubgraph Package\n  pkg --0..*--> resource\n  resource --1..*--> file\n  pkg --> version\nend\n\nsubgraph Profile\n  publisher --1..*--> user\n  publisher --0..*--> pkg\nend\n\n\n\n\nProfile\n\n\nSet of an authenticated and authorized entities like publishers and users. They are responsible for publishing, deleting or maintaining data on platform.\n\n\nImportant:\n Users do not have Data Packages, Publishers do. Users are \nmembers\n of Publishers.\n\n\nPublisher\n\n\nPublisher is an organization which \"owns\" Data Packages. Publisher may have zero or more Data Packages. Publisher may also have one or more user.\n\n\nUser\n\n\nUser is an authenticated entity, that is member of Publisher organization, that can read, edit, create or delete data packages depending on their permissions.\n\n\nPackage\n\n\nSet of Data Packages published under publisher name.\n\n\nData Package\n\n\nA Data Package is a simple way of \u201cpackaging\u201d up and describing data so that it can be easily shared and used. You can imagine as collection of data and and it's meta-data (\ndatapackage.json\n), usually covering some concrete topic Eg: \n\"Gold Prices\"\n or \n\"Population Growth Rate In My country\"\n etc.\n\n\nEach Data Package may have zero or more resources and one or more versions.\n\n\nResources\n - think like \"tables\" - Each can map to one or more physical files (usually just one). Think of a data table split into multiple CSV files on disk.\n\n\nVersion of a Data Package\n - similar to git commits and tags. People can mean different things by a \"Version\":\n\n\n\n\nTag - Same as label or version - a nice human usable label e.g. \n\"v0.3\"\n, \n\"master\"\n, \n\"2013\"\n\n\nCommit/Hash - Corresponds to the hash of datapackage.json, with that datapackage.json including all hashes of all data files\n\n\n\n\nWe interpret Version as \n\"Tag\"\n concept. \n\"Commit/Hash\"\n is not supported", 
            "title": "Platform"
        }, 
        {
            "location": "/developers/platform/#platform", 
            "text": "The DataHub platform has been designed as a set of loosely coupled components, each performing distinct functions related to the platform as a whole.   Architecture  Domain Model  Profile  Package", 
            "title": "Platform"
        }, 
        {
            "location": "/developers/platform/#architecture", 
            "text": "graph TD\n\n\n  cli((CLI fa:fa-user))\n  auth[Auth Service]\n  cli --login--> auth\n\n\n    cli --store--> raw[Raw Store API + Storage]  \n\n    cli --package-info--> pipeline-store\n  raw --data resource--> pipeline-runner\n\n  pipeline-store -.generate.-> pipeline-runner\n\n  pipeline-runner --> package[Package Storage]\n    package --api--> frontend[Frontend]\n  frontend --> user[User fa:fa-user]\n\n\n\n  package -.publish.->metastore[MetaStore]\n  pipeline-store -.publish.-> metastore[MetaStore]\n  metastore[MetaStore] --api--> frontend   DataHub-CLI  - Command Line Interface for publishing  Data Packages  Front-end Web Application  - Core part of platform - API, Login   Sign-Up and Browse   Search (page not yet implemented)  Views and Renderer  - JS Library responsible for visualization and views on platform", 
            "title": "Architecture"
        }, 
        {
            "location": "/developers/platform/#raw-storage", 
            "text": "We first save all raw files before sending to pipeline-runner. Pipeline-runner  is a service that runs the data package pipelines. It is used to normalise and modify the data before it is displayed publicly   We use AWS S3 instance for storing data", 
            "title": "Raw Storage"
        }, 
        {
            "location": "/developers/platform/#package-storage", 
            "text": "We store files after passing pipeline-runner   We use AWS S3 instance for storing data", 
            "title": "Package Storage"
        }, 
        {
            "location": "/developers/platform/#bitstore", 
            "text": "We are preserving the data byte by byte.   We use AWS S3 instance for storing data", 
            "title": "BitStore"
        }, 
        {
            "location": "/developers/platform/#metastore", 
            "text": "The MetaStore stores Data Package meta-data along with other management information like publishers, users and permissions.  We use AWS RDS Postgresql database for storing meta-data.", 
            "title": "MetaStore"
        }, 
        {
            "location": "/developers/platform/#users-and-permissions", 
            "text": "We are using GitHub auth API for authenticating users on our platform. See more information on  authentication page  We have a standard access control matrix with 3 axes for authorization. See more information on authorization page", 
            "title": "Users and Permissions"
        }, 
        {
            "location": "/developers/platform/#domain-model", 
            "text": "There are two main concepts to understand in DataHub domain model -  Profile  and  Package  \ngraph TD\n\npkg[Data Package]\nresource[Resource]\nfile[File]\nversion[Version]\nuser[User]\npublisher[Publisher]\n\nsubgraph Package\n  pkg --0..*--> resource\n  resource --1..*--> file\n  pkg --> version\nend\n\nsubgraph Profile\n  publisher --1..*--> user\n  publisher --0..*--> pkg\nend", 
            "title": "Domain model"
        }, 
        {
            "location": "/developers/platform/#profile", 
            "text": "Set of an authenticated and authorized entities like publishers and users. They are responsible for publishing, deleting or maintaining data on platform.  Important:  Users do not have Data Packages, Publishers do. Users are  members  of Publishers.", 
            "title": "Profile"
        }, 
        {
            "location": "/developers/platform/#publisher", 
            "text": "Publisher is an organization which \"owns\" Data Packages. Publisher may have zero or more Data Packages. Publisher may also have one or more user.", 
            "title": "Publisher"
        }, 
        {
            "location": "/developers/platform/#user", 
            "text": "User is an authenticated entity, that is member of Publisher organization, that can read, edit, create or delete data packages depending on their permissions.", 
            "title": "User"
        }, 
        {
            "location": "/developers/platform/#package", 
            "text": "Set of Data Packages published under publisher name.", 
            "title": "Package"
        }, 
        {
            "location": "/developers/platform/#data-package", 
            "text": "A Data Package is a simple way of \u201cpackaging\u201d up and describing data so that it can be easily shared and used. You can imagine as collection of data and and it's meta-data ( datapackage.json ), usually covering some concrete topic Eg:  \"Gold Prices\"  or  \"Population Growth Rate In My country\"  etc.  Each Data Package may have zero or more resources and one or more versions.  Resources  - think like \"tables\" - Each can map to one or more physical files (usually just one). Think of a data table split into multiple CSV files on disk.  Version of a Data Package  - similar to git commits and tags. People can mean different things by a \"Version\":   Tag - Same as label or version - a nice human usable label e.g.  \"v0.3\" ,  \"master\" ,  \"2013\"  Commit/Hash - Corresponds to the hash of datapackage.json, with that datapackage.json including all hashes of all data files   We interpret Version as  \"Tag\"  concept.  \"Commit/Hash\"  is not supported", 
            "title": "Data Package"
        }, 
        {
            "location": "/developers/deploy/", 
            "text": "DevOps - Production Deployment\n\n\nWe use various cloud services for the platform, for example AWS S3 for storing data and metadata, and the application runs on Docker Cloud.\n\n\nWe have fully automated the deployment of the platform including the setup of all necessary services so that it is one command to deploy. Code and instructions here:\n\n\nhttps://github.com/datahq/deploy\n\n\nBelow we provide a conceptual outline.\n\n\nOutline - Conceptually\n\n\n\ngraph TD\n\n  user[fa:fa-user User] --> frontend[Frontend]\n  frontend --> apiproxy[API Proxy]\n  frontend --> bits[BitStore - S3]\n\n\n\n\nNew Structure\n\n\nThis diagram shows the current deployment architecture.\n\n\n\ngraph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> docker\n  cloudflare --> bits\n  docker[Docker - Node JS Application] --> apiproxy[API Proxy]\n  docker --> bits\n\n\n\n\nOld Structures\n\n\nHeroku\n\n\n\ngraph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> heroku\n  cloudflare --> bits\n  heroku[Heroku - Flask] --> rds[RDS Database]\n  heroku --> bits\n\n\n\n\nAWS Lambda - Flask via Zappa\n\n\nWe are no longer using AWS and Heroku in this way. However, we have kept this for historical purposes and in case we return to any of them.\n\n\n\ngraph TD\n\n  user[fa:fa-user User] --> cloudfront[Cloudfront]\n  cloudfront --> apigateway[API Gateway]\n  apigateway --> lambda[AWS Lambda - Flask via Zappa]\n  cloudfront --> s3assets[S3 Assets]\n  lambda --> rds[RDS Database]\n  lambda --> bits[BitStore]\n  cloudfront --> bits", 
            "title": "Deploy"
        }, 
        {
            "location": "/developers/deploy/#devops-production-deployment", 
            "text": "We use various cloud services for the platform, for example AWS S3 for storing data and metadata, and the application runs on Docker Cloud.  We have fully automated the deployment of the platform including the setup of all necessary services so that it is one command to deploy. Code and instructions here:  https://github.com/datahq/deploy  Below we provide a conceptual outline.", 
            "title": "DevOps - Production Deployment"
        }, 
        {
            "location": "/developers/deploy/#outline-conceptually", 
            "text": "graph TD\n\n  user[fa:fa-user User] --> frontend[Frontend]\n  frontend --> apiproxy[API Proxy]\n  frontend --> bits[BitStore - S3]", 
            "title": "Outline - Conceptually"
        }, 
        {
            "location": "/developers/deploy/#new-structure", 
            "text": "This diagram shows the current deployment architecture.  \ngraph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> docker\n  cloudflare --> bits\n  docker[Docker - Node JS Application] --> apiproxy[API Proxy]\n  docker --> bits", 
            "title": "New Structure"
        }, 
        {
            "location": "/developers/deploy/#old-structures", 
            "text": "", 
            "title": "Old Structures"
        }, 
        {
            "location": "/developers/deploy/#heroku", 
            "text": "graph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> heroku\n  cloudflare --> bits\n  heroku[Heroku - Flask] --> rds[RDS Database]\n  heroku --> bits", 
            "title": "Heroku"
        }, 
        {
            "location": "/developers/deploy/#aws-lambda-flask-via-zappa", 
            "text": "We are no longer using AWS and Heroku in this way. However, we have kept this for historical purposes and in case we return to any of them.  \ngraph TD\n\n  user[fa:fa-user User] --> cloudfront[Cloudfront]\n  cloudfront --> apigateway[API Gateway]\n  apigateway --> lambda[AWS Lambda - Flask via Zappa]\n  cloudfront --> s3assets[S3 Assets]\n  lambda --> rds[RDS Database]\n  lambda --> bits[BitStore]\n  cloudfront --> bits", 
            "title": "AWS Lambda - Flask via Zappa"
        }, 
        {
            "location": "/developers/api/", 
            "text": "DataHub API\n\n\nThe DataHub API provides a range of endpoints to interact with the platform. All endpoints live under the URL \nhttps://api.datahub.io\n where our API is divided into the following sections: \nauth, rawstore, sources\n \n\n\nAuth\n\n\nA generic OAuth2 authentication service and user permission manager. \n\n\nCheck an authentication token's validity\n\n\n/auth/check\n\n\nMethod:\n \nGET\n\n\nQuery Parameters:\n\n\n\n\njwt\n - authentication token\n\n\nnext\n - URL to redirect to when finished authentication\n\n\n\n\nReturns:\n\n\nIf authenticated:\n\n\n{\n    \nauthenticated\n: true,\n    \nprofile\n: {\n        \nid\n: \nuser-id\n,\n        \nname\n: \nuser-name\n,\n        \nemail\n: \nuser-email\n,\n        \navatar_url\n: \nurl-for-user's-profile-photo\n,\n        \nidhash\n: \nunique-id-of-the-user\n,\n        \nusername\n: \nuser-selected-id\n // If user has a username\n    }\n}\n\n\n\nIf not:\n\n\n{\n    \nauthenticated\n: false,\n    \nproviders\n: {\n        \ngoogle\n: {\n            \nurl\n: \nurl-for-logging-in-with-the-Google-provider\n\n        },\n        \ngithub\n: {\n            \nurl\n: \nurl-for-logging-in-with-the-Github-provider\n\n        },\n    }\n}\n\n\n\nWhen the authentication flow is finished, the caller will be redirected to the \nnext\n URL with an extra query parameter\n\njwt\n which contains the authentication token. The caller should cache this token for further interactions with the API.\n\n\nGet permission for a service\n\n\n/user/authorize\n\n\nMethod:\n \nGET\n\n\nQuery Parameters:\n\n\n\n\njwt\n - user token (received from \n/user/check\n)\n\n\nservice\n - the relevant service (e.g. \nstorage-service\n)\n\n\n\n\nReturns:\n\n\n{\n    \ntoken\n: \ntoken-for-the-relevant-service\n\n    \nuserid\n: \nunique-id-of-the-user\n,\n    \npermissions\n: {\n        \npermission-x\n: true,\n        \npermission-y\n: false\n    },\n    \nservice\n: \nrelevant-service\n\n}\n\n\n\nChange the username\n\n\n/user/update\n\n\nMethod:\n \nPOST\n\n\nQuery Parameters:\n\n\n\n\njwt\n - authentication token (received from \n/user/check\n)\n\n\nusername\n - A new username for the user profile (this action is only allowed once)\n\n\n\n\nReturns:\n\n\n{\n    \nsuccess\n: true,\n    \nerror\n: \nerror-message-if-applicable\n\n}\n\n\nNote\n: trying to update other user profile fields like \nemail\n will fail silently and return\n\n\n{ \n    \nsuccess\n: true\n}\n\n\n\nReceive authorization public key\n\n\n/user/public-key\n\n\nMethod:\n \nGET\n\n\nReturns:\n\n\nThe service's public key in PEM format.\n\n\nCan be used by services to validate that the permission token is authentic.\n\n\nRawstore\n\n\nGet authorized upload URL(s)\n\n\n/authorize\n\n\nMethod:\n \nPOST\n\n\nQuery Parameters:\n\n\n\n\njwt\n - permission token (received from \n/user/authorize\n)\n\n\n\n\nHeaders:\n\n\n\n\nAuth-Token\n - permission token (received from conductor)\n\n\n\n\nBody:\n\n\n{\n    \nmetadata\n: {\n        \nowner\n: \nuser-id-of-uploader\n,\n        \nname\n: \ndata-set-unique-id\n\n    },\n    \nfiledata\n: {\n        \nrelative-path-to-file-in-package-1\n: {\n            \nlength\n: 1234, //length in bytes of data\n            \nmd5\n: \nmd5-hash-of-the-data\n,\n            \ntype\n: \ncontent-type-of-the-data\n,\n            \nname\n: \nfile-name\n\n        },\n        \nrelative-path-to-file-in-package-2\n: {\n            \nlength\n: 4321,\n            \nmd5\n: \nmd5-hash-of-the-data\n,\n            \ntype\n: \ncontent-type-of-the-data\n,\n            \nname\n: \nfile-name\n\n        }\n        ...\n    }\n}\n\n\n\nReturns:\n\n\nSigned urls to upload into S3:\n\n\n{\n  fileData: {\n    \nfile-name-1\n: {\n      \nmd5-hash\n: \n...\n,\n      \nname\n: \nfile-name\n,\n      \ntype\n: \nfile-type\n,\n      \nupload_query\n: {\n        'Content-MD5': '...',\n        'Content-Type': '...',\n        'acl': 'public-read',\n        'key': '\npath\n',\n        'policy': '...',\n        'x-amz-algorithm': 'AWS4-HMAC-SHA256',\n        'x-amz-credential': '...',\n        'x-amz-date': '\ndate-time-in-ISO',\n        'x-amz-signature': '...'\n      },\n      \nupload_url\n: \ns3-url\n\n    },\n    \nfile-name-2\n: ...,\n    ...\n  }\n}\n\n\n\n\nGet information regarding the datastore\n\n\n/info\n\n\nMethod:\n \nGET\n\n\nQuery Parameters:\n\n\n\n\njwt\n - permission token (received from \n/user/authorize\n)\n\n\n\n\nHeaders:\n\n\n\n\nAuth-Token\n - permission token (can be used instead of the \njwt\n query parameter)\n\n\n\n\nReturns:\n\n\nJSON content with the following structure:\n\n{\n    \nprefixes\n: [\n        \nhttps://api.datahub.io/rawstore/123456789\n,\n        ...\n    ]\n}\n\n\nprefixes\n is the list of possible prefixes for an uploaded file for this user.\n\n\nSources\n\n\nGet status\n\n\n/source/{identifier}/status\n\n\nMethod:\n \nGET\n\n\nReturns:\n\n\n{\n   'state': 'queued/running/errored',\n   'errors': [\n       'error-message', // ...\n   ],\n   'logs': [\n              'log-line', \n              'log-line', // ...\n           ],\n   'history': [\n      {\n       'execution-time': 'iso-time',\n       'success': true or false,\n       'termination-time': 'iso-time' or null\n      }, // ...   \n   ],\n   'outputs': [\n       {\n        'kind': '\nkind\n', \n        'url': '\nurl\n', \n        'created-at': '\niso-time\n',\n        'filename': '\ndisplayable-filename\n',\n        'title': '\ndisplayable-title\n'\n       }\n   ],\n   'stats': {\n       'key': 'value' // e.g. 'count-of-rows', etc.\n   }\n}\n\n\n\nUpload\n\n\n/source/upload\n\n\nMethod:\n \nPOST\n\n\nQuery Parameters:\n\n\nHeaders:\n\n\n\n\nAuth-Token\n - permission token (received from conductor)\n\n\n\n\nBody:\n\n\n{\n    'meta': {\n        'version': 1, // version of the _spec_\n        'owner': '\nuser-id\n', // Will be validated on upload\n        'id': '\nid\n'\n    },\n    'inputs': [\n        {\n            'kind': 'datapackage',\n            'url': '\nurl\n',\n            'parameters': {\n                'resource-mapping': {\n                    '\nresource-name\n': '\nresource-url\n'\n                }\n            }\n        }    \n    ],\n    'processing': [\n        {\n            // TBD\n        }\n    ],\n    'outputs': [\n        {\n            'kind': 'datapackage',\n            'parameters': {\n                // e.g. 'create-previews': false\n            }\n        }\n    ]\n}\n\n\n\nReturns:\n\n\n{\n  \nsuccess\n: true,\n  \nid\n: \nidentifier\n\n  \nerrors\n: [\n      \nerror-message\n\n  ]\n}", 
            "title": "API"
        }, 
        {
            "location": "/developers/api/#datahub-api", 
            "text": "The DataHub API provides a range of endpoints to interact with the platform. All endpoints live under the URL  https://api.datahub.io  where our API is divided into the following sections:  auth, rawstore, sources", 
            "title": "DataHub API"
        }, 
        {
            "location": "/developers/api/#auth", 
            "text": "A generic OAuth2 authentication service and user permission manager.", 
            "title": "Auth"
        }, 
        {
            "location": "/developers/api/#check-an-authentication-tokens-validity", 
            "text": "/auth/check  Method:   GET  Query Parameters:   jwt  - authentication token  next  - URL to redirect to when finished authentication   Returns:  If authenticated:  {\n     authenticated : true,\n     profile : {\n         id :  user-id ,\n         name :  user-name ,\n         email :  user-email ,\n         avatar_url :  url-for-user's-profile-photo ,\n         idhash :  unique-id-of-the-user ,\n         username :  user-selected-id  // If user has a username\n    }\n}  If not:  {\n     authenticated : false,\n     providers : {\n         google : {\n             url :  url-for-logging-in-with-the-Google-provider \n        },\n         github : {\n             url :  url-for-logging-in-with-the-Github-provider \n        },\n    }\n}  When the authentication flow is finished, the caller will be redirected to the  next  URL with an extra query parameter jwt  which contains the authentication token. The caller should cache this token for further interactions with the API.", 
            "title": "Check an authentication token's validity"
        }, 
        {
            "location": "/developers/api/#get-permission-for-a-service", 
            "text": "/user/authorize  Method:   GET  Query Parameters:   jwt  - user token (received from  /user/check )  service  - the relevant service (e.g.  storage-service )   Returns:  {\n     token :  token-for-the-relevant-service \n     userid :  unique-id-of-the-user ,\n     permissions : {\n         permission-x : true,\n         permission-y : false\n    },\n     service :  relevant-service \n}", 
            "title": "Get permission for a service"
        }, 
        {
            "location": "/developers/api/#change-the-username", 
            "text": "/user/update  Method:   POST  Query Parameters:   jwt  - authentication token (received from  /user/check )  username  - A new username for the user profile (this action is only allowed once)   Returns:  {\n     success : true,\n     error :  error-message-if-applicable \n}  Note : trying to update other user profile fields like  email  will fail silently and return  { \n     success : true\n}", 
            "title": "Change the username"
        }, 
        {
            "location": "/developers/api/#receive-authorization-public-key", 
            "text": "/user/public-key  Method:   GET  Returns:  The service's public key in PEM format.  Can be used by services to validate that the permission token is authentic.", 
            "title": "Receive authorization public key"
        }, 
        {
            "location": "/developers/api/#rawstore", 
            "text": "", 
            "title": "Rawstore"
        }, 
        {
            "location": "/developers/api/#get-authorized-upload-urls", 
            "text": "/authorize  Method:   POST  Query Parameters:   jwt  - permission token (received from  /user/authorize )   Headers:   Auth-Token  - permission token (received from conductor)   Body:  {\n     metadata : {\n         owner :  user-id-of-uploader ,\n         name :  data-set-unique-id \n    },\n     filedata : {\n         relative-path-to-file-in-package-1 : {\n             length : 1234, //length in bytes of data\n             md5 :  md5-hash-of-the-data ,\n             type :  content-type-of-the-data ,\n             name :  file-name \n        },\n         relative-path-to-file-in-package-2 : {\n             length : 4321,\n             md5 :  md5-hash-of-the-data ,\n             type :  content-type-of-the-data ,\n             name :  file-name \n        }\n        ...\n    }\n}  Returns:  Signed urls to upload into S3:  {\n  fileData: {\n     file-name-1 : {\n       md5-hash :  ... ,\n       name :  file-name ,\n       type :  file-type ,\n       upload_query : {\n        'Content-MD5': '...',\n        'Content-Type': '...',\n        'acl': 'public-read',\n        'key': ' path ',\n        'policy': '...',\n        'x-amz-algorithm': 'AWS4-HMAC-SHA256',\n        'x-amz-credential': '...',\n        'x-amz-date': ' date-time-in-ISO',\n        'x-amz-signature': '...'\n      },\n       upload_url :  s3-url \n    },\n     file-name-2 : ...,\n    ...\n  }\n}", 
            "title": "Get authorized upload URL(s)"
        }, 
        {
            "location": "/developers/api/#get-information-regarding-the-datastore", 
            "text": "/info  Method:   GET  Query Parameters:   jwt  - permission token (received from  /user/authorize )   Headers:   Auth-Token  - permission token (can be used instead of the  jwt  query parameter)   Returns:  JSON content with the following structure: {\n     prefixes : [\n         https://api.datahub.io/rawstore/123456789 ,\n        ...\n    ]\n}  prefixes  is the list of possible prefixes for an uploaded file for this user.", 
            "title": "Get information regarding the datastore"
        }, 
        {
            "location": "/developers/api/#sources", 
            "text": "", 
            "title": "Sources"
        }, 
        {
            "location": "/developers/api/#get-status", 
            "text": "/source/{identifier}/status  Method:   GET  Returns:  {\n   'state': 'queued/running/errored',\n   'errors': [\n       'error-message', // ...\n   ],\n   'logs': [\n              'log-line', \n              'log-line', // ...\n           ],\n   'history': [\n      {\n       'execution-time': 'iso-time',\n       'success': true or false,\n       'termination-time': 'iso-time' or null\n      }, // ...   \n   ],\n   'outputs': [\n       {\n        'kind': ' kind ', \n        'url': ' url ', \n        'created-at': ' iso-time ',\n        'filename': ' displayable-filename ',\n        'title': ' displayable-title '\n       }\n   ],\n   'stats': {\n       'key': 'value' // e.g. 'count-of-rows', etc.\n   }\n}", 
            "title": "Get status"
        }, 
        {
            "location": "/developers/api/#upload", 
            "text": "/source/upload  Method:   POST  Query Parameters:  Headers:   Auth-Token  - permission token (received from conductor)   Body:  {\n    'meta': {\n        'version': 1, // version of the _spec_\n        'owner': ' user-id ', // Will be validated on upload\n        'id': ' id '\n    },\n    'inputs': [\n        {\n            'kind': 'datapackage',\n            'url': ' url ',\n            'parameters': {\n                'resource-mapping': {\n                    ' resource-name ': ' resource-url '\n                }\n            }\n        }    \n    ],\n    'processing': [\n        {\n            // TBD\n        }\n    ],\n    'outputs': [\n        {\n            'kind': 'datapackage',\n            'parameters': {\n                // e.g. 'create-previews': false\n            }\n        }\n    ]\n}  Returns:  {\n   success : true,\n   id :  identifier \n   errors : [\n       error-message \n  ]\n}", 
            "title": "Upload"
        }, 
        {
            "location": "/developers/publish/", 
            "text": "Publish\n\n\nExplanation of DataHub publishing flow from client and back-end perspectives.\n\n\nClient Perspective\n\n\nPublishing flow takes the following steps and processes to communicate with DataHub API:\n\n\n\nsequenceDiagram\nUpload Agent CLI->>Upload Agent CLI: Check Data Package valid\nUpload Agent CLI-->>Auth(SSO): Get Session Token (Sends base auth)\nAuth(SSO)-->>Upload Agent CLI: session token\nUpload Agent CLI->>BitStore Upload Auth: Get BitStore Upload token [send session token]\nBitStore Upload Auth->>Auth(SSO): Check key / token\nAuth(SSO)->>BitStore Upload Auth: OK / Not OK\nBitStore Upload Auth->>Upload Agent CLI: S3 auth Token\nUpload Agent CLI->>Data Storage (S3 Raw): Send file plus token\nData Storage (S3 Raw)->>Upload Agent CLI: OK / Not OK\nUpload Agent CLI->>MetaData Storage API: Finalize (After all data uploaded)\nMetaData Storage API->>Upload Agent CLI: OK / Not OK\n\n\n\n\n\n\n\n\nUpload API - see \nPOST /api/package/upload\n in \npackage\n section of \nAPI\n\n\nAuthentication API - see \nPOST /api/auth/token\n in \nauth\n section of \nAPI\n. Read more \nabout authentication\n\n\n[Authorization API][authz] - see \nPOST /api/datastore/authorize\n in \npackage\n section of \nAPI\n. Read more \nabout authorization\n\n\n\n\nSee example \ncode snippet in dpm-py\n\n\n\n\nBack-end perspective\n\n\nDataHub Metadata and Data Flow\n\n\n\n\nPink = service we build\n\n\nBlue = external service\n\n\nDark gray = not yet implemented\n\n\n\n\n\ngraph TD\n\nuser[Publisher fa:fa-user]\nupload-api[\"Upload API (S3 API)\"]\nbitstore(Bitstore S3)\nmetaingestor[Meta Ingestor]\ndataingestor[Data Ingestor]\nmetastore(\"Metastore (RDS)\")\nreadapi[Read API]\ndataproxy[\"DataProxy (convert raw data to json on the fly)\"]\ndatastore[\"Datastore (RDS)\"]\ns3readapi[S3 Get API]\nreaduser[Consumer fa:fa-user]\n\nuser --s3 signed upload url--> upload-api\nupload-api --> bitstore\nbitstore --> metaingestor\nmetaingestor --> metastore\nmetastore --> readapi\nbitstore -.-> dataproxy\nbitstore -.-> dataingestor\ndataingestor -.-> datastore\ndatastore -.-> readapi\nbitstore --> s3readapi\ns3readapi --> readuser\ndataproxy -.-> readuser\nreadapi --> readuser\n\n  classDef extservice fill:lightblue,stroke:#333,stroke-width:4px;\n  classDef notimplemented fill:darkgrey,stroke:#bbb,stroke-width:1px;\n  classDef service fill:pink,stroke:#333,stroke-width:4px;\n  class datastore,dataingestor,dataproxy notimplemented;\n  class bitstore,metastore,s3readapi extservice;\n  class readapi service;\n\n\n\n\n\n\nAuthentication\n\n\nAuthorization\n\n\nMetastore\n\n\nBitStore", 
            "title": "Publish"
        }, 
        {
            "location": "/developers/publish/#publish", 
            "text": "Explanation of DataHub publishing flow from client and back-end perspectives.", 
            "title": "Publish"
        }, 
        {
            "location": "/developers/publish/#client-perspective", 
            "text": "Publishing flow takes the following steps and processes to communicate with DataHub API:  \nsequenceDiagram\nUpload Agent CLI->>Upload Agent CLI: Check Data Package valid\nUpload Agent CLI-->>Auth(SSO): Get Session Token (Sends base auth)\nAuth(SSO)-->>Upload Agent CLI: session token\nUpload Agent CLI->>BitStore Upload Auth: Get BitStore Upload token [send session token]\nBitStore Upload Auth->>Auth(SSO): Check key / token\nAuth(SSO)->>BitStore Upload Auth: OK / Not OK\nBitStore Upload Auth->>Upload Agent CLI: S3 auth Token\nUpload Agent CLI->>Data Storage (S3 Raw): Send file plus token\nData Storage (S3 Raw)->>Upload Agent CLI: OK / Not OK\nUpload Agent CLI->>MetaData Storage API: Finalize (After all data uploaded)\nMetaData Storage API->>Upload Agent CLI: OK / Not OK    Upload API - see  POST /api/package/upload  in  package  section of  API  Authentication API - see  POST /api/auth/token  in  auth  section of  API . Read more  about authentication  [Authorization API][authz] - see  POST /api/datastore/authorize  in  package  section of  API . Read more  about authorization   See example  code snippet in dpm-py", 
            "title": "Client Perspective"
        }, 
        {
            "location": "/developers/publish/#back-end-perspective", 
            "text": "DataHub Metadata and Data Flow   Pink = service we build  Blue = external service  Dark gray = not yet implemented   \ngraph TD\n\nuser[Publisher fa:fa-user]\nupload-api[\"Upload API (S3 API)\"]\nbitstore(Bitstore S3)\nmetaingestor[Meta Ingestor]\ndataingestor[Data Ingestor]\nmetastore(\"Metastore (RDS)\")\nreadapi[Read API]\ndataproxy[\"DataProxy (convert raw data to json on the fly)\"]\ndatastore[\"Datastore (RDS)\"]\ns3readapi[S3 Get API]\nreaduser[Consumer fa:fa-user]\n\nuser --s3 signed upload url--> upload-api\nupload-api --> bitstore\nbitstore --> metaingestor\nmetaingestor --> metastore\nmetastore --> readapi\nbitstore -.-> dataproxy\nbitstore -.-> dataingestor\ndataingestor -.-> datastore\ndatastore -.-> readapi\nbitstore --> s3readapi\ns3readapi --> readuser\ndataproxy -.-> readuser\nreadapi --> readuser\n\n  classDef extservice fill:lightblue,stroke:#333,stroke-width:4px;\n  classDef notimplemented fill:darkgrey,stroke:#bbb,stroke-width:1px;\n  classDef service fill:pink,stroke:#333,stroke-width:4px;\n  class datastore,dataingestor,dataproxy notimplemented;\n  class bitstore,metastore,s3readapi extservice;\n  class readapi service;   Authentication  Authorization  Metastore  BitStore", 
            "title": "Back-end perspective"
        }, 
        {
            "location": "/developers/views/", 
            "text": "Views\n\n\nTo render Data Packages in browsers we use DataHub views written in JavaScript. The module implemented in ReactJS framework and it can render tables, maps and various graphs using third-party libraries.\n\n\n\n  graph TD\n\n  url[\"metadata URL passed from back-end\"]\n  dp-js[datapackage-js]\n  dprender[datapackage-render-js]\n  table[\"table view\"]\n  chart[\"graph view\"]\n  hot[HandsOnTable]\n  map[LeafletMap]\n  vega[Vega]\n  plotly[Plotly]\n  browser[Browser]\n\n  url --> dp-js\n  dp-js --fetched dp--> dprender\n  dprender --spec--> table\n  table --1..n--> hot\n  dprender --geojson--> map\n  dprender --spec--> chart\n  chart --0..n--> vega\n  chart --0..n--> plotly\n  hot --table--> browser\n  map --map--> browser\n  vega --graph--> browser\n  plotly --graph--> browser\n\n\n\n\nNotice that DataHub views render a table view per tabular resource. If GeoJSON resource is given, it renders a map. Graph views should be specified in \nviews\n property of a Data Package.\n\n\nLinks\n\n\n\n\ndpr-js repo\n\n\ndatapackage-render-js\n\n\nviews specification and analysis", 
            "title": "Views"
        }, 
        {
            "location": "/developers/views/#views", 
            "text": "To render Data Packages in browsers we use DataHub views written in JavaScript. The module implemented in ReactJS framework and it can render tables, maps and various graphs using third-party libraries.  \n  graph TD\n\n  url[\"metadata URL passed from back-end\"]\n  dp-js[datapackage-js]\n  dprender[datapackage-render-js]\n  table[\"table view\"]\n  chart[\"graph view\"]\n  hot[HandsOnTable]\n  map[LeafletMap]\n  vega[Vega]\n  plotly[Plotly]\n  browser[Browser]\n\n  url --> dp-js\n  dp-js --fetched dp--> dprender\n  dprender --spec--> table\n  table --1..n--> hot\n  dprender --geojson--> map\n  dprender --spec--> chart\n  chart --0..n--> vega\n  chart --0..n--> plotly\n  hot --table--> browser\n  map --map--> browser\n  vega --graph--> browser\n  plotly --graph--> browser  Notice that DataHub views render a table view per tabular resource. If GeoJSON resource is given, it renders a map. Graph views should be specified in  views  property of a Data Package.", 
            "title": "Views"
        }, 
        {
            "location": "/developers/views/#links", 
            "text": "dpr-js repo  datapackage-render-js  views specification and analysis", 
            "title": "Links"
        }, 
        {
            "location": "/developers/authorization/", 
            "text": "Authorization Set up\n\n\nAuthorization is the process of giving someone permission to do or have something. In multi-user systems, a system administrator defines for the system which users are allowed access to the system and what privileges of use.\n\n\nWe have a standard access control matrix with 3 axes:\n\n\n\n\nActions: CREATE, READ, WRITE, DELETE, PURGE etc. these can vary among different entities\n\n\nEntities (object): User, Publisher, Package, Package Resource, \u2026\n\n\nUsers: a user or type of user\n\n\n\n\nPermission is a tuple of \n(Users, Entities, Actions)\n\n\nIntroducing Roles\n\n\nIt can be tiresome and inefficient to list for every object all the users permitted to perform a given action. For example:\n\n\n\n\nMany users in an organization get same set of privileges because of their position in the organization.\n\n\nWe want to change the permissions associated with a certain level in the organization and to have those permissions changed for all people in that level\n\n\nA user may change level frequently (ex. user may get promoted)\n\n\n\n\nSo we create roles\n\n\n\n\nPer object roles e.g. Package Owner\n\n\nPer system roles e.g. System Administrator\n\n\nA list or algorithm for assigning Users =\n Roles\n\n\n\n\nAccess control algorithm:\n\nis_allowed(user, entity, action)\n\n\nFor this user: what roles do they have related to this entity and the system?\nGiven those roles: what actions do they have: UNIONrole\n\n\nNote: it would get more complex if some roles deny access. E.g. Role: Spammer might mean you are denied action to posting etc. Right now we don\u2019t have that issue.\n\n\nIs the desired action in that set?\n\n\nRoles\n\n\nThe example roles are given below.\n\n\n\n\nPackage\n\n\nOwner  =\n all actions\n\n\nEditor\n\n\nRead\n\n\nCreate\n\n\nDelete\n\n\nUndelete\n\n\nUpdate\n\n\nTag\n\n\n\n\n\n\nViewer  =\n Only read\n\n\n\n\n\n\nPublisher\n\n\nOwner =\n all actions on Publisher\n\n\nEditor\n\n\nViewMemberList\n\n\nAddMember\n\n\nRemoveMember\n\n\nRead\n\n\n\n\n\n\nViewer =\n Only Read\n\n\n\n\n\n\nSystem\n\n\nLoggedIn\n\n\nPackage::Create\n\n\nPublisher::Create\n\n\n\n\n\n\nAll =\n Package::Read on public packages\n\n\nSysadmin =\n all actions\n\n\n\n\n\n\n\n\nThis\n contains the current roles.\n\n\nBusiness roles\n\n\n\n\nPublisher Owner\n\n\nPublisher::Owner\n\n\n\n\n\n\nPublisher Member\n\n\nPublisher::Editor\n\n\n\n\n\n\n(Logged in) User\n\n\nSystem::LoggedIn\n\n\n\n\n\n\nSys Admin\n\n\nSystem::Sysadmin\n\n\n\n\n\n\nVisitor\n\n\nSystem::Anonymous\n\n\n\n\n\n\n\n\n\n\nNOTE: business roles and authorization roles are distinct. Of course, in implementing access control we will use the business logic inherent in business roles. However, business roles are not explicitly present in the access control system.\n\n\n\n\nActions\n\n\n\n\nNote: not an exhaustive list. \nThis\n contains the current Actions.\n\n\n\n\n\n\nPackage:\n\n\nPackage::Read\n\n\nPackage::Create\n\n\nPackage::Delete\n\n\nPackage::Undelete\n\n\nPackage::Purge\n\n\nPackage::Update\n\n\nPackage::Tag\n\n\n\n\n\n\nPublisher:\n\n\nPublisher::Create\n\n\nPublisher::AddMember\n\n\nPublisher::RemoveMember\n\n\nPublisher::Read\n\n\nPublisher::Delete\n\n\nPublisher::Update\n\n\nPublisher::ViewMemberList\n\n\n\n\n\n\n\n\nExamples\n\n\nFirst time visitor or not logged in:\n\n\nThe business role will be \nSystem::Anonymous\n. So the user can only has the action permission of \nPackage::Read\n.\nSo the user can only view the public data packages.\n\n\nLogged in user:\n\n\nThe business role will be \nSystem::LoggedIn\n . So the user will have permission of :\n\n\n\n\nPublisher::Create\n : The user can create new publisher.\n\n\nPackage::Create\n : The user can create new data package.\n\n\nPackage::Read\n : Can read public data packages", 
            "title": "Authorization"
        }, 
        {
            "location": "/developers/authorization/#authorization-set-up", 
            "text": "Authorization is the process of giving someone permission to do or have something. In multi-user systems, a system administrator defines for the system which users are allowed access to the system and what privileges of use.  We have a standard access control matrix with 3 axes:   Actions: CREATE, READ, WRITE, DELETE, PURGE etc. these can vary among different entities  Entities (object): User, Publisher, Package, Package Resource, \u2026  Users: a user or type of user   Permission is a tuple of  (Users, Entities, Actions)", 
            "title": "Authorization Set up"
        }, 
        {
            "location": "/developers/authorization/#introducing-roles", 
            "text": "It can be tiresome and inefficient to list for every object all the users permitted to perform a given action. For example:   Many users in an organization get same set of privileges because of their position in the organization.  We want to change the permissions associated with a certain level in the organization and to have those permissions changed for all people in that level  A user may change level frequently (ex. user may get promoted)   So we create roles   Per object roles e.g. Package Owner  Per system roles e.g. System Administrator  A list or algorithm for assigning Users =  Roles   Access control algorithm: is_allowed(user, entity, action)  For this user: what roles do they have related to this entity and the system?\nGiven those roles: what actions do they have: UNIONrole  Note: it would get more complex if some roles deny access. E.g. Role: Spammer might mean you are denied action to posting etc. Right now we don\u2019t have that issue.  Is the desired action in that set?", 
            "title": "Introducing Roles"
        }, 
        {
            "location": "/developers/authorization/#roles", 
            "text": "The example roles are given below.   Package  Owner  =  all actions  Editor  Read  Create  Delete  Undelete  Update  Tag    Viewer  =  Only read    Publisher  Owner =  all actions on Publisher  Editor  ViewMemberList  AddMember  RemoveMember  Read    Viewer =  Only Read    System  LoggedIn  Package::Create  Publisher::Create    All =  Package::Read on public packages  Sysadmin =  all actions     This  contains the current roles.", 
            "title": "Roles"
        }, 
        {
            "location": "/developers/authorization/#business-roles", 
            "text": "Publisher Owner  Publisher::Owner    Publisher Member  Publisher::Editor    (Logged in) User  System::LoggedIn    Sys Admin  System::Sysadmin    Visitor  System::Anonymous      NOTE: business roles and authorization roles are distinct. Of course, in implementing access control we will use the business logic inherent in business roles. However, business roles are not explicitly present in the access control system.", 
            "title": "Business roles"
        }, 
        {
            "location": "/developers/authorization/#actions", 
            "text": "Note: not an exhaustive list.  This  contains the current Actions.    Package:  Package::Read  Package::Create  Package::Delete  Package::Undelete  Package::Purge  Package::Update  Package::Tag    Publisher:  Publisher::Create  Publisher::AddMember  Publisher::RemoveMember  Publisher::Read  Publisher::Delete  Publisher::Update  Publisher::ViewMemberList", 
            "title": "Actions"
        }, 
        {
            "location": "/developers/authorization/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/developers/authorization/#first-time-visitor-or-not-logged-in", 
            "text": "The business role will be  System::Anonymous . So the user can only has the action permission of  Package::Read .\nSo the user can only view the public data packages.", 
            "title": "First time visitor or not logged in:"
        }, 
        {
            "location": "/developers/authorization/#logged-in-user", 
            "text": "The business role will be  System::LoggedIn  . So the user will have permission of :   Publisher::Create  : The user can create new publisher.  Package::Create  : The user can create new data package.  Package::Read  : Can read public data packages", 
            "title": "Logged in user:"
        }, 
        {
            "location": "/developers/authentication/", 
            "text": "DataHub Authentication\n\n\nThis page describes authentication of DataHub users. The details provided can be used by developers, willing to contribute to the existing \ndpm\n API client or implement custom client for The DataHub API.\n\n\nThe DataHub Frontend allows users to be registered via \ngithub\n using the web browser. After a successful registration, user will be given unique API-KEY to authenticate with DataHub API server.\n\n\nAPI authentication\n\n\nSome DataHub API methods require client to provide identity of a registered user. To prove its identity, client first has to obtain temporal JWT token, providing permanent API-KEY of a registered user. After that client can pass this token in the header of a request to the API.\n\n\nTo obtain a temporal JWT token, client should send POST request to \n/api/auth/token\n. Request should have json-encoded body with 'username' and 'secret' keys, where 'secret' is an API-KEY of the user:\n\n\nresponse = requests.post(\n        url='https://datapackaged.com/api/auth/token',\n        {'username': 'my_username', 'secret': '1dd5f984bc'}))\n\n\n\nIf the username and API-KEY are valid, server will return json response with JWT token: \n{'token': 'a6d8b887'}\n\n\nauth_token = response.json().get('token')\n\n\n\nThis token should be temporarily stored by the client. To access any API method with authentication, client should include this token in the \"Authorization\" header.\n\n\nrequests.post(api_url, headers={'Authorization', 'Bearer %s' % auth_token})", 
            "title": "Authentication"
        }, 
        {
            "location": "/developers/authentication/#datahub-authentication", 
            "text": "This page describes authentication of DataHub users. The details provided can be used by developers, willing to contribute to the existing  dpm  API client or implement custom client for The DataHub API.  The DataHub Frontend allows users to be registered via  github  using the web browser. After a successful registration, user will be given unique API-KEY to authenticate with DataHub API server.", 
            "title": "DataHub Authentication"
        }, 
        {
            "location": "/developers/authentication/#api-authentication", 
            "text": "Some DataHub API methods require client to provide identity of a registered user. To prove its identity, client first has to obtain temporal JWT token, providing permanent API-KEY of a registered user. After that client can pass this token in the header of a request to the API.  To obtain a temporal JWT token, client should send POST request to  /api/auth/token . Request should have json-encoded body with 'username' and 'secret' keys, where 'secret' is an API-KEY of the user:  response = requests.post(\n        url='https://datapackaged.com/api/auth/token',\n        {'username': 'my_username', 'secret': '1dd5f984bc'}))  If the username and API-KEY are valid, server will return json response with JWT token:  {'token': 'a6d8b887'}  auth_token = response.json().get('token')  This token should be temporarily stored by the client. To access any API method with authentication, client should include this token in the \"Authorization\" header.  requests.post(api_url, headers={'Authorization', 'Bearer %s' % auth_token})", 
            "title": "API authentication"
        }, 
        {
            "location": "/publishers/", 
            "text": "Publishers\n\n\nThis section of the DataHub documentation is for data publishers. Here you can learn about getting your data ready for loading into DataHub, and how you can interact with your data once it is loaded.\n\n\n\n\nPublishing a Data Package\n\n\nSign up \n get a secret key\n\n\nInstall command line tool\n\n\nConfigure\n\n\nPublish a dataset\n\n\nView it online\n\n\n\n\n\n\n\n\nPublishing a Data Package\n\n\nSign up \n get a secret key\n\n\nYou can sign up using your GitHub account. Once you are signed in, you will be redirected to a dashboard, where you can find your secret key (access token).\n\n\nInstall command line tool\n\n\nNext you need to install \ndpm\n - the data package manager command line tool:\n\n\n$ [sudo] pip install git+https://github.com/frictionlessdata/dpm-py.git\n\n\n\nConfigure\n\n\nYou will need the secret key (access token) to set your configurations:\n\n\n$ dpm configure\n\n\n Username:  \n your user name \n\n\n Your access_token:  \n you secret key \n\n\n Server URL: https://www.datapackaged.com\n\n\n\nNote: server URL may vary depending on application development stage\n\n\nPublish a dataset\n\n\nWe assume you know what a \nData Package\n is.\n\n\nGo to a directory where your data package is located and publish it:\n\n\n$ cd your-data-package-directory/\n$ dpm publish\n\n\n\nView it online\n\n\nOnce your data package is successfully published, you will get an URL to your dataset on the website. Open the URL in your favourite browser and explore it.", 
            "title": "Getting started"
        }, 
        {
            "location": "/publishers/#publishers", 
            "text": "This section of the DataHub documentation is for data publishers. Here you can learn about getting your data ready for loading into DataHub, and how you can interact with your data once it is loaded.   Publishing a Data Package  Sign up   get a secret key  Install command line tool  Configure  Publish a dataset  View it online", 
            "title": "Publishers"
        }, 
        {
            "location": "/publishers/#publishing-a-data-package", 
            "text": "", 
            "title": "Publishing a Data Package"
        }, 
        {
            "location": "/publishers/#sign-up-get-a-secret-key", 
            "text": "You can sign up using your GitHub account. Once you are signed in, you will be redirected to a dashboard, where you can find your secret key (access token).", 
            "title": "Sign up &amp; get a secret key"
        }, 
        {
            "location": "/publishers/#install-command-line-tool", 
            "text": "Next you need to install  dpm  - the data package manager command line tool:  $ [sudo] pip install git+https://github.com/frictionlessdata/dpm-py.git", 
            "title": "Install command line tool"
        }, 
        {
            "location": "/publishers/#configure", 
            "text": "You will need the secret key (access token) to set your configurations:  $ dpm configure  Username:    your user name    Your access_token:    you secret key    Server URL: https://www.datapackaged.com  Note: server URL may vary depending on application development stage", 
            "title": "Configure"
        }, 
        {
            "location": "/publishers/#publish-a-dataset", 
            "text": "We assume you know what a  Data Package  is.  Go to a directory where your data package is located and publish it:  $ cd your-data-package-directory/\n$ dpm publish", 
            "title": "Publish a dataset"
        }, 
        {
            "location": "/publishers/#view-it-online", 
            "text": "Once your data package is successfully published, you will get an URL to your dataset on the website. Open the URL in your favourite browser and explore it.", 
            "title": "View it online"
        }, 
        {
            "location": "/publishers/core-datasets/", 
            "text": "Core Datasets\n\n\nImportant, commonly-used datasets as high quality, easy-to-use \n open data packages\n\n\nCore Datasets are important, commonly-used \n\"core\" datasets\n like GDP or country codes made available as \nhigh-quality\n, \neasy-to-use\n and \nopen\n \ndata packages\n. Find them online here on the DataHub:\n\n\nhttp://datapackaged.com/core/\n\n\nKey features are:\n\n\n\n\nHigh Quality \n Reliable\n -- sourcing, normalizing and quality checking a set of \nkey reference and indicator datasets such as country codes, currencies, GDP and population\n\n\nStandardized \n Bulk\n -- all datasets provided in a \nstandardized\n form and can be accessed in \nbulk as CSV\n together with a simple \nJSON schema\n\n\nVersioned \n Packaged\n -- all data is in \ndata packages\n and is \nversioned\n using git so all changes are visible and data can be \ncollaboratively maintained\n\n\n\n\nThe \"Core Datasets\" effort is part of the broader \nFrictionless Data initiative\n.\n\n\n\n\n\n\nCore Data Curators\n\n\nThe Core Data Curators curate the core datasets.\n\n\nCuration involves identifying and locating core (public) datasets, then packaging them up as high-quality, reliable, and easy-to-use \ndata packages\n (standardized, structured, open).\n\n\nNew team members wanted:\n We are always seeking volunteers to join the Data Curators team. Get to be part of a crack team and develop and hone your data wrangling skills whilst helping to provide high quality data to the community.\n\n\n\n\nAnyone can contribute\n: details on the \nroles and skills needed below\n.\n\n\nGet involved\n: read more below or jump straight to \nthe sign-up section\n.\n\n\nData Curators Guide\n: can't wait to get started as a Data Curator? You can dive straight in and start packaging datasets using the \ncore data curators guide\n.\n\n\n\n\n\n\n\n\n\nWhat Roles and Skills are Needed\n\n\nWe have a variety of roles from identifying new \"core\" datasets, to collecting and packaging the data, to performing quality control.\n\n\nCore Skills\n -- at least one of these skills is strongly recommended:\n\n\n\n\nData Wrangling Experience\n. Many of our source datasets are not complex (just an Excel file or similar) and can be \"wrangled\" in a Spreadsheet program. What we therefore recommend is at least one of:\n\n\nExperience with a Spreadsheet application such as Excel or (preferably) Google Docs including use of formulas and (desirably) macros (you should at least know how you could quickly convert a cell containing '2014' to '2014-01-01' across 1000 rows)\n\n\nCoding for data processing (especially scraping) in one or more of python, javascript, bash\n\n\n\n\n\n\nData sleuthing\n - the ability to dig up data on the web (specific desirable skills: you know how to search by filetype in google, you know where the developer tools are in chrome or firefox, you know how to find the URL a form posts to)\n\n\n\n\nDesirable Skills\n (the more the better!):\n\n\n\n\nData vs Metadata: know difference between data and metadata\n\n\nFamiliarity with Git (and Github)\n\n\nFamiliarity with a command line (preferably bash)\n\n\nKnow what JSON is\n\n\nMac or Unix is your default operating system (will make access to relevant tools that much easier)\n\n\nKnowledge of Web APIs and/or HTML\n\n\nUse of curl or similar command line tool for accessing Web APIs or web pages\n\n\nScraping using a command line tool or (even better) by coding yourself\n\n\nKnow what a Data Package and a Tabular Data Package are\n\n\nKnow what a text editor is (e.g. notepad, textmate, vim, emacs, ...) and know how to use it (useful for both working with data and for editing Data Package metadata)\n\n\n\n\n\n\nGet Involved - Sign Up Now!\n\n\nHere's what you need to know when you sign up:\n\n\n\n\nTime commitment\n: Members of the team commit to at least 8-16h per month (though this will be an average - if you are especially busy with other things one month and do less that is fine)\n\n\nSchedule\n: There is no schedule so you can contribute at any time that is good for you - evenings, weekeneds, lunch-times etc\n\n\nLocation\n: all activity will be carried out online so you can be based anywhere in the world\n\n\nSkills\n: see above\n\n\n\n\nTo register your interest fill in the following form. Any questions, please \nget in touch directly\n.\n\n\nLoading...", 
            "title": "Core Datasets"
        }, 
        {
            "location": "/publishers/core-datasets/#core-datasets", 
            "text": "Important, commonly-used datasets as high quality, easy-to-use   open data packages  Core Datasets are important, commonly-used  \"core\" datasets  like GDP or country codes made available as  high-quality ,  easy-to-use  and  open   data packages . Find them online here on the DataHub:  http://datapackaged.com/core/  Key features are:   High Quality   Reliable  -- sourcing, normalizing and quality checking a set of  key reference and indicator datasets such as country codes, currencies, GDP and population  Standardized   Bulk  -- all datasets provided in a  standardized  form and can be accessed in  bulk as CSV  together with a simple  JSON schema  Versioned   Packaged  -- all data is in  data packages  and is  versioned  using git so all changes are visible and data can be  collaboratively maintained   The \"Core Datasets\" effort is part of the broader  Frictionless Data initiative .", 
            "title": "Core Datasets"
        }, 
        {
            "location": "/publishers/core-datasets/#core-data-curators", 
            "text": "The Core Data Curators curate the core datasets.  Curation involves identifying and locating core (public) datasets, then packaging them up as high-quality, reliable, and easy-to-use  data packages  (standardized, structured, open).  New team members wanted:  We are always seeking volunteers to join the Data Curators team. Get to be part of a crack team and develop and hone your data wrangling skills whilst helping to provide high quality data to the community.   Anyone can contribute : details on the  roles and skills needed below .  Get involved : read more below or jump straight to  the sign-up section .  Data Curators Guide : can't wait to get started as a Data Curator? You can dive straight in and start packaging datasets using the  core data curators guide .", 
            "title": "Core Data Curators"
        }, 
        {
            "location": "/publishers/core-datasets/#what-roles-and-skills-are-needed", 
            "text": "We have a variety of roles from identifying new \"core\" datasets, to collecting and packaging the data, to performing quality control.  Core Skills  -- at least one of these skills is strongly recommended:   Data Wrangling Experience . Many of our source datasets are not complex (just an Excel file or similar) and can be \"wrangled\" in a Spreadsheet program. What we therefore recommend is at least one of:  Experience with a Spreadsheet application such as Excel or (preferably) Google Docs including use of formulas and (desirably) macros (you should at least know how you could quickly convert a cell containing '2014' to '2014-01-01' across 1000 rows)  Coding for data processing (especially scraping) in one or more of python, javascript, bash    Data sleuthing  - the ability to dig up data on the web (specific desirable skills: you know how to search by filetype in google, you know where the developer tools are in chrome or firefox, you know how to find the URL a form posts to)   Desirable Skills  (the more the better!):   Data vs Metadata: know difference between data and metadata  Familiarity with Git (and Github)  Familiarity with a command line (preferably bash)  Know what JSON is  Mac or Unix is your default operating system (will make access to relevant tools that much easier)  Knowledge of Web APIs and/or HTML  Use of curl or similar command line tool for accessing Web APIs or web pages  Scraping using a command line tool or (even better) by coding yourself  Know what a Data Package and a Tabular Data Package are  Know what a text editor is (e.g. notepad, textmate, vim, emacs, ...) and know how to use it (useful for both working with data and for editing Data Package metadata)", 
            "title": "What Roles and Skills are Needed"
        }, 
        {
            "location": "/publishers/core-datasets/#get-involved-sign-up-now", 
            "text": "Here's what you need to know when you sign up:   Time commitment : Members of the team commit to at least 8-16h per month (though this will be an average - if you are especially busy with other things one month and do less that is fine)  Schedule : There is no schedule so you can contribute at any time that is good for you - evenings, weekeneds, lunch-times etc  Location : all activity will be carried out online so you can be based anywhere in the world  Skills : see above   To register your interest fill in the following form. Any questions, please  get in touch directly .  Loading...", 
            "title": "Get Involved - Sign Up Now!"
        }, 
        {
            "location": "/publishers/core-data-curators/", 
            "text": "Core Data Curators Guide\n\n\nThis is a guide for curators of \n\"core datasets\"\n. Curators collect and maintain these important and commonly-used (\u201ccore\u201d) datasets as high-quality, easy-to-use, open \nData Packages\n.\n\n\nQuick Links\n\n\n\n\nDiscussion forum\n - discussion takes place here by default\n\n\nThis is the place to ask questions, get help etc - just open a new topic\n\n\nIntroduction to Core Datasets Project\n\n\nJoin the Core Data Curators Team\n\n\nPackaging Queue (GitHub Issues Tracker)\n\n\nPublish Data Packages Howto on Frictionless Data Site\n\n\n\n\nQuick Start\n\n\n\n\nPlease take 2m to introduce yourself in the \ndiscussion forum\n so that other team members can get to know you\n\n\nRead the contributing guide below so you:\n\n\nunderstand the details of the curator workflow\n\n\ncan work out where you'd like to contribute\n\n\n\n\n\n\nStop: have you read the contributing guide? The next items only make sense if you have!\n\n\nNow you can dive in with one or both of:\n\n\nResearching: start reviewing the \ncurrent queue\n - add new items, comment on existing ones etc\n\n\nPackaging:  check out the \n\u201cReady to Package\u201d\n section of the queue and assign yourself (drop a comment in the issue claiming it)\n\n\n\n\n\n\n\n\nContributor Guide\n\n\n\n\nFig 1: Overview of the Curation Workflow\n\n\nThere are 2 areas of activity:\n\n\n\n\nPreparing datasets as Core Data Packages - finding them, cleaning them, data-packaging them\n\n\nMaintaining Core Data Packages - keeping them up to date with the source dataset, handling changes, responding to user queries\n\n\n\n\nEach of these has sub-steps which we detail below and you can contribute in any and all of these.\n\n\nKey principles of our approach are that:\n\n\n\n\nWe package data rather than create it \u2013 our focus is to take source data and ensure it is of high quality and in a standard form\n\n\nWe preserve a clean separation between the data source, the data package and this registry \u2013 for example, data packages are stored in git repos hosted separately (preferably github)\n\n\n\n\nPreparing Datasets as Core Data Packages\n\n\nThere are different areas where people can contribute:\n\n\n\n\nResearch\n\n\nPackaging up data\n\n\nQuality assurance\n\n\nFinal Publication into the official core datasets list\n\n\n\n\nOften you will contribute in all 4 by taking a dataset all the way from a suggestion to a fully packaged data package published online.\n\n\n1. Research\n\n\nThis involves researching and selecting datasets as core datasets and adding them to the queue for packaging - no coding or data wrangling skill is needed for this\n\n\n\n\nTo propose a dataset for addition you \nopen an issue in the Registry\n with the details of the proposed dataset.\n\n\nIdentify relevant source or sources for the dataset\n\n\nTo propose a dataset you do not have to know where to get the data from (e.g. you could suggest \u201cUS GDP\u201d as a core dataset without yet knowing where to get the data from)\n\n\nDiscuss with Queue Manager(s) (they will spot your submission and start commenting in the GitHub issue)\n\n\nIf good =\n Shortlist for Packaging - add \nLabel \u201cStatus: Ready to Package\u201d\n\n\n\n\n2. Packaging up data\n\n\nOnce we have a suggested dataset marked as \"ready to package\" we can move to packaging it up.\n\n\nHow to package up data is covered in the \ngeneral publishing guide\n.\n\n\n3. Quality Assurance\n\n\nThis involves validating and checking packaged datasets to ensure they are of high quality and ready to publish.\n\n\n\n\nValidate\n the Data Package and \nreview\n the data in the Data Package.\n\n\nIn the review phase, you should be looking at a table with the data you have input before. That will ensure your data package is working without any issues and that it follows the same quality standards that any other package.\n\n\nPost a validation link and a view link in the comments for the issue in the Registry related to your Data Package.\n\n\n\n\n4. Publishing\n\n\nWe have a few extra specific requirements:\n\n\n\n\nAll Data Packages must (ultimately) be stored in a public GitHub repo\n\n\nFirst publish to your own repository\n\n\nThen arrange a move the repository to \ngithub.com/datasets/ organization\n - as the owner of a repository you can initiate a transfer request to github.com/datasets/ which can then be approved\n\n\nAdd to the \ncatalog list\n \nand\n the \ncore list\n \nand\n the associated csv files: \ncatalog-list.csv\n and \ncore-list.csv\n.\n\n\nReload \nhttp://data.okfn.org/data/\n by visiting \nhttp://data.okfn.org/admin/reload/\n\n\nIf you have access, tweet from the @OKFNLabs account a link to the \nhttp://data.okfn.org/data/\n page for the dataset.\n\n\n\n\nMaintaining Data Packages\n\n\nMany data packages package data that changes over time - for example, many time series get updated monthly or daily.\n\n\nWe need people to become the \"maintainer\" for a given dataset and keep it up to date by regularly adding in the new data.\n\n\nList of datasets needing a maintainer\n\n\nCore Data Assessment Criteria\n\n\nFor a dataset to be designated as \"core\" it should meet the following criteria:\n\n\n\n\nQuality - the dataset must be well structured\n\n\nRelevance and importance - the focus at present is on indicators and reference data\n\n\nOngoing support - it should have a maintainer\n\n\nOpenness - data should be \nopen data\n and openly licensed in accordance with the \nOpen Definition\n\n\n\n\n\n\nGuide for Managing Curators\n\n\nIntro Email for New Joiners\n\n\nHi,\n\nWe are delighted to welcome you to the Core Data Curators team of crack data curators.\n\nTo kick-off your core data curatorship we invite you to:\n\n1. Introduce yourself in the forum here:\n\n    http://discuss.okfn.org/t/core-data-curators-introductions/145/24\n\n2. Take a look at the Core Data Curators guide:\n\n    http://docs.datahub.io/publishers/core-data-curators\n\nRegards,\n\nXXX", 
            "title": "Core Data Curators"
        }, 
        {
            "location": "/publishers/core-data-curators/#core-data-curators-guide", 
            "text": "This is a guide for curators of  \"core datasets\" . Curators collect and maintain these important and commonly-used (\u201ccore\u201d) datasets as high-quality, easy-to-use, open  Data Packages .", 
            "title": "Core Data Curators Guide"
        }, 
        {
            "location": "/publishers/core-data-curators/#quick-links", 
            "text": "Discussion forum  - discussion takes place here by default  This is the place to ask questions, get help etc - just open a new topic  Introduction to Core Datasets Project  Join the Core Data Curators Team  Packaging Queue (GitHub Issues Tracker)  Publish Data Packages Howto on Frictionless Data Site", 
            "title": "Quick Links"
        }, 
        {
            "location": "/publishers/core-data-curators/#quick-start", 
            "text": "Please take 2m to introduce yourself in the  discussion forum  so that other team members can get to know you  Read the contributing guide below so you:  understand the details of the curator workflow  can work out where you'd like to contribute    Stop: have you read the contributing guide? The next items only make sense if you have!  Now you can dive in with one or both of:  Researching: start reviewing the  current queue  - add new items, comment on existing ones etc  Packaging:  check out the  \u201cReady to Package\u201d  section of the queue and assign yourself (drop a comment in the issue claiming it)", 
            "title": "Quick Start"
        }, 
        {
            "location": "/publishers/core-data-curators/#contributor-guide", 
            "text": "Fig 1: Overview of the Curation Workflow  There are 2 areas of activity:   Preparing datasets as Core Data Packages - finding them, cleaning them, data-packaging them  Maintaining Core Data Packages - keeping them up to date with the source dataset, handling changes, responding to user queries   Each of these has sub-steps which we detail below and you can contribute in any and all of these.  Key principles of our approach are that:   We package data rather than create it \u2013 our focus is to take source data and ensure it is of high quality and in a standard form  We preserve a clean separation between the data source, the data package and this registry \u2013 for example, data packages are stored in git repos hosted separately (preferably github)", 
            "title": "Contributor Guide"
        }, 
        {
            "location": "/publishers/core-data-curators/#preparing-datasets-as-core-data-packages", 
            "text": "There are different areas where people can contribute:   Research  Packaging up data  Quality assurance  Final Publication into the official core datasets list   Often you will contribute in all 4 by taking a dataset all the way from a suggestion to a fully packaged data package published online.", 
            "title": "Preparing Datasets as Core Data Packages"
        }, 
        {
            "location": "/publishers/core-data-curators/#1-research", 
            "text": "This involves researching and selecting datasets as core datasets and adding them to the queue for packaging - no coding or data wrangling skill is needed for this   To propose a dataset for addition you  open an issue in the Registry  with the details of the proposed dataset.  Identify relevant source or sources for the dataset  To propose a dataset you do not have to know where to get the data from (e.g. you could suggest \u201cUS GDP\u201d as a core dataset without yet knowing where to get the data from)  Discuss with Queue Manager(s) (they will spot your submission and start commenting in the GitHub issue)  If good =  Shortlist for Packaging - add  Label \u201cStatus: Ready to Package\u201d", 
            "title": "1. Research"
        }, 
        {
            "location": "/publishers/core-data-curators/#2-packaging-up-data", 
            "text": "Once we have a suggested dataset marked as \"ready to package\" we can move to packaging it up.  How to package up data is covered in the  general publishing guide .", 
            "title": "2. Packaging up data"
        }, 
        {
            "location": "/publishers/core-data-curators/#3-quality-assurance", 
            "text": "This involves validating and checking packaged datasets to ensure they are of high quality and ready to publish.   Validate  the Data Package and  review  the data in the Data Package.  In the review phase, you should be looking at a table with the data you have input before. That will ensure your data package is working without any issues and that it follows the same quality standards that any other package.  Post a validation link and a view link in the comments for the issue in the Registry related to your Data Package.", 
            "title": "3. Quality Assurance"
        }, 
        {
            "location": "/publishers/core-data-curators/#4-publishing", 
            "text": "We have a few extra specific requirements:   All Data Packages must (ultimately) be stored in a public GitHub repo  First publish to your own repository  Then arrange a move the repository to  github.com/datasets/ organization  - as the owner of a repository you can initiate a transfer request to github.com/datasets/ which can then be approved  Add to the  catalog list   and  the  core list   and  the associated csv files:  catalog-list.csv  and  core-list.csv .  Reload  http://data.okfn.org/data/  by visiting  http://data.okfn.org/admin/reload/  If you have access, tweet from the @OKFNLabs account a link to the  http://data.okfn.org/data/  page for the dataset.", 
            "title": "4. Publishing"
        }, 
        {
            "location": "/publishers/core-data-curators/#maintaining-data-packages", 
            "text": "Many data packages package data that changes over time - for example, many time series get updated monthly or daily.  We need people to become the \"maintainer\" for a given dataset and keep it up to date by regularly adding in the new data.  List of datasets needing a maintainer", 
            "title": "Maintaining Data Packages"
        }, 
        {
            "location": "/publishers/core-data-curators/#core-data-assessment-criteria", 
            "text": "For a dataset to be designated as \"core\" it should meet the following criteria:   Quality - the dataset must be well structured  Relevance and importance - the focus at present is on indicators and reference data  Ongoing support - it should have a maintainer  Openness - data should be  open data  and openly licensed in accordance with the  Open Definition", 
            "title": "Core Data Assessment Criteria"
        }, 
        {
            "location": "/publishers/core-data-curators/#guide-for-managing-curators", 
            "text": "", 
            "title": "Guide for Managing Curators"
        }, 
        {
            "location": "/publishers/core-data-curators/#intro-email-for-new-joiners", 
            "text": "Hi,\n\nWe are delighted to welcome you to the Core Data Curators team of crack data curators.\n\nTo kick-off your core data curatorship we invite you to:\n\n1. Introduce yourself in the forum here:\n\n    http://discuss.okfn.org/t/core-data-curators-introductions/145/24\n\n2. Take a look at the Core Data Curators guide:\n\n    http://docs.datahub.io/publishers/core-data-curators\n\nRegards,\n\nXXX", 
            "title": "Intro Email for New Joiners"
        }, 
        {
            "location": "/publishers/cli/", 
            "text": "DATA: The Data Package Manager CLI\n\n\n\n\nGetting started\n\n\nInstallation\n\n\nCommands\n\n\nConfiguration\n\n\n\n\nUsage\n\n\n\n\nPublish\n\n\nDownload\n\n\nDelete\n\n\nInformation\n\n\nNormalize\n\n\nValidate\n\n\nConfiguration\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\n\n\nGetting started\n\n\nThe data is a command-line tool aimed to help publishers to prepare and upload data to the DataHub. With data you will be able to:\n\n\n\n\nPublish Data Package to DataHub\n\n\nGet Data Package from DataHub\n\n\nRemove uploaded Data Package from DataHub\n\n\nGet information about particular Data Package\n\n\nNormalize Data Package according to the specs\n\n\nValidate your data to ensure its quality\n\n\nSet up configuration file in order to publish\n\n\n\n\nInstallation\n\n\nInstalling binaries without npm\n\n\nOn the \nreleases\n page, you can download pre-built binaries for MacOS and LinuxOS x64. You may need to put the pre-built binary in the bin directory (e.g.: /usr/local/bin/).\n\n\nmv path/to/data-{os-distribution} /usr/local/bin/data\n\n\n\nInstalling from npm\n\n\nYou can also install it from \nnpm\n as follows:\n\nnpm install -g data\n\n\nCommands\n\n\nYou can see the latest commands and get help by doing:\n\n\ndata --help\n\n\n\nThe output of the help command:\n\n\n\u2752 data [options] \ncommand\n \nargs\n\n\nCommands:\n  DataHub:\n    push        [path]        Push data to the DataHub\n    get         [pkg-id]      Get data from DataHub\n    purge       [owner/name]  Permanently deletes data from DataHub\n  Data Package specific:\n    info        [pkg-id]      Get info on data\n    normalize                 Normalize datapackage.json\n    validate                  Validate Data Package structure\n\n  Administrative:\n    config                    Set up configuration\n    help        [cmd]         Show help on cmd\n\nOptions:\n-h, --help              Output usage information\n-v, --version           Output the version\n\n\n\n\nConfiguration\n\n\nData can be configured using \ndata config[ure]\n command. It will ask you to provide a username, secretToken, server and bitStore addresses of DataHub.\n\n\nThe config is stored in \n~/.datahub/config\n, you can edit it with text editor.\nSimple example config file can look like this:\n\n\nusername = myname\naccess_token = mykey\nserver = server URL for publishing Eg: https://www.datapackaged.com\n\n\n\nUsage\n\n\nPublish\n\n\nTo publish a Data Package, go to the Data Package directory (with \ndatapackage.json\n) and\nrun:\n\n\ndata push\n\n\n\nIf your configured \nusername\n and \nsecretToken\n are correct, data will\nupload datapackage.json and all relevant resources to the DataHub server.\n\n\nGet\n\n\nTo get Data Package run the following command:\n\ndata get \npublisher\n/\npackage\n\nNew Data Package will be downloaded into current working directory. \n\n\nDelete\n\n\nTo delete permanently Data Package from DataHub, you can use \ndpm purge\n command:\n\n\ndata purge\n\n\n\nInformation\n\n\nYou can get information about particular Data Package\n\n\ndata info\n\n\n\nNormalize\n\n\nTo normalize Data Package descriptor according to the specs\n\n\ndata norm[alize] [path]\n\n\n\nValidate\n\n\nTo validate Data Package descriptor against schema\n\n\ndata validate [path | URL]\n\n\n\nConfiguration\n\n\nTo set up configuration file:\n\ndata config[ure]\n\n\nLinks\n\n\n\n\nCode repo", 
            "title": "CLI"
        }, 
        {
            "location": "/publishers/cli/#data-the-data-package-manager-cli", 
            "text": "Getting started  Installation  Commands  Configuration   Usage   Publish  Download  Delete  Information  Normalize  Validate  Configuration     Links", 
            "title": "DATA: The Data Package Manager CLI"
        }, 
        {
            "location": "/publishers/cli/#getting-started", 
            "text": "The data is a command-line tool aimed to help publishers to prepare and upload data to the DataHub. With data you will be able to:   Publish Data Package to DataHub  Get Data Package from DataHub  Remove uploaded Data Package from DataHub  Get information about particular Data Package  Normalize Data Package according to the specs  Validate your data to ensure its quality  Set up configuration file in order to publish", 
            "title": "Getting started"
        }, 
        {
            "location": "/publishers/cli/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/publishers/cli/#installing-binaries-without-npm", 
            "text": "On the  releases  page, you can download pre-built binaries for MacOS and LinuxOS x64. You may need to put the pre-built binary in the bin directory (e.g.: /usr/local/bin/).  mv path/to/data-{os-distribution} /usr/local/bin/data", 
            "title": "Installing binaries without npm"
        }, 
        {
            "location": "/publishers/cli/#installing-from-npm", 
            "text": "You can also install it from  npm  as follows: npm install -g data", 
            "title": "Installing from npm"
        }, 
        {
            "location": "/publishers/cli/#commands", 
            "text": "You can see the latest commands and get help by doing:  data --help  The output of the help command:  \u2752 data [options]  command   args \n\nCommands:\n  DataHub:\n    push        [path]        Push data to the DataHub\n    get         [pkg-id]      Get data from DataHub\n    purge       [owner/name]  Permanently deletes data from DataHub\n  Data Package specific:\n    info        [pkg-id]      Get info on data\n    normalize                 Normalize datapackage.json\n    validate                  Validate Data Package structure\n\n  Administrative:\n    config                    Set up configuration\n    help        [cmd]         Show help on cmd\n\nOptions:\n-h, --help              Output usage information\n-v, --version           Output the version", 
            "title": "Commands"
        }, 
        {
            "location": "/publishers/cli/#configuration", 
            "text": "Data can be configured using  data config[ure]  command. It will ask you to provide a username, secretToken, server and bitStore addresses of DataHub.  The config is stored in  ~/.datahub/config , you can edit it with text editor.\nSimple example config file can look like this:  username = myname\naccess_token = mykey\nserver = server URL for publishing Eg: https://www.datapackaged.com", 
            "title": "Configuration"
        }, 
        {
            "location": "/publishers/cli/#usage", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/publishers/cli/#publish", 
            "text": "To publish a Data Package, go to the Data Package directory (with  datapackage.json ) and\nrun:  data push  If your configured  username  and  secretToken  are correct, data will\nupload datapackage.json and all relevant resources to the DataHub server.", 
            "title": "Publish"
        }, 
        {
            "location": "/publishers/cli/#get", 
            "text": "To get Data Package run the following command: data get  publisher / package \nNew Data Package will be downloaded into current working directory.", 
            "title": "Get"
        }, 
        {
            "location": "/publishers/cli/#delete", 
            "text": "To delete permanently Data Package from DataHub, you can use  dpm purge  command:  data purge", 
            "title": "Delete"
        }, 
        {
            "location": "/publishers/cli/#information", 
            "text": "You can get information about particular Data Package  data info", 
            "title": "Information"
        }, 
        {
            "location": "/publishers/cli/#normalize", 
            "text": "To normalize Data Package descriptor according to the specs  data norm[alize] [path]", 
            "title": "Normalize"
        }, 
        {
            "location": "/publishers/cli/#validate", 
            "text": "To validate Data Package descriptor against schema  data validate [path | URL]", 
            "title": "Validate"
        }, 
        {
            "location": "/publishers/cli/#configuration_1", 
            "text": "To set up configuration file: data config[ure]", 
            "title": "Configuration"
        }, 
        {
            "location": "/publishers/cli/#links", 
            "text": "Code repo", 
            "title": "Links"
        }, 
        {
            "location": "/publishers/views/", 
            "text": "Views\n\n\n\n\nIntroduction\n\n\nMotivation\n\n\nConcepts and Background\n\n\n\n\n\n\nExamples of using views\n\n\nSimple graph spec\n\n\nVega graphs\n\n\nMaps\n\n\nTables and Transforms\n\n\n\n\n\n\n\n\nIntroduction\n\n\nMotivation\n\n\nProducers and consumers of data [packages] want to have their data presented in tables and graphs -- \"views\" on the data.\n\n\nWhy? For a range of reasons -- from simple eyeballing to drawing out key insights.\n\n\n\ngraph LR\n  data[Your Data] --> table[Table]\n  data --> grap[Graph]\n\n  classDef implemented fill:lightblue,stroke:#333,stroke-width:4px;\n  class abc implemented;\n\n\n\n\nTo achieve this we need to provide:\n\n\n\n\nA tool-chain to create these views from the data.\n\n\nA descriptive language for specifying views such as tables, graphs, map.\n\n\n\n\nThese requirements are addressed through the introduction of Data Package \"Views\" and associated tooling.\n\n\n\ngraph LR\n\n  subgraph Data Package\n    resource[Resource]\n    view[View]\n    resource -.-> view\n  end\n\n  view --> toolchain\n  toolchain --> svg[\"Rendered Graph (SVG)\"]\n  toolchain --> table[Table]\n\n\n\n\nWe take a \"running code\" approach -- as in the rest of the Frictionless Data effort. We develop the spec by building a working implementation, and iterate around the spec as we develop. As development progresses the spec crystallizes and becomes ever less subject to change.\n\n\nOur current implementation efforts focus on provide javascript tools and building a data package registry (DPR).\n\n\nConcepts and Background\n\n\nTo generate visualizations you usually want the following 3 types of information:\n\n\n\n\nmetadata: e.g. title of graph, credits etc\n\n\ngraph: description / specification of the graph itself\n\n\ndata: specification of data sources for the graph including location and key metadata like types\n\n\n\n\nThe data spec itself often consists of three distinct parts:\n\n\n\n\n\"raw / graph data\": a spec / description of data exactly in the form needed by the visualization system. This is often a very well defined spec e.g. an array of series.\n\n\nlocate/describe: a spec of where to get data from e.g. \nurl\n or \ndata\n attribute plus some information on that data such as format and types.\n\n\ntransform: a spec of how transform external data prior to use e.g. pivoting or filtering it\n\n\n\n\nFrom this description it should be clear that the latter two data specs -- locate/describe and transform -- are actually generic and independent of the specific graphing library. The only thing the graphing library really needs is a clear description of the \"raw\" format which it directly consumes. Thus, we can consider a natural grouping of specs as:\n\n\n\n\ngeneral-metadata - e.g. title of graph, credits etc [provided by e.g. Data Package / define yourself!]\n\n\ndata: sourcing and transform [provided by e.g. Data Resource]\n\n\nsourcing: how to source data from external sources\n\n\ntransform: how to transform data e.g. pivot it, select one field, scale a field etc\n\n\n\n\n\n\ngraph description / specification [provided by e.g. Vega]\n\n\ngraph data (raw): data as directly consumed by graph spec (usually JSON based if we are talking about JS web-based visualization)\n\n\n\n\n\n\n\n\nHowever, in many visualization tools -- including specs like Vega -- these items are combined together. This is understandable as these tools seek to off users a \"complete solution\". However, \ndecoupling these parts and having clearly defined interfaces would offer significant benefits\n:\n\n\n\n\nExtensibility: it would be easier to extend and adapt the system. For example, adding new data import options could be done without changing the graph system.\n\n\nComposability: we can combine different parts together in different ways. For example, data import and transformation could be used for generating data for tabular display as well as graphing.\n\n\nReusability: we want to reuse existing tools and specifications wherever possible. If we keep the specs relatively separate we can reuse the best spec for each job.\n\n\nReliability: when the system is decoupled it is easier to test and check.\n\n\n\n\nIn summary, a smaller pieces, loosely joined makes it easier to adapt and evolve the specs and the associated tooling.\n\n\nExamples of using views\n\n\nIn this section, examples of using Data Package views are provided. Each example has a README section with small tutorial.\n\n\nSimple graph spec\n\n\nSimple graph spec is the easiest and quickest way to specify a view in a Data Package. Using simple graph spec publishers can generate graphs, e.g., line and bar charts.\n\n\n\n\nSimple Graph Spec Tutorial\n\n\n\n\nVega graphs\n\n\nPublishers can also describe graphs using Vega specifications:\n\n\n\n\nVega Graph Spec Tutorial - Yields of Barley\n\n\nVega Graph Spec Tutorial - US presidents\n\n\nVega Graph Spec Tutorial - US Airports\n\n\n\n\nMaps\n\n\nAt the moment, we only support \n.geojson\n format:\n\n\n\n\nGeoJSON Tutorial\n\n\n\n\nTables and Transforms\n\n\nIn the following examples, we demonstrate how transforms can be used in Data Package views. Transformed data will be displayed as table views.\n\n\n\n\nFilter \n Formula\n\n\nSample\n\n\nAggregate", 
            "title": "Views"
        }, 
        {
            "location": "/publishers/views/#views", 
            "text": "Introduction  Motivation  Concepts and Background    Examples of using views  Simple graph spec  Vega graphs  Maps  Tables and Transforms", 
            "title": "Views"
        }, 
        {
            "location": "/publishers/views/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/publishers/views/#motivation", 
            "text": "Producers and consumers of data [packages] want to have their data presented in tables and graphs -- \"views\" on the data.  Why? For a range of reasons -- from simple eyeballing to drawing out key insights.  \ngraph LR\n  data[Your Data] --> table[Table]\n  data --> grap[Graph]\n\n  classDef implemented fill:lightblue,stroke:#333,stroke-width:4px;\n  class abc implemented;  To achieve this we need to provide:   A tool-chain to create these views from the data.  A descriptive language for specifying views such as tables, graphs, map.   These requirements are addressed through the introduction of Data Package \"Views\" and associated tooling.  \ngraph LR\n\n  subgraph Data Package\n    resource[Resource]\n    view[View]\n    resource -.-> view\n  end\n\n  view --> toolchain\n  toolchain --> svg[\"Rendered Graph (SVG)\"]\n  toolchain --> table[Table]  We take a \"running code\" approach -- as in the rest of the Frictionless Data effort. We develop the spec by building a working implementation, and iterate around the spec as we develop. As development progresses the spec crystallizes and becomes ever less subject to change.  Our current implementation efforts focus on provide javascript tools and building a data package registry (DPR).", 
            "title": "Motivation"
        }, 
        {
            "location": "/publishers/views/#concepts-and-background", 
            "text": "To generate visualizations you usually want the following 3 types of information:   metadata: e.g. title of graph, credits etc  graph: description / specification of the graph itself  data: specification of data sources for the graph including location and key metadata like types   The data spec itself often consists of three distinct parts:   \"raw / graph data\": a spec / description of data exactly in the form needed by the visualization system. This is often a very well defined spec e.g. an array of series.  locate/describe: a spec of where to get data from e.g.  url  or  data  attribute plus some information on that data such as format and types.  transform: a spec of how transform external data prior to use e.g. pivoting or filtering it   From this description it should be clear that the latter two data specs -- locate/describe and transform -- are actually generic and independent of the specific graphing library. The only thing the graphing library really needs is a clear description of the \"raw\" format which it directly consumes. Thus, we can consider a natural grouping of specs as:   general-metadata - e.g. title of graph, credits etc [provided by e.g. Data Package / define yourself!]  data: sourcing and transform [provided by e.g. Data Resource]  sourcing: how to source data from external sources  transform: how to transform data e.g. pivot it, select one field, scale a field etc    graph description / specification [provided by e.g. Vega]  graph data (raw): data as directly consumed by graph spec (usually JSON based if we are talking about JS web-based visualization)     However, in many visualization tools -- including specs like Vega -- these items are combined together. This is understandable as these tools seek to off users a \"complete solution\". However,  decoupling these parts and having clearly defined interfaces would offer significant benefits :   Extensibility: it would be easier to extend and adapt the system. For example, adding new data import options could be done without changing the graph system.  Composability: we can combine different parts together in different ways. For example, data import and transformation could be used for generating data for tabular display as well as graphing.  Reusability: we want to reuse existing tools and specifications wherever possible. If we keep the specs relatively separate we can reuse the best spec for each job.  Reliability: when the system is decoupled it is easier to test and check.   In summary, a smaller pieces, loosely joined makes it easier to adapt and evolve the specs and the associated tooling.", 
            "title": "Concepts and Background"
        }, 
        {
            "location": "/publishers/views/#examples-of-using-views", 
            "text": "In this section, examples of using Data Package views are provided. Each example has a README section with small tutorial.", 
            "title": "Examples of using views"
        }, 
        {
            "location": "/publishers/views/#simple-graph-spec", 
            "text": "Simple graph spec is the easiest and quickest way to specify a view in a Data Package. Using simple graph spec publishers can generate graphs, e.g., line and bar charts.   Simple Graph Spec Tutorial", 
            "title": "Simple graph spec"
        }, 
        {
            "location": "/publishers/views/#vega-graphs", 
            "text": "Publishers can also describe graphs using Vega specifications:   Vega Graph Spec Tutorial - Yields of Barley  Vega Graph Spec Tutorial - US presidents  Vega Graph Spec Tutorial - US Airports", 
            "title": "Vega graphs"
        }, 
        {
            "location": "/publishers/views/#maps", 
            "text": "At the moment, we only support  .geojson  format:   GeoJSON Tutorial", 
            "title": "Maps"
        }, 
        {
            "location": "/publishers/views/#tables-and-transforms", 
            "text": "In the following examples, we demonstrate how transforms can be used in Data Package views. Transformed data will be displayed as table views.   Filter   Formula  Sample  Aggregate", 
            "title": "Tables and Transforms"
        }
    ]
}