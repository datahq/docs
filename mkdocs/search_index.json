{
    "docs": [
        {
            "location": "/", 
            "text": "DataHub Documentation\n\n\nWelcome to the DataHub documentation. Choose the appropriate section and dive right in!\n\n\nSections\n\n\n\n\nFor developers\n: \n3 Python, JavaScript and data pipelines? Start here!\n\n\nFor publishers\n: Want to store your data on DataHub? Start here!", 
            "title": "Home"
        }, 
        {
            "location": "/#datahub-documentation", 
            "text": "Welcome to the DataHub documentation. Choose the appropriate section and dive right in!", 
            "title": "DataHub Documentation"
        }, 
        {
            "location": "/#sections", 
            "text": "For developers :  3 Python, JavaScript and data pipelines? Start here!  For publishers : Want to store your data on DataHub? Start here!", 
            "title": "Sections"
        }, 
        {
            "location": "/developers/", 
            "text": "Developers\n\n\nThis section of the DataHub documentation is for developers. Here you can learn about the design of the platform and how to get DataHub running locally or on your own servers, and the process for contributing enhancements and bug fixes to the code.\n\n\n\n\nWe use following GitHub repositories for DataHub platform:\n\n\n\n\nDEPLOY\n - Automated deployment\n\n\nFRONTEND\n - Frontend application in node.js\n\n\nASSEMBLER\n - Data assembly line\n\n\nAUTH\n - A generic OAuth2 authentication service and user permission manager.\n\n\nSPECSTORE\n - API server for managing a Source Spec Registry\n\n\nBITSTORE\n - A microservice for storing blobs i.e. files.\n\n\nDOCS\n - Documentations\n\n\n\n\n\ngraph TD\n\nsubgraph Repos\n  frontend[Frontend]\n  assembler[Assembler]\n  auth[Auth]\n  specstore[Specstore]\n  bitstore[Bitstore]\n  docs[Docs]\nend\n\nsubgraph Sites\n  dhio[datahub.io]\n  dhdocs[docs.datahub.io]\n  docs --> dhdocs\nend\n\ndeploy((DEPLOY))\ndeploy --> dhio\nfrontend --> deploy\nassembler --> deploy\nauth --> deploy\nspecstore --> deploy\nbitstore --> deploy\n\n\n\n\n\nInstall\n\n\nWe use several different services to run our platform, please follow the installation instructions here:\n\n\n\n\n\n\nInstall Assembler\n\n\n\n\n\n\nInstall Auth\n\n\n\n\n\n\nInstall Specstore\n\n\n\n\n\n\nInstall Bitstore\n\n\n\n\n\n\nInstall DataHub-CLI\n\n\n\n\n\n\nDeploy\n\n\nFor deployment of the application in a production environment, please see \nthe deploy page\n.\n\n\nAuthorization\n\n\nThe authorization set up enables system to restricts user permission to execute.\n\n\nAuthorization docs\n\n\nAuthentication\n\n\nSome DataHub API methods require client to provide user identity. API Client can use JWT token to perform authenticated requests.\n\n\nAuthentication docs\n\n\nDataHub CLI\n\n\nThe DataHub CLI is a Node JS lib and command line interface to interact with an DataHub instance.\n\n\nCLI code", 
            "title": "Getting started"
        }, 
        {
            "location": "/developers/#developers", 
            "text": "This section of the DataHub documentation is for developers. Here you can learn about the design of the platform and how to get DataHub running locally or on your own servers, and the process for contributing enhancements and bug fixes to the code.   We use following GitHub repositories for DataHub platform:   DEPLOY  - Automated deployment  FRONTEND  - Frontend application in node.js  ASSEMBLER  - Data assembly line  AUTH  - A generic OAuth2 authentication service and user permission manager.  SPECSTORE  - API server for managing a Source Spec Registry  BITSTORE  - A microservice for storing blobs i.e. files.  DOCS  - Documentations   \ngraph TD\n\nsubgraph Repos\n  frontend[Frontend]\n  assembler[Assembler]\n  auth[Auth]\n  specstore[Specstore]\n  bitstore[Bitstore]\n  docs[Docs]\nend\n\nsubgraph Sites\n  dhio[datahub.io]\n  dhdocs[docs.datahub.io]\n  docs --> dhdocs\nend\n\ndeploy((DEPLOY))\ndeploy --> dhio\nfrontend --> deploy\nassembler --> deploy\nauth --> deploy\nspecstore --> deploy\nbitstore --> deploy", 
            "title": "Developers"
        }, 
        {
            "location": "/developers/#install", 
            "text": "We use several different services to run our platform, please follow the installation instructions here:    Install Assembler    Install Auth    Install Specstore    Install Bitstore    Install DataHub-CLI", 
            "title": "Install"
        }, 
        {
            "location": "/developers/#deploy", 
            "text": "For deployment of the application in a production environment, please see  the deploy page .", 
            "title": "Deploy"
        }, 
        {
            "location": "/developers/#authorization", 
            "text": "The authorization set up enables system to restricts user permission to execute.  Authorization docs", 
            "title": "Authorization"
        }, 
        {
            "location": "/developers/#authentication", 
            "text": "Some DataHub API methods require client to provide user identity. API Client can use JWT token to perform authenticated requests.  Authentication docs", 
            "title": "Authentication"
        }, 
        {
            "location": "/developers/#datahub-cli", 
            "text": "The DataHub CLI is a Node JS lib and command line interface to interact with an DataHub instance.  CLI code", 
            "title": "DataHub CLI"
        }, 
        {
            "location": "/developers/platform/", 
            "text": "Platform\n\n\nThe DataHub platform has been designed as a set of loosely coupled components, each performing distinct functions related to the platform as a whole.\n\n\n\n\nArchitecture\n\n\nDomain Model\n\n\nProfile\n\n\nPackage\n\n\n\n\n\n\n\n\nArchitecture\n\n\n\n\ngraph TD\n\n\n  cli((CLI fa:fa-user))\n  auth[Auth Service]\n  cli --login--> auth\n\n\n    cli --store--> raw[Raw Store API\n+ Storage]  \n\n    cli --package-info--> pipeline-store\n  raw --data resource--> pipeline-runner\n\n  pipeline-store -.generate.-> pipeline-runner\n\n  pipeline-runner --> package[Package Storage]\n    package --api--> frontend[Frontend]\n  frontend --> user[User fa:fa-user]\n\n\n\n  package -.publish.->metastore[MetaStore]\n  pipeline-store -.publish.-> metastore[MetaStore]\n  metastore[MetaStore] --api--> frontend\n\n\n\n\n\n\n\nDataHub-CLI\n - Command Line Interface for publishing \nData Packages\n\n\nFront-end Web Application\n - Core part of platform - API, Login \n Sign-Up and Browse \n Search (page not yet implemented)\n\n\nViews and Renderer\n - JS Library responsible for visualization and views on platform\n\n\n\n\nRaw Storage\n\n\nWe first save all raw files before sending to pipeline-runner.\n\nPipeline-runner\n is a service that runs the data package pipelines. It is used to normalise and modify the data before it is displayed publicly\n\n\n\n\nWe use AWS S3 instance for storing data\n\n\n\n\nPackage Storage\n\n\nWe store files after passing pipeline-runner\n\n\n\n\nWe use AWS S3 instance for storing data\n\n\n\n\nBitStore\n\n\nWe are preserving the data byte by byte.\n\n\n\n\nWe use AWS S3 instance for storing data\n\n\n\n\nMetaStore\n\n\nThe MetaStore stores Data Package meta-data along with other management information like publishers, users and permissions.\n\n\nWe use AWS RDS Postgresql database for storing meta-data.\n\n\nUsers and Permissions\n\n\n\n\nWe are using GitHub auth API for authenticating users on our platform. See more information on \nauthentication page\n\n\nWe have a standard access control matrix with 3 axes for authorization. See more information on\n\nauthorization page\n\n\n\n\nDomain model\n\n\nThere are two main concepts to understand in DataHub domain model - \nProfile\n and \nPackage\n\n\n\ngraph TD\n\npkg[Data Package]\nresource[Resource]\nfile[File]\nversion[Version]\nuser[User]\npublisher[Publisher]\n\nsubgraph Package\n  pkg --0..*--> resource\n  resource --1..*--> file\n  pkg --> version\nend\n\nsubgraph Profile\n  publisher --1..*--> user\n  publisher --0..*--> pkg\nend\n\n\n\n\nProfile\n\n\nSet of an authenticated and authorized entities like publishers and users. They are responsible for publishing, deleting or maintaining data on platform.\n\n\nImportant:\n Users do not have Data Packages, Publishers do. Users are \nmembers\n of Publishers.\n\n\nPublisher\n\n\nPublisher is an organization which \"owns\" Data Packages. Publisher may have zero or more Data Packages. Publisher may also have one or more user.\n\n\nUser\n\n\nUser is an authenticated entity, that is member of Publisher organization, that can read, edit, create or delete data packages depending on their permissions.\n\n\nPackage\n\n\nSet of Data Packages published under publisher name.\n\n\nData Package\n\n\nA Data Package is a simple way of \u201cpackaging\u201d up and describing data so that it can be easily shared and used. You can imagine as collection of data and and it's meta-data (\ndatapackage.json\n), usually covering some concrete topic Eg: \n\"Gold Prices\"\n or \n\"Population Growth Rate In My country\"\n etc.\n\n\nEach Data Package may have zero or more resources and one or more versions.\n\n\nResources\n - think like \"tables\" - Each can map to one or more physical files (usually just one). Think of a data table split into multiple CSV files on disk.\n\n\nVersion of a Data Package\n - similar to git commits and tags. People can mean different things by a \"Version\":\n\n\n\n\nTag - Same as label or version - a nice human usable label e.g. \n\"v0.3\"\n, \n\"master\"\n, \n\"2013\"\n\n\nCommit/Hash - Corresponds to the hash of datapackage.json, with that datapackage.json including all hashes of all data files\n\n\n\n\nWe interpret Version as \n\"Tag\"\n concept. \n\"Commit/Hash\"\n is not supported", 
            "title": "Platform"
        }, 
        {
            "location": "/developers/platform/#platform", 
            "text": "The DataHub platform has been designed as a set of loosely coupled components, each performing distinct functions related to the platform as a whole.   Architecture  Domain Model  Profile  Package", 
            "title": "Platform"
        }, 
        {
            "location": "/developers/platform/#architecture", 
            "text": "graph TD\n\n\n  cli((CLI fa:fa-user))\n  auth[Auth Service]\n  cli --login--> auth\n\n\n    cli --store--> raw[Raw Store API + Storage]  \n\n    cli --package-info--> pipeline-store\n  raw --data resource--> pipeline-runner\n\n  pipeline-store -.generate.-> pipeline-runner\n\n  pipeline-runner --> package[Package Storage]\n    package --api--> frontend[Frontend]\n  frontend --> user[User fa:fa-user]\n\n\n\n  package -.publish.->metastore[MetaStore]\n  pipeline-store -.publish.-> metastore[MetaStore]\n  metastore[MetaStore] --api--> frontend   DataHub-CLI  - Command Line Interface for publishing  Data Packages  Front-end Web Application  - Core part of platform - API, Login   Sign-Up and Browse   Search (page not yet implemented)  Views and Renderer  - JS Library responsible for visualization and views on platform", 
            "title": "Architecture"
        }, 
        {
            "location": "/developers/platform/#raw-storage", 
            "text": "We first save all raw files before sending to pipeline-runner. Pipeline-runner  is a service that runs the data package pipelines. It is used to normalise and modify the data before it is displayed publicly   We use AWS S3 instance for storing data", 
            "title": "Raw Storage"
        }, 
        {
            "location": "/developers/platform/#package-storage", 
            "text": "We store files after passing pipeline-runner   We use AWS S3 instance for storing data", 
            "title": "Package Storage"
        }, 
        {
            "location": "/developers/platform/#bitstore", 
            "text": "We are preserving the data byte by byte.   We use AWS S3 instance for storing data", 
            "title": "BitStore"
        }, 
        {
            "location": "/developers/platform/#metastore", 
            "text": "The MetaStore stores Data Package meta-data along with other management information like publishers, users and permissions.  We use AWS RDS Postgresql database for storing meta-data.", 
            "title": "MetaStore"
        }, 
        {
            "location": "/developers/platform/#users-and-permissions", 
            "text": "We are using GitHub auth API for authenticating users on our platform. See more information on  authentication page  We have a standard access control matrix with 3 axes for authorization. See more information on authorization page", 
            "title": "Users and Permissions"
        }, 
        {
            "location": "/developers/platform/#domain-model", 
            "text": "There are two main concepts to understand in DataHub domain model -  Profile  and  Package  \ngraph TD\n\npkg[Data Package]\nresource[Resource]\nfile[File]\nversion[Version]\nuser[User]\npublisher[Publisher]\n\nsubgraph Package\n  pkg --0..*--> resource\n  resource --1..*--> file\n  pkg --> version\nend\n\nsubgraph Profile\n  publisher --1..*--> user\n  publisher --0..*--> pkg\nend", 
            "title": "Domain model"
        }, 
        {
            "location": "/developers/platform/#profile", 
            "text": "Set of an authenticated and authorized entities like publishers and users. They are responsible for publishing, deleting or maintaining data on platform.  Important:  Users do not have Data Packages, Publishers do. Users are  members  of Publishers.", 
            "title": "Profile"
        }, 
        {
            "location": "/developers/platform/#publisher", 
            "text": "Publisher is an organization which \"owns\" Data Packages. Publisher may have zero or more Data Packages. Publisher may also have one or more user.", 
            "title": "Publisher"
        }, 
        {
            "location": "/developers/platform/#user", 
            "text": "User is an authenticated entity, that is member of Publisher organization, that can read, edit, create or delete data packages depending on their permissions.", 
            "title": "User"
        }, 
        {
            "location": "/developers/platform/#package", 
            "text": "Set of Data Packages published under publisher name.", 
            "title": "Package"
        }, 
        {
            "location": "/developers/platform/#data-package", 
            "text": "A Data Package is a simple way of \u201cpackaging\u201d up and describing data so that it can be easily shared and used. You can imagine as collection of data and and it's meta-data ( datapackage.json ), usually covering some concrete topic Eg:  \"Gold Prices\"  or  \"Population Growth Rate In My country\"  etc.  Each Data Package may have zero or more resources and one or more versions.  Resources  - think like \"tables\" - Each can map to one or more physical files (usually just one). Think of a data table split into multiple CSV files on disk.  Version of a Data Package  - similar to git commits and tags. People can mean different things by a \"Version\":   Tag - Same as label or version - a nice human usable label e.g.  \"v0.3\" ,  \"master\" ,  \"2013\"  Commit/Hash - Corresponds to the hash of datapackage.json, with that datapackage.json including all hashes of all data files   We interpret Version as  \"Tag\"  concept.  \"Commit/Hash\"  is not supported", 
            "title": "Data Package"
        }, 
        {
            "location": "/developers/deploy/", 
            "text": "DevOps - Production Deployment\n\n\nWe use various cloud services for the platform, for example AWS S3 for storing data and metadata, and the application runs on Docker Cloud.\n\n\nWe have fully automated the deployment of the platform including the setup of all necessary services so that it is one command to deploy. Code and instructions here:\n\n\nhttps://github.com/datahq/deploy\n\n\nBelow we provide a conceptual outline.\n\n\nOutline - Conceptually\n\n\n\ngraph TD\n\n  user[fa:fa-user User] --> frontend[Frontend]\n  frontend --> apiproxy[API Proxy]\n  frontend --> bits[BitStore - S3]\n\n\n\n\nNew Structure\n\n\nThis diagram shows the current deployment architecture.\n\n\n\ngraph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> docker\n  cloudflare --> bits\n  docker[Docker - Node JS Application] --> apiproxy[API Proxy]\n  docker --> bits\n\n\n\n\nOld Structures\n\n\nHeroku\n\n\n\ngraph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> heroku\n  cloudflare --> bits\n  heroku[Heroku - Flask] --> rds[RDS Database]\n  heroku --> bits\n\n\n\n\nAWS Lambda - Flask via Zappa\n\n\nWe are no longer using AWS and Heroku in this way. However, we have kept this for historical purposes and in case we return to any of them.\n\n\n\ngraph TD\n\n  user[fa:fa-user User] --> cloudfront[Cloudfront]\n  cloudfront --> apigateway[API Gateway]\n  apigateway --> lambda[AWS Lambda - Flask via Zappa]\n  cloudfront --> s3assets[S3 Assets]\n  lambda --> rds[RDS Database]\n  lambda --> bits[BitStore]\n  cloudfront --> bits", 
            "title": "Deploy"
        }, 
        {
            "location": "/developers/deploy/#devops-production-deployment", 
            "text": "We use various cloud services for the platform, for example AWS S3 for storing data and metadata, and the application runs on Docker Cloud.  We have fully automated the deployment of the platform including the setup of all necessary services so that it is one command to deploy. Code and instructions here:  https://github.com/datahq/deploy  Below we provide a conceptual outline.", 
            "title": "DevOps - Production Deployment"
        }, 
        {
            "location": "/developers/deploy/#outline-conceptually", 
            "text": "graph TD\n\n  user[fa:fa-user User] --> frontend[Frontend]\n  frontend --> apiproxy[API Proxy]\n  frontend --> bits[BitStore - S3]", 
            "title": "Outline - Conceptually"
        }, 
        {
            "location": "/developers/deploy/#new-structure", 
            "text": "This diagram shows the current deployment architecture.  \ngraph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> docker\n  cloudflare --> bits\n  docker[Docker - Node JS Application] --> apiproxy[API Proxy]\n  docker --> bits", 
            "title": "New Structure"
        }, 
        {
            "location": "/developers/deploy/#old-structures", 
            "text": "", 
            "title": "Old Structures"
        }, 
        {
            "location": "/developers/deploy/#heroku", 
            "text": "graph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> heroku\n  cloudflare --> bits\n  heroku[Heroku - Flask] --> rds[RDS Database]\n  heroku --> bits", 
            "title": "Heroku"
        }, 
        {
            "location": "/developers/deploy/#aws-lambda-flask-via-zappa", 
            "text": "We are no longer using AWS and Heroku in this way. However, we have kept this for historical purposes and in case we return to any of them.  \ngraph TD\n\n  user[fa:fa-user User] --> cloudfront[Cloudfront]\n  cloudfront --> apigateway[API Gateway]\n  apigateway --> lambda[AWS Lambda - Flask via Zappa]\n  cloudfront --> s3assets[S3 Assets]\n  lambda --> rds[RDS Database]\n  lambda --> bits[BitStore]\n  cloudfront --> bits", 
            "title": "AWS Lambda - Flask via Zappa"
        }, 
        {
            "location": "/developers/api/", 
            "text": "DataHub API\n\n\nThe DataHub API provides a range of endpoints to interact with the platform. All endpoints live under the URL \nhttps://api.datahub.io\n where our API is divided into the following sections: \nauth, rawstore, sources\n \n\n\nAuth\n\n\nA generic OAuth2 authentication service and user permission manager. \n\n\nCheck an authentication token's validity\n\n\n/auth/check\n\n\nMethod:\n \nGET\n\n\nQuery Parameters:\n\n\n\n\njwt\n - authentication token\n\n\nnext\n - URL to redirect to when finished authentication\n\n\n\n\nReturns:\n\n\nIf authenticated:\n\n\n{\n    \nauthenticated\n: true,\n    \nprofile\n: {\n        \nid\n: \nuser-id\n,\n        \nname\n: \nuser-name\n,\n        \nemail\n: \nuser-email\n,\n        \navatar_url\n: \nurl-for-user's-profile-photo\n,\n        \nidhash\n: \nunique-id-of-the-user\n,\n        \nusername\n: \nuser-selected-id\n // If user has a username\n    }\n}\n\n\n\nIf not:\n\n\n{\n    \nauthenticated\n: false,\n    \nproviders\n: {\n        \ngoogle\n: {\n            \nurl\n: \nurl-for-logging-in-with-the-Google-provider\n\n        },\n        \ngithub\n: {\n            \nurl\n: \nurl-for-logging-in-with-the-Github-provider\n\n        },\n    }\n}\n\n\n\nWhen the authentication flow is finished, the caller will be redirected to the \nnext\n URL with an extra query parameter\n\njwt\n which contains the authentication token. The caller should cache this token for further interactions with the API.\n\n\nGet permission for a service\n\n\n/user/authorize\n\n\nMethod:\n \nGET\n\n\nQuery Parameters:\n\n\n\n\njwt\n - user token (received from \n/user/check\n)\n\n\nservice\n - the relevant service (e.g. \nstorage-service\n)\n\n\n\n\nReturns:\n\n\n{\n    \ntoken\n: \ntoken-for-the-relevant-service\n\n    \nuserid\n: \nunique-id-of-the-user\n,\n    \npermissions\n: {\n        \npermission-x\n: true,\n        \npermission-y\n: false\n    },\n    \nservice\n: \nrelevant-service\n\n}\n\n\n\nChange the username\n\n\n/user/update\n\n\nMethod:\n \nPOST\n\n\nQuery Parameters:\n\n\n\n\njwt\n - authentication token (received from \n/user/check\n)\n\n\nusername\n - A new username for the user profile (this action is only allowed once)\n\n\n\n\nReturns:\n\n\n{\n    \nsuccess\n: true,\n    \nerror\n: \nerror-message-if-applicable\n\n}\n\n\nNote\n: trying to update other user profile fields like \nemail\n will fail silently and return\n\n\n{ \n    \nsuccess\n: true\n}\n\n\n\nReceive authorization public key\n\n\n/user/public-key\n\n\nMethod:\n \nGET\n\n\nReturns:\n\n\nThe service's public key in PEM format.\n\n\nCan be used by services to validate that the permission token is authentic.\n\n\nRawstore\n\n\nGet authorized upload URL(s)\n\n\n/authorize\n\n\nMethod:\n \nPOST\n\n\nQuery Parameters:\n\n\n\n\njwt\n - permission token (received from \n/user/authorize\n)\n\n\n\n\nHeaders:\n\n\n\n\nAuth-Token\n - permission token (received from conductor)\n\n\n\n\nBody:\n\n\n{\n    \nmetadata\n: {\n        \nowner\n: \nuser-id-of-uploader\n,\n        \nname\n: \ndata-set-unique-id\n\n    },\n    \nfiledata\n: {\n        \nrelative-path-to-file-in-package-1\n: {\n            \nlength\n: 1234, //length in bytes of data\n            \nmd5\n: \nmd5-hash-of-the-data\n,\n            \ntype\n: \ncontent-type-of-the-data\n,\n            \nname\n: \nfile-name\n\n        },\n        \nrelative-path-to-file-in-package-2\n: {\n            \nlength\n: 4321,\n            \nmd5\n: \nmd5-hash-of-the-data\n,\n            \ntype\n: \ncontent-type-of-the-data\n,\n            \nname\n: \nfile-name\n\n        }\n        ...\n    }\n}\n\n\n\nReturns:\n\n\nSigned urls to upload into S3:\n\n\n{\n  fileData: {\n    \nfile-name-1\n: {\n      \nmd5-hash\n: \n...\n,\n      \nname\n: \nfile-name\n,\n      \ntype\n: \nfile-type\n,\n      \nupload_query\n: {\n        'Content-MD5': '...',\n        'Content-Type': '...',\n        'acl': 'public-read',\n        'key': '\npath\n',\n        'policy': '...',\n        'x-amz-algorithm': 'AWS4-HMAC-SHA256',\n        'x-amz-credential': '...',\n        'x-amz-date': '\ndate-time-in-ISO',\n        'x-amz-signature': '...'\n      },\n      \nupload_url\n: \ns3-url\n\n    },\n    \nfile-name-2\n: ...,\n    ...\n  }\n}\n\n\n\n\nGet information regarding the datastore\n\n\n/info\n\n\nMethod:\n \nGET\n\n\nQuery Parameters:\n\n\n\n\njwt\n - permission token (received from \n/user/authorize\n)\n\n\n\n\nHeaders:\n\n\n\n\nAuth-Token\n - permission token (can be used instead of the \njwt\n query parameter)\n\n\n\n\nReturns:\n\n\nJSON content with the following structure:\n\n{\n    \nprefixes\n: [\n        \nhttps://api.datahub.io/rawstore/123456789\n,\n        ...\n    ]\n}\n\n\nprefixes\n is the list of possible prefixes for an uploaded file for this user.\n\n\nSources\n\n\nGet status\n\n\n/source/{identifier}/status\n\n\nMethod:\n \nGET\n\n\nReturns:\n\n\n{\n   'state': 'queued/running/errored',\n   'errors': [\n       'error-message', // ...\n   ],\n   'logs': [\n              'log-line', \n              'log-line', // ...\n           ],\n   'history': [\n      {\n       'execution-time': 'iso-time',\n       'success': true or false,\n       'termination-time': 'iso-time' or null\n      }, // ...   \n   ],\n   'outputs': [\n       {\n        'kind': '\nkind\n', \n        'url': '\nurl\n', \n        'created-at': '\niso-time\n',\n        'filename': '\ndisplayable-filename\n',\n        'title': '\ndisplayable-title\n'\n       }\n   ],\n   'stats': {\n       'key': 'value' // e.g. 'count-of-rows', etc.\n   }\n}\n\n\n\nUpload\n\n\n/source/upload\n\n\nMethod:\n \nPOST\n\n\nQuery Parameters:\n\n\nHeaders:\n\n\n\n\nAuth-Token\n - permission token (received from conductor)\n\n\n\n\nBody:\n\n\n{\n    'meta': {\n        'version': 1, // version of the _spec_\n        'owner': '\nuser-id\n', // Will be validated on upload\n        'id': '\nid\n'\n    },\n    'inputs': [\n        {\n            'kind': 'datapackage',\n            'url': '\nurl\n',\n            'parameters': {\n                'resource-mapping': {\n                    '\nresource-name\n': '\nresource-url\n'\n                }\n            }\n        }    \n    ],\n    'processing': [\n        {\n            // TBD\n        }\n    ],\n    'outputs': [\n        {\n            'kind': 'datapackage',\n            'parameters': {\n                // e.g. 'create-previews': false\n            }\n        }\n    ]\n}\n\n\n\nReturns:\n\n\n{\n  \nsuccess\n: true,\n  \nid\n: \nidentifier\n\n  \nerrors\n: [\n      \nerror-message\n\n  ]\n}", 
            "title": "API"
        }, 
        {
            "location": "/developers/api/#datahub-api", 
            "text": "The DataHub API provides a range of endpoints to interact with the platform. All endpoints live under the URL  https://api.datahub.io  where our API is divided into the following sections:  auth, rawstore, sources", 
            "title": "DataHub API"
        }, 
        {
            "location": "/developers/api/#auth", 
            "text": "A generic OAuth2 authentication service and user permission manager.", 
            "title": "Auth"
        }, 
        {
            "location": "/developers/api/#check-an-authentication-tokens-validity", 
            "text": "/auth/check  Method:   GET  Query Parameters:   jwt  - authentication token  next  - URL to redirect to when finished authentication   Returns:  If authenticated:  {\n     authenticated : true,\n     profile : {\n         id :  user-id ,\n         name :  user-name ,\n         email :  user-email ,\n         avatar_url :  url-for-user's-profile-photo ,\n         idhash :  unique-id-of-the-user ,\n         username :  user-selected-id  // If user has a username\n    }\n}  If not:  {\n     authenticated : false,\n     providers : {\n         google : {\n             url :  url-for-logging-in-with-the-Google-provider \n        },\n         github : {\n             url :  url-for-logging-in-with-the-Github-provider \n        },\n    }\n}  When the authentication flow is finished, the caller will be redirected to the  next  URL with an extra query parameter jwt  which contains the authentication token. The caller should cache this token for further interactions with the API.", 
            "title": "Check an authentication token's validity"
        }, 
        {
            "location": "/developers/api/#get-permission-for-a-service", 
            "text": "/user/authorize  Method:   GET  Query Parameters:   jwt  - user token (received from  /user/check )  service  - the relevant service (e.g.  storage-service )   Returns:  {\n     token :  token-for-the-relevant-service \n     userid :  unique-id-of-the-user ,\n     permissions : {\n         permission-x : true,\n         permission-y : false\n    },\n     service :  relevant-service \n}", 
            "title": "Get permission for a service"
        }, 
        {
            "location": "/developers/api/#change-the-username", 
            "text": "/user/update  Method:   POST  Query Parameters:   jwt  - authentication token (received from  /user/check )  username  - A new username for the user profile (this action is only allowed once)   Returns:  {\n     success : true,\n     error :  error-message-if-applicable \n}  Note : trying to update other user profile fields like  email  will fail silently and return  { \n     success : true\n}", 
            "title": "Change the username"
        }, 
        {
            "location": "/developers/api/#receive-authorization-public-key", 
            "text": "/user/public-key  Method:   GET  Returns:  The service's public key in PEM format.  Can be used by services to validate that the permission token is authentic.", 
            "title": "Receive authorization public key"
        }, 
        {
            "location": "/developers/api/#rawstore", 
            "text": "", 
            "title": "Rawstore"
        }, 
        {
            "location": "/developers/api/#get-authorized-upload-urls", 
            "text": "/authorize  Method:   POST  Query Parameters:   jwt  - permission token (received from  /user/authorize )   Headers:   Auth-Token  - permission token (received from conductor)   Body:  {\n     metadata : {\n         owner :  user-id-of-uploader ,\n         name :  data-set-unique-id \n    },\n     filedata : {\n         relative-path-to-file-in-package-1 : {\n             length : 1234, //length in bytes of data\n             md5 :  md5-hash-of-the-data ,\n             type :  content-type-of-the-data ,\n             name :  file-name \n        },\n         relative-path-to-file-in-package-2 : {\n             length : 4321,\n             md5 :  md5-hash-of-the-data ,\n             type :  content-type-of-the-data ,\n             name :  file-name \n        }\n        ...\n    }\n}  Returns:  Signed urls to upload into S3:  {\n  fileData: {\n     file-name-1 : {\n       md5-hash :  ... ,\n       name :  file-name ,\n       type :  file-type ,\n       upload_query : {\n        'Content-MD5': '...',\n        'Content-Type': '...',\n        'acl': 'public-read',\n        'key': ' path ',\n        'policy': '...',\n        'x-amz-algorithm': 'AWS4-HMAC-SHA256',\n        'x-amz-credential': '...',\n        'x-amz-date': ' date-time-in-ISO',\n        'x-amz-signature': '...'\n      },\n       upload_url :  s3-url \n    },\n     file-name-2 : ...,\n    ...\n  }\n}", 
            "title": "Get authorized upload URL(s)"
        }, 
        {
            "location": "/developers/api/#get-information-regarding-the-datastore", 
            "text": "/info  Method:   GET  Query Parameters:   jwt  - permission token (received from  /user/authorize )   Headers:   Auth-Token  - permission token (can be used instead of the  jwt  query parameter)   Returns:  JSON content with the following structure: {\n     prefixes : [\n         https://api.datahub.io/rawstore/123456789 ,\n        ...\n    ]\n}  prefixes  is the list of possible prefixes for an uploaded file for this user.", 
            "title": "Get information regarding the datastore"
        }, 
        {
            "location": "/developers/api/#sources", 
            "text": "", 
            "title": "Sources"
        }, 
        {
            "location": "/developers/api/#get-status", 
            "text": "/source/{identifier}/status  Method:   GET  Returns:  {\n   'state': 'queued/running/errored',\n   'errors': [\n       'error-message', // ...\n   ],\n   'logs': [\n              'log-line', \n              'log-line', // ...\n           ],\n   'history': [\n      {\n       'execution-time': 'iso-time',\n       'success': true or false,\n       'termination-time': 'iso-time' or null\n      }, // ...   \n   ],\n   'outputs': [\n       {\n        'kind': ' kind ', \n        'url': ' url ', \n        'created-at': ' iso-time ',\n        'filename': ' displayable-filename ',\n        'title': ' displayable-title '\n       }\n   ],\n   'stats': {\n       'key': 'value' // e.g. 'count-of-rows', etc.\n   }\n}", 
            "title": "Get status"
        }, 
        {
            "location": "/developers/api/#upload", 
            "text": "/source/upload  Method:   POST  Query Parameters:  Headers:   Auth-Token  - permission token (received from conductor)   Body:  {\n    'meta': {\n        'version': 1, // version of the _spec_\n        'owner': ' user-id ', // Will be validated on upload\n        'id': ' id '\n    },\n    'inputs': [\n        {\n            'kind': 'datapackage',\n            'url': ' url ',\n            'parameters': {\n                'resource-mapping': {\n                    ' resource-name ': ' resource-url '\n                }\n            }\n        }    \n    ],\n    'processing': [\n        {\n            // TBD\n        }\n    ],\n    'outputs': [\n        {\n            'kind': 'datapackage',\n            'parameters': {\n                // e.g. 'create-previews': false\n            }\n        }\n    ]\n}  Returns:  {\n   success : true,\n   id :  identifier \n   errors : [\n       error-message \n  ]\n}", 
            "title": "Upload"
        }, 
        {
            "location": "/developers/publish/", 
            "text": "Publish\n\n\nExplanation of DataHub publishing flow from client and back-end perspectives.\n\n\nClient Perspective\n\n\nPublishing flow takes the following steps and processes to communicate with DataHub API:\n\n\n\nsequenceDiagram\nUpload Agent CLI->>Upload Agent CLI: Check Data Package valid\nUpload Agent CLI-->>Auth(SSO): Get Session Token (Sends base auth)\nAuth(SSO)-->>Upload Agent CLI: session token\nUpload Agent CLI->>BitStore Upload Auth: Get BitStore Upload token [send session token]\nBitStore Upload Auth->>Auth(SSO): Check key / token\nAuth(SSO)->>BitStore Upload Auth: OK / Not OK\nBitStore Upload Auth->>Upload Agent CLI: S3 auth Token\nUpload Agent CLI->>Data Storage (S3 Raw): Send file plus token\nData Storage (S3 Raw)->>Upload Agent CLI: OK / Not OK\nUpload Agent CLI->>MetaData Storage API: Finalize (After all data uploaded)\nMetaData Storage API->>Upload Agent CLI: OK / Not OK\n\n\n\n\n\n\n\n\nUpload API - see \nPOST /api/package/upload\n in \npackage\n section of \nAPI\n\n\nAuthentication API - see \nPOST /api/auth/token\n in \nauth\n section of \nAPI\n. Read more \nabout authentication\n\n\n[Authorization API][authz] - see \nPOST /api/datastore/authorize\n in \npackage\n section of \nAPI\n. Read more \nabout authorization\n\n\n\n\nSee example \ncode snippet in dpm-py\n\n\n\n\nBack-end perspective\n\n\nDataHub Metadata and Data Flow\n\n\n\n\nPink = service we build\n\n\nBlue = external service\n\n\nDark gray = not yet implemented\n\n\n\n\n\ngraph TD\n\nuser[Publisher fa:fa-user]\nupload-api[\"Upload API (S3 API)\"]\nbitstore(Bitstore S3)\nmetaingestor[Meta Ingestor]\ndataingestor[Data Ingestor]\nmetastore(\"Metastore (RDS)\")\nreadapi[Read API]\ndataproxy[\"DataProxy (convert raw data to json on the fly)\"]\ndatastore[\"Datastore (RDS)\"]\ns3readapi[S3 Get API]\nreaduser[Consumer fa:fa-user]\n\nuser --s3 signed upload url--> upload-api\nupload-api --> bitstore\nbitstore --> metaingestor\nmetaingestor --> metastore\nmetastore --> readapi\nbitstore -.-> dataproxy\nbitstore -.-> dataingestor\ndataingestor -.-> datastore\ndatastore -.-> readapi\nbitstore --> s3readapi\ns3readapi --> readuser\ndataproxy -.-> readuser\nreadapi --> readuser\n\n  classDef extservice fill:lightblue,stroke:#333,stroke-width:4px;\n  classDef notimplemented fill:darkgrey,stroke:#bbb,stroke-width:1px;\n  classDef service fill:pink,stroke:#333,stroke-width:4px;\n  class datastore,dataingestor,dataproxy notimplemented;\n  class bitstore,metastore,s3readapi extservice;\n  class readapi service;\n\n\n\n\n\n\nAuthentication\n\n\nAuthorization\n\n\nMetastore\n\n\nBitStore", 
            "title": "Publish"
        }, 
        {
            "location": "/developers/publish/#publish", 
            "text": "Explanation of DataHub publishing flow from client and back-end perspectives.", 
            "title": "Publish"
        }, 
        {
            "location": "/developers/publish/#client-perspective", 
            "text": "Publishing flow takes the following steps and processes to communicate with DataHub API:  \nsequenceDiagram\nUpload Agent CLI->>Upload Agent CLI: Check Data Package valid\nUpload Agent CLI-->>Auth(SSO): Get Session Token (Sends base auth)\nAuth(SSO)-->>Upload Agent CLI: session token\nUpload Agent CLI->>BitStore Upload Auth: Get BitStore Upload token [send session token]\nBitStore Upload Auth->>Auth(SSO): Check key / token\nAuth(SSO)->>BitStore Upload Auth: OK / Not OK\nBitStore Upload Auth->>Upload Agent CLI: S3 auth Token\nUpload Agent CLI->>Data Storage (S3 Raw): Send file plus token\nData Storage (S3 Raw)->>Upload Agent CLI: OK / Not OK\nUpload Agent CLI->>MetaData Storage API: Finalize (After all data uploaded)\nMetaData Storage API->>Upload Agent CLI: OK / Not OK    Upload API - see  POST /api/package/upload  in  package  section of  API  Authentication API - see  POST /api/auth/token  in  auth  section of  API . Read more  about authentication  [Authorization API][authz] - see  POST /api/datastore/authorize  in  package  section of  API . Read more  about authorization   See example  code snippet in dpm-py", 
            "title": "Client Perspective"
        }, 
        {
            "location": "/developers/publish/#back-end-perspective", 
            "text": "DataHub Metadata and Data Flow   Pink = service we build  Blue = external service  Dark gray = not yet implemented   \ngraph TD\n\nuser[Publisher fa:fa-user]\nupload-api[\"Upload API (S3 API)\"]\nbitstore(Bitstore S3)\nmetaingestor[Meta Ingestor]\ndataingestor[Data Ingestor]\nmetastore(\"Metastore (RDS)\")\nreadapi[Read API]\ndataproxy[\"DataProxy (convert raw data to json on the fly)\"]\ndatastore[\"Datastore (RDS)\"]\ns3readapi[S3 Get API]\nreaduser[Consumer fa:fa-user]\n\nuser --s3 signed upload url--> upload-api\nupload-api --> bitstore\nbitstore --> metaingestor\nmetaingestor --> metastore\nmetastore --> readapi\nbitstore -.-> dataproxy\nbitstore -.-> dataingestor\ndataingestor -.-> datastore\ndatastore -.-> readapi\nbitstore --> s3readapi\ns3readapi --> readuser\ndataproxy -.-> readuser\nreadapi --> readuser\n\n  classDef extservice fill:lightblue,stroke:#333,stroke-width:4px;\n  classDef notimplemented fill:darkgrey,stroke:#bbb,stroke-width:1px;\n  classDef service fill:pink,stroke:#333,stroke-width:4px;\n  class datastore,dataingestor,dataproxy notimplemented;\n  class bitstore,metastore,s3readapi extservice;\n  class readapi service;   Authentication  Authorization  Metastore  BitStore", 
            "title": "Back-end perspective"
        }, 
        {
            "location": "/developers/views/", 
            "text": "Views\n\n\nTo render Data Packages in browsers we use DataHub views written in JavaScript. The module implemented in ReactJS framework and it can render tables, maps and various graphs using third-party libraries.\n\n\n\n  graph TD\n\n  url[\"metadata URL passed from back-end\"]\n  dp-js[datapackage-js]\n  dprender[datapackage-render-js]\n  table[\"table view\"]\n  chart[\"graph view\"]\n  hot[HandsOnTable]\n  map[LeafletMap]\n  vega[Vega]\n  plotly[Plotly]\n  browser[Browser]\n\n  url --> dp-js\n  dp-js --fetched dp--> dprender\n  dprender --spec--> table\n  table --1..n--> hot\n  dprender --geojson--> map\n  dprender --spec--> chart\n  chart --0..n--> vega\n  chart --0..n--> plotly\n  hot --table--> browser\n  map --map--> browser\n  vega --graph--> browser\n  plotly --graph--> browser\n\n\n\n\nNotice that DataHub views render a table view per tabular resource. If GeoJSON resource is given, it renders a map. Graph views should be specified in \nviews\n property of a Data Package.\n\n\nLinks\n\n\n\n\ndpr-js repo\n\n\ndatapackage-render-js\n\n\nviews specification and analysis", 
            "title": "Views"
        }, 
        {
            "location": "/developers/views/#views", 
            "text": "To render Data Packages in browsers we use DataHub views written in JavaScript. The module implemented in ReactJS framework and it can render tables, maps and various graphs using third-party libraries.  \n  graph TD\n\n  url[\"metadata URL passed from back-end\"]\n  dp-js[datapackage-js]\n  dprender[datapackage-render-js]\n  table[\"table view\"]\n  chart[\"graph view\"]\n  hot[HandsOnTable]\n  map[LeafletMap]\n  vega[Vega]\n  plotly[Plotly]\n  browser[Browser]\n\n  url --> dp-js\n  dp-js --fetched dp--> dprender\n  dprender --spec--> table\n  table --1..n--> hot\n  dprender --geojson--> map\n  dprender --spec--> chart\n  chart --0..n--> vega\n  chart --0..n--> plotly\n  hot --table--> browser\n  map --map--> browser\n  vega --graph--> browser\n  plotly --graph--> browser  Notice that DataHub views render a table view per tabular resource. If GeoJSON resource is given, it renders a map. Graph views should be specified in  views  property of a Data Package.", 
            "title": "Views"
        }, 
        {
            "location": "/developers/views/#links", 
            "text": "dpr-js repo  datapackage-render-js  views specification and analysis", 
            "title": "Links"
        }, 
        {
            "location": "/developers/authorization/", 
            "text": "Authorization Set up\n\n\nAuthorization is the process of giving someone permission to do or have something. In multi-user systems, a system administrator defines for the system which users are allowed access to the system and what privileges of use.\n\n\nWe have a standard access control matrix with 3 axes:\n\n\n\n\nActions: CREATE, READ, WRITE, DELETE, PURGE etc. these can vary among different entities\n\n\nEntities (object): User, Publisher, Package, Package Resource, \u2026\n\n\nUsers: a user or type of user\n\n\n\n\nPermission is a tuple of \n(Users, Entities, Actions)\n\n\nIntroducing Roles\n\n\nIt can be tiresome and inefficient to list for every object all the users permitted to perform a given action. For example:\n\n\n\n\nMany users in an organization get same set of privileges because of their position in the organization.\n\n\nWe want to change the permissions associated with a certain level in the organization and to have those permissions changed for all people in that level\n\n\nA user may change level frequently (ex. user may get promoted)\n\n\n\n\nSo we create roles\n\n\n\n\nPer object roles e.g. Package Owner\n\n\nPer system roles e.g. System Administrator\n\n\nA list or algorithm for assigning Users =\n Roles\n\n\n\n\nAccess control algorithm:\n\nis_allowed(user, entity, action)\n\n\nFor this user: what roles do they have related to this entity and the system?\nGiven those roles: what actions do they have: UNIONrole\n\n\nNote: it would get more complex if some roles deny access. E.g. Role: Spammer might mean you are denied action to posting etc. Right now we don\u2019t have that issue.\n\n\nIs the desired action in that set?\n\n\nRoles\n\n\nThe example roles are given below.\n\n\n\n\nPackage\n\n\nOwner  =\n all actions\n\n\nEditor\n\n\nRead\n\n\nCreate\n\n\nDelete\n\n\nUndelete\n\n\nUpdate\n\n\nTag\n\n\n\n\n\n\nViewer  =\n Only read\n\n\n\n\n\n\nPublisher\n\n\nOwner =\n all actions on Publisher\n\n\nEditor\n\n\nViewMemberList\n\n\nAddMember\n\n\nRemoveMember\n\n\nRead\n\n\n\n\n\n\nViewer =\n Only Read\n\n\n\n\n\n\nSystem\n\n\nLoggedIn\n\n\nPackage::Create\n\n\nPublisher::Create\n\n\n\n\n\n\nAll =\n Package::Read on public packages\n\n\nSysadmin =\n all actions\n\n\n\n\n\n\n\n\nThis\n contains the current roles.\n\n\nBusiness roles\n\n\n\n\nPublisher Owner\n\n\nPublisher::Owner\n\n\n\n\n\n\nPublisher Member\n\n\nPublisher::Editor\n\n\n\n\n\n\n(Logged in) User\n\n\nSystem::LoggedIn\n\n\n\n\n\n\nSys Admin\n\n\nSystem::Sysadmin\n\n\n\n\n\n\nVisitor\n\n\nSystem::Anonymous\n\n\n\n\n\n\n\n\n\n\nNOTE: business roles and authorization roles are distinct. Of course, in implementing access control we will use the business logic inherent in business roles. However, business roles are not explicitly present in the access control system.\n\n\n\n\nActions\n\n\n\n\nNote: not an exhaustive list. \nThis\n contains the current Actions.\n\n\n\n\n\n\nPackage:\n\n\nPackage::Read\n\n\nPackage::Create\n\n\nPackage::Delete\n\n\nPackage::Undelete\n\n\nPackage::Purge\n\n\nPackage::Update\n\n\nPackage::Tag\n\n\n\n\n\n\nPublisher:\n\n\nPublisher::Create\n\n\nPublisher::AddMember\n\n\nPublisher::RemoveMember\n\n\nPublisher::Read\n\n\nPublisher::Delete\n\n\nPublisher::Update\n\n\nPublisher::ViewMemberList\n\n\n\n\n\n\n\n\nExamples\n\n\nFirst time visitor or not logged in:\n\n\nThe business role will be \nSystem::Anonymous\n. So the user can only has the action permission of \nPackage::Read\n.\nSo the user can only view the public data packages.\n\n\nLogged in user:\n\n\nThe business role will be \nSystem::LoggedIn\n . So the user will have permission of :\n\n\n\n\nPublisher::Create\n : The user can create new publisher.\n\n\nPackage::Create\n : The user can create new data package.\n\n\nPackage::Read\n : Can read public data packages", 
            "title": "Authorization"
        }, 
        {
            "location": "/developers/authorization/#authorization-set-up", 
            "text": "Authorization is the process of giving someone permission to do or have something. In multi-user systems, a system administrator defines for the system which users are allowed access to the system and what privileges of use.  We have a standard access control matrix with 3 axes:   Actions: CREATE, READ, WRITE, DELETE, PURGE etc. these can vary among different entities  Entities (object): User, Publisher, Package, Package Resource, \u2026  Users: a user or type of user   Permission is a tuple of  (Users, Entities, Actions)", 
            "title": "Authorization Set up"
        }, 
        {
            "location": "/developers/authorization/#introducing-roles", 
            "text": "It can be tiresome and inefficient to list for every object all the users permitted to perform a given action. For example:   Many users in an organization get same set of privileges because of their position in the organization.  We want to change the permissions associated with a certain level in the organization and to have those permissions changed for all people in that level  A user may change level frequently (ex. user may get promoted)   So we create roles   Per object roles e.g. Package Owner  Per system roles e.g. System Administrator  A list or algorithm for assigning Users =  Roles   Access control algorithm: is_allowed(user, entity, action)  For this user: what roles do they have related to this entity and the system?\nGiven those roles: what actions do they have: UNIONrole  Note: it would get more complex if some roles deny access. E.g. Role: Spammer might mean you are denied action to posting etc. Right now we don\u2019t have that issue.  Is the desired action in that set?", 
            "title": "Introducing Roles"
        }, 
        {
            "location": "/developers/authorization/#roles", 
            "text": "The example roles are given below.   Package  Owner  =  all actions  Editor  Read  Create  Delete  Undelete  Update  Tag    Viewer  =  Only read    Publisher  Owner =  all actions on Publisher  Editor  ViewMemberList  AddMember  RemoveMember  Read    Viewer =  Only Read    System  LoggedIn  Package::Create  Publisher::Create    All =  Package::Read on public packages  Sysadmin =  all actions     This  contains the current roles.", 
            "title": "Roles"
        }, 
        {
            "location": "/developers/authorization/#business-roles", 
            "text": "Publisher Owner  Publisher::Owner    Publisher Member  Publisher::Editor    (Logged in) User  System::LoggedIn    Sys Admin  System::Sysadmin    Visitor  System::Anonymous      NOTE: business roles and authorization roles are distinct. Of course, in implementing access control we will use the business logic inherent in business roles. However, business roles are not explicitly present in the access control system.", 
            "title": "Business roles"
        }, 
        {
            "location": "/developers/authorization/#actions", 
            "text": "Note: not an exhaustive list.  This  contains the current Actions.    Package:  Package::Read  Package::Create  Package::Delete  Package::Undelete  Package::Purge  Package::Update  Package::Tag    Publisher:  Publisher::Create  Publisher::AddMember  Publisher::RemoveMember  Publisher::Read  Publisher::Delete  Publisher::Update  Publisher::ViewMemberList", 
            "title": "Actions"
        }, 
        {
            "location": "/developers/authorization/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/developers/authorization/#first-time-visitor-or-not-logged-in", 
            "text": "The business role will be  System::Anonymous . So the user can only has the action permission of  Package::Read .\nSo the user can only view the public data packages.", 
            "title": "First time visitor or not logged in:"
        }, 
        {
            "location": "/developers/authorization/#logged-in-user", 
            "text": "The business role will be  System::LoggedIn  . So the user will have permission of :   Publisher::Create  : The user can create new publisher.  Package::Create  : The user can create new data package.  Package::Read  : Can read public data packages", 
            "title": "Logged in user:"
        }, 
        {
            "location": "/developers/authentication/", 
            "text": "DataHub Authentication\n\n\nThis page describes authentication of DataHub users. The details provided can be used by developers, willing to contribute to the existing \ndpm\n API client or implement custom client for The DataHub API.\n\n\nThe DataHub Frontend allows users to be registered via \ngithub\n using the web browser. After a successful registration, user will be given unique API-KEY to authenticate with DataHub API server.\n\n\nAPI authentication\n\n\nSome DataHub API methods require client to provide identity of a registered user. To prove its identity, client first has to obtain temporal JWT token, providing permanent API-KEY of a registered user. After that client can pass this token in the header of a request to the API.\n\n\nTo obtain a temporal JWT token, client should send POST request to \n/api/auth/token\n. Request should have json-encoded body with 'username' and 'secret' keys, where 'secret' is an API-KEY of the user:\n\n\nresponse = requests.post(\n        url='https://datapackaged.com/api/auth/token',\n        {'username': 'my_username', 'secret': '1dd5f984bc'}))\n\n\n\nIf the username and API-KEY are valid, server will return json response with JWT token: \n{'token': 'a6d8b887'}\n\n\nauth_token = response.json().get('token')\n\n\n\nThis token should be temporarily stored by the client. To access any API method with authentication, client should include this token in the \"Authorization\" header.\n\n\nrequests.post(api_url, headers={'Authorization', 'Bearer %s' % auth_token})", 
            "title": "Authentication"
        }, 
        {
            "location": "/developers/authentication/#datahub-authentication", 
            "text": "This page describes authentication of DataHub users. The details provided can be used by developers, willing to contribute to the existing  dpm  API client or implement custom client for The DataHub API.  The DataHub Frontend allows users to be registered via  github  using the web browser. After a successful registration, user will be given unique API-KEY to authenticate with DataHub API server.", 
            "title": "DataHub Authentication"
        }, 
        {
            "location": "/developers/authentication/#api-authentication", 
            "text": "Some DataHub API methods require client to provide identity of a registered user. To prove its identity, client first has to obtain temporal JWT token, providing permanent API-KEY of a registered user. After that client can pass this token in the header of a request to the API.  To obtain a temporal JWT token, client should send POST request to  /api/auth/token . Request should have json-encoded body with 'username' and 'secret' keys, where 'secret' is an API-KEY of the user:  response = requests.post(\n        url='https://datapackaged.com/api/auth/token',\n        {'username': 'my_username', 'secret': '1dd5f984bc'}))  If the username and API-KEY are valid, server will return json response with JWT token:  {'token': 'a6d8b887'}  auth_token = response.json().get('token')  This token should be temporarily stored by the client. To access any API method with authentication, client should include this token in the \"Authorization\" header.  requests.post(api_url, headers={'Authorization', 'Bearer %s' % auth_token})", 
            "title": "API authentication"
        }, 
        {
            "location": "/developers/user-stories/", 
            "text": "User Stories\n\n\nDataHub is the place where \npeople\n can \nstore, share and publish\n their data, \ncollect, inspect and process\n it with \npowerful tools\n, and \ndiscover and use\n data shared by others. [order matters]\n\n\nPeople = data wranglers = those who use machines (e.g. code, command line tools) to work with their data rather than editing it by hand (as, for example, many analysts do in Excel). (Think people who use Python vs people who use Excel for data work)\n\n\n\n\nData is not chaotic and is in some sense neat\n\n\nCan present your data with various visualization tools (graphs, charts, tables etc.) \n\n\nEasy to publish\n\n\nSpecific data (power) tools and integrations\n\n\nCan validate your data before publishing\n\n\nData API\n\n\nData Conversion / Bundling: zip the data, provide sqlite\n\n\nGenerate a node package of your data\n\n\n(Versioning)\n\n\n\n\n\n\n\n\nTable of Contents\n\n\n\n\n\n\nUser Stories\n\n\nPersonas\n\n\nStories\n\n\n1. Get Started\n\n\n1. Sign in / Sign up [DONE]\n\n\nSign up via github (and/or google)\n\n\nNext Step after Sign Up\n\n\nInvite User to Join Platform\n\n\n\n\n\n\n2. Publish Data Packages\n\n\nPublish with a Client\n\n\nConfigure Client\n\n\n\n\n\n\nUpdate a Data Package\n\n\nDelete a Data Package\n\n\nPurge a Data Package\n\n\nValidate Data in Data Package\n\n\nValidate in CLI\n\n\nValidate on Server\n\n\n\n\n\n\nCache Data Package Resource data (on the server)\n\n\nPublish with Web Interface\n\n\nUndelete data package\n\n\nRender (views) in data package in CLI before upload\n\n\n\n\n\n\n3. Find and View Data Packages\n\n\nView a Data Package Online\n\n\n(Pre)View a not-yet-published Data Package Online\n\n\nSee How Much a Data Package is Used (Downloaded) {2d}\n\n\nBrowse Data Packages\n\n\nSearch for Data Packages\n\n\nDownload Data Package Descriptor\n\n\nDownload Data Package in One File (e.g. zip)\n\n\n\n\n\n\n4. Install a Data Package (locally)\n\n\nUse DataPackage in Node (package auto-generated)\n\n\nImport DataPackage into R\n\n\nImport DataPackage into Pandas\n\n\nSQL / SQLite database\n\n\nSee changes between versions\n\n\nLow Priority\n\n\n\n\n\n\n5. Versioning and Changes in Data Packages\n\n\nExplicit Versioning - Publisher\n\n\nExplicit Versioning - Consumer\n\n\nRevisioning - Implicit Versioning\n\n\nChange Notifications\n\n\n\n\n\n\n6. Publishers\n\n\nCreate a New Publisher\n\n\nFind a Publisher (and users?)\n\n\nView a Publisher Profile\n\n\nSearch among publishers packages\n\n\nRegistered Users Profile and packages\n\n\n\n\n\n\nPublisher and User Leaderboard\n\n\nManage Publisher\n\n\nCreate and Edit Profile\n\n\nAdd and Manage Members\n\n\n\n\n\n\n\n\n\n\n7. Web Hooks and Extensions\n\n\n8. Administer Site\n\n\nConfigure Site\n\n\nSee usage metrics\n\n\nPricing and Billing\n\n\n\n\n\n\nPrivate Data Packages\n\n\nSell My Data through your site\n\n\n\n\n\n\n\n\n\n\nPersonas\n\n\n\n\n[Geek] Publisher\n. Knows how to use a command line or other automated tooling. Wants to publish their data package in order to satisfy their teams requirements to publish data.\n\n\nNon-Geek Publisher. Tbc \u2026\n\n\n\n\n\n\nConsumer\n: A person or organization looking to use data packages (or data in general)\n\n\nData Analyst\n\n\nCoder (of data driven applications)\n\n\n\u2026\n\n\n\n\n\n\nAdmin\n: A person or organization who runs an instance of a DataHub\n\n\n\n\nStories\n\n\n1. Get Started\n\n\n1. Sign in / Sign up [DONE]\n\n\nAs a Geek Publisher I want to sign up for an account so that I can publish my data package to the registry and to have a publisher account to publish my data package under.\n\n\nGenerally want this to be as minimal, easy and quick as possible\n\n\n\n\nSign in with a Google account\n\n\n(?) what about up other social accounts?\n\n\n\n\n\n\nEssential profile information (after sign in we prompt for this)\n\n\nemail address\n\n\nName\n\n\n(?) - future. Credit card details for payment - can we integrate with payment system (?)\n\n\n\n\n\n\nThey need to choose a user name which is url friendly unique human readable name for our app. Can be used in sign in and in many other places.\n\n\nWHY? Where would we need this? For url on site \n for publisher\n\n\nSame as publisher names (needed for URLs): [a-z-_.]\n\n\nExplain: they cannot change this later e.g. \"Choose wisely! Once you set this it cannot be changed later!\"\n\n\n\n\n\n\nSend the user an email confirming their account is set up and suggesting next steps\n\n\n\n\nAutomatically:\n\n\n\n\nAuto-create a publisher for them\n\n\nSame name as their user name but a publisher\n\n\nThat way they can start publishing straight away \u2026\n\n\n\n\n\n\n\n\nTODO: (??) should we do *all\n of this via the command line client (a la npmjs) *\n\n\nSign up via github (and/or google)\n\n\nAs a Visitor I want to sign up via github or google so that I don\u2019t have to enter lots of information and remember my password for yet another website\n\n\n\n\nHow do we deal with username conflicts? What about publisher name conflicts?\n\n\nThis does not arise in simple username system because we have only pool of usernames\n\n\n\n\n\n\n\n\nNext Step after Sign Up\n\n\nAs a Geek Publisher I want to know what do next after signing up so that I can get going quickly.\n\n\nThings to do:\n\n\n\n\nEdit your profile\n\n\nDownload a client / Configure your client (if you have one already)\n\n\nInstructions on getting relevant auth credentials\n\n\nNote they will *need* to have set a username / password in their profile\n\n\n\n\n\n\nJoin a Publisher (understand what a publisher is!)\n\n\n\n\nInvite User to Join Platform\n\n\nAs an Admin (or existing Registered User?) I want to invite someone to join the platform so that they can start contributing or using data\n\n\n\n\nGet an invitation email with a sign up link\n\n\nSome commonality with Publisher invite member below\n\n\n\n\n2. Publish Data Packages\n\n\nPublish with a Client\n\n\nAs a Geek Publisher I want to import (publish) my data package into the registry so my data has a permanent online home so that I and others can have access\n\n\nOn command line looks like:\n\n\n\n\ncd my/data/package\n\n\ndpm publish\n\n\n\u2026 working \u2026\n\n\nSUCCESS\n\n\n\n\nNotes\n\n\n\n\nPermissions: must be a member of the Publisher\n\n\nInternally: DataPackageCreate or DataPackageUpdate capability\n\n\n\n\n\n\nHandle conflicts: if data package already exists, return 409. Client instructions should be already exists and use \"--force\" or similar to overwrite\n\n\nAPI endpoint behind the scenes: POST {api}/package/\n\n\nTODO: private data packages\n\n\nAnd payment!\n\n\n\n\n\n\n\n\nConfigure Client\n\n\nAs a Geek Publisher I want to configure my client so I can start publishing data packages.\n\n\nTODO: check whether we want to use OAUTH.\n\n\nTODO: research JWT (JSON Web Tokens)\n\n\nLocally in $HOME store store:\n\n\n.dpm/credentials # stores your API key and user name\n\n\n.dpm/config # stores info like your default publisher\n\n\nUpdate a Data Package\n\n\nAs a Geek Publisher I want to use a publish command to update a data package that is already in the registry so it appears there\n\n\n\n\nOld version will be lost (!)\n\n\n\n\nDelete a Data Package\n\n\nAs a Geek Publisher I want to unpublish (delete) a data package so it is no longer visible to anyone\n\n\nPurge a Data Package\n\n\nAs a Geek Publisher I want to permanently delete (purge) a data package so that it no longer takes up storage space\n\n\nValidate Data in Data Package\n\n\nValidate in CLI\n\n\nAs a Publisher [owner/member] I want to validate the data I am about to publish to the registry so that I publish \u201cgood\u201d data and know that I am doing so and do not have to manually check that the published data looks ok (e.g. rendering charts properly) (and if wrong I have to re-upload)\n\n\ndpmpy datavalidate [file-path]\n\n\n\n\n[file-path] - run this against a given file. Look in the resources to see if this file is there and if so use the schema. Otherwise just do goodtables table \u2026\n\n\nIf no file provided run validate against each resource in turn in the datapackage\n\n\n\n\n\n\nOutput to stdout.\n\n\nDefault: human-readable - nice version of output from goodtables.\n\n\nOption for JSON e.g. --json to put machine readable output\n\n\ncheck goodtables command line tool and follow if possible. Can probably reuse code\n\n\n\n\n\n\n\n\n\n\nAuto-run this before publish unless explicit suppression (e.g. --skip-datavalidate)\n\n\nUse goodtables\n\n\n\n\nValidate on Server\n\n\nAs a Publisher [owner] i want my data to be validated when I publish it so that I know immediately if I have accidentally \u201cbroken\u201d my data or have bugs and can take action to correct\n\n\nAs a Consumer I want to know that the data I am downloading is \u201cgood\u201d and can be relied on so that I don\u2019t have to check it myself or run into annoying bugs later on\n\n\n\n\nImplies showing something in the UI e.g. \u201cData Valid\u201d (like build passing)\n\n\n\n\nImplementation notes to self\n\n\n\n\nNeed a new table to store results of validation and a concept of a \u201crun\u201d\n\n\nStore details of the run [e.g. time to complete, ]\n\n\n\n\n\n\nHow to automate doing validation (using goodtables we assume) - do we reuse a separate service (goodtables.io in some way) or run ourselves in a process like ECS ???\n\n\nDisplay this in frontend\n\n\n\n\nCache Data Package Resource data (on the server)\n\n\nAs a Publisher I want to publish a data package where its resource data is stored on my servers but the registry caches a copy of that data so that if my data is lost or gets broken I still have a copy people can use\n\n\nAs a Consumer I want to be able to get the data for a data package even if the original data has been moved or removed so that I can still use is and my app or analysis keeps working\n\n\n\n\nTODO: what does this mean for the UI or command line tools. How does the CLI know about this, how does it use it?\n\n\n\n\nPublish with Web Interface\n\n\nAs a Publisher I want to publish a data package in the UI so that it is available and published\n\n\n\n\nPublish => they already have datapackage.json and all the data. They just want to be able to upload and store this.\n\n\n\n\nAs a Publisher I want to create a data package in the UI so that it is available and published\n\n\n\n\nCreate => no datapackage.json - just data files. Need to add key descriptors information, upload data files and have schemas created etc etc.\n\n\n\n\nUndelete data package\n\n\n[cli] As a Publisher I want to be able to restore the deleted data package via cli, so that it is back visible and available to view, download (and searchable)\n\n\ndpmpy undelete\n\n\n\n[webui] As a Publisher i want to undelete the deleted data packages, so that the deleted data packages is now visible again.\n\n\nRender (views) in data package in CLI before upload\n\n\nAs a Publisher, I want to be able to preview the views (graphs and table (?)) of the current data package using cli prior to publishing so that I can refine the json declarations of datapackage view section to achieve a great looking result.\n\n\n3. Find and View Data Packages\n\n\nView a Data Package Online\n\n\nEPIC: As a Consumer I want to view a data package online so I can get a sense of whether this is the dataset I want\n\n\n\n\nObsess here about \u201cwhether this is the dataset I want\u201d\n\n\n*Publishers want this too \u2026 *\n\n\nAlso important for SEO if we have good info here\n\n\n\n\nFeatures\n\n\n\n\nVisualize data in charts - gives one an immediate sense of what this is\n\n\nOne graph section at top of page after README excerpt\n\n\nOne graph for each entry in the \u201cviews\u201d\n\n\n\n\n\n\nInteractive table - allows me to see what is in the table\n\n\nOne table for each resource\n\n\n\n\n\n\n\n\nThis user story can be viewed from two perspectives:\n\n\n\n\nFrom a publisher point of view\n\n\nFrom a consumer point of view\n\n\n\n\nAs a \npublisher\n i want to show the world how my published data is so that it immediately catches consumer\u2019s attention (and so I know it looks right - e.g. graph is ok)\n\n\nAs a \nconsumer\n i want to view the data package so that i can get a sense of whether i want this dataset or not.\n\n\nAcceptance criteria - what does done mean!\n\n\n\n\nA table for each resource\n\n\nSimple graph spec works => converts to plotly\n\n\nMultiple time series\n\n\n\n\n\n\nPlotly spec graphs work\n\n\nAll core graphs work (not sure how to check every one but representative ones)\n\n\nRecline graphs specs (are handled - temporary basis)\n\n\nLoading spinners whilst data is loading so users know what is happening\n\n\n\n\nBonus:\n\n\n\n\nComplex examples e.g. time series with a log scale \u2026 (e.g. hard drive data \u2026)\n\n\n\n\nFeatures: \nDP view status\n\n\n\n\nDifferent options to view data as graph.\n\n\nRecline\n\n\nVega-lite\n\n\nVega\n\n\n[Plotly]\n\n\n\n\n\n\nGeneral Functionality\n\n\nMultiple views [wrongly done. We iterate over resource not views]\n\n\nTable as a view\n\n\n\n\n\n\nInteractive table so that consumer can do\n\n\nFilter\n\n\nJoin\n\n\n\n\n\n\n\n\n(Pre)View a not-yet-published Data Package Online\n\n\nAs a (potential) Publisher I want to preview a datapackage I have prepared so that I can check it works and share the results (if there is something wrong with others)\n\n\n\n\nBe able to supply a URL to my datapackage (e.g. on github) and have it previewed as it would look on DPR\n\n\nBe able to upload a datapackage and have it previewed\n\n\n\n\nRufus: this was a very common use case for me (and others) when using data.okfn.org. Possibly less relevant if the command line tool can do previewing but still relevant IMO (some people may not have command line tool, and it is useful to be able to share a link e.g. when doing core datasets curation and there is something wrong with a datapackage).\n\n\nRufus: also need for an online validation tool\n\n\nSee How Much a Data Package is Used (Downloaded) {2d}\n\n\nAs a Consumer i want to see how much the data has been downloaded so that i can choose most popular (=\n probably most reliable and complete) in the case when there are several alternatives for my usecase (maybe from different publishers)\n\n\nBrowse Data Packages\n\n\nAs a potential Publisher, unaware of datapackages, I want to see real examples of published packages (with the contents datapackage.json), so that I can understand how useful and simple is the datapackage format and the registry itself.\n\n\nAs a Consumer I want to see some example data packages quickly so I get a sense of what is on this site and if it is useful to look further\n\n\n\n\nBrowse based on what properties? Most recent, most downloaded?\n\n\nMost downloaded\n\n\nStart with: we could just go with core data packages\n\n\n\n\n\n\n\n\nSearch for Data Packages\n\n\nAs a Consumer I want to search data packages so that I can find the ones I want\n\n\n\n\nEssential question: what is it you want?\n\n\nRufus: in my view generic search is actually *not* important to start with. People do not want to randomly search. More useful is to go via a publisher at the beginning.\n\n\n\n\n\n\nSearch results should provide enough information to help a user decide whether to dig further e.g. title, short description\n\n\nFor future when we have it: [number of downloads], stars etc\n\n\n\n\n\n\n\n\nMinimum viable search (based on implementation questions)\n\n\n\n\nFilter by publisher\n\n\nFree text search against title\n\n\nDescription could be added if we start doing actual scoring as easy to add additional fields\n\n\n\n\n\n\nScoring would be nice but not essential\n\n\n\n\n\n\n\n\nImplementation questions:\n\n\n\n\nSearch:\n\n\nShould search perform ranking (that requires scoring support)\n\n\nFree text queries should search against which fields (with what weighting)?\n\n\n\n\n\n\nFiltering: On what individual properties of the data package should be able to filter?\n\n\nThemes and profiles:\n\n\nSearching for a given profile: not possible atm.\n\n\nThemes: Should we tag data packages by themes like finance, education and let user find data package by that?\n        * Maybe but not now - maybe in the future\n\n\n\n\n\n\nIf we follow the go via a publisher at the beginning then should we list the most popular publisher on the home page of user[logged-in/ not logged in]?\n\n\nIf most popular publisher then by what mesaure?\n\n\nSort by Most published?\n\n\nSort by Most followers?\n\n\nSort by most downloads?\n\n\nOr all show top5 in each facet?\n\n\n\n\n\n\n\n\n\n\n\n\nSub user stories:\n\n\n\n\n[WONTFIX?] As a Consumer i want to find the data packages by profile (ex: spending) so that I can find the kind of data I want quickly and easily and in one big list\n\n\nAs a Consumer i want to search based on description of data package, so that I can find package which related to some key words\n\n\n\n\nDownload Data Package Descriptor\n\n\nAs a Consumer I want to download the data package descriptor (datapackage.json) on its own so that \u2026\n\n\n*Rufus: I can\u2019t understand why anyone would want to do this *\n\n\nDownload Data Package in One File (e.g. zip)\n\n\nAs a Consumer I want to download the data package in one file so that I don\u2019t have to download descriptor and each resource by hand\n\n\nOnly useful if no cli tool and no install command\n\n\n4. Install a Data Package (locally)\n\n\nLet\u2019s move discussion to the github: \nhttps://github.com/frictionlessdata/dpm-py/issues/30\n\n\nTODO add these details from the requirement doc\n\n\n\n\nLocal \u201cData Package\u201d cache storage (`.datapackages` or similar)\n\n\nStores copies of packages from Registry\n\n\nStores new Data Packages the user has created\n\n\nThis\n \nRuby lib\n \nimplements something similar\n\n\n\n\n\n\n\n\nUse DataPackage in Node (package auto-generated)\n\n\nAs a NodeJS developer I want to use data package as a node lib in my project so that I can depend on it using my normal dependency framework\n\n\n\n\nSee this \nreal-world example\n of this request for country-list\n\n\n=> auto-building node package and publishing to npm (not that hard to do \u2026)\n\n\nConvert CSV data to json (that\u2019s what you probably want from node?)\n\n\nGenerate package.json\n\n\nPush to npm (register the dataset users)\n\n\nRufus: My guess here is that to implement here we want something a bit like github integrations \u2013 specific additional hooks which also get some configuration (or do it like travis - github integration plus a tiny config file - in our case rather than a .travis.yml we have a .node.yml or whatever)\n\n\n\n\n\n\nIs it configurable for user that enable to push to npm or not?\n\n\nYes. Since we need to push to a specific npm user (for each publisher) this will need to be configured (along with authorization - where does that go?)\n\n\n\n\n\n\nIs this something done for *all* data packages or does user need to turn something on? Probably want them to turn this on \u2026\n\n\n\n\nQuestions:\n\n\n\n\nFrom where we should push the data package to npm repo.\n\n\nIs it from dpmpy or from server? Obviously from a server - this needs to be automated. But you can use dpmpy if you want (but I\u2019m not sure we do want to \u2026)\n\n\n\n\n\n\nWhat to do with multiple resources? Ans: include all resources\n\n\nDo we include datapackage.json into the node package? Yes, include it so they get all the original metadata.\n\n\n\n\nGeneric version is:\n\n\nAs a Web Developer I want to download a DataPackage (like currency codes or country names) so that I can use it in the web service I am building [...]\n\n\nImport DataPackage into R\n\n\nAs a Consumer [R user] I want to load a Data Package from R so that I can immediately start playing with it\n\n\n\n\nShould we try and publish to CRAN?\n\n\nProbably not? Why? think it can be quite painful getting permission to publish to CRAN and very easy to load from the registry\n\n\nOn the CRAN website I can't find a way to automate publishing. It seems possible by filling web-form, but to know the status we have to wait and parse email.\n\n\n\n\n\n\n\n\n\n\nUsing this library: \nhttps://github.com/ropenscilabs/datapkg\n\n\nWhere can i know about this?\n\n\nOn each data package view page \u2026\n\n\n\n\n\n\n\n\nGeneric version:\n\n\nAs a Data Analyst I want to download a data package, so that I can study it and wrangle with it to infer new data or generate new insights.\n\n\nAs a Data Analyst, I want to update previously downloaded data package, so that I can work with the most recent data.\n\n\nQuestion:\n\n\n\n\nIs the version structure would have been problem for the user. As out directory structure is e.g \u201cbits.datapackaged.com/metadata/core/demo-package/_v/latest/*\u201d? If now how to\n\n\n\n\nImport DataPackage into Pandas\n\n\nTODO - like R\n\n\nSQL / SQLite database\n\n\nAs a Consumer I want to download a DataPackage\u2019s data one coherent SQLite database so that I can get it easily in one form\n\n\nQuestion:\n\n\n\n\nWhy does we need to store datapackage data in sqlite. Is not it better to store in file structure?\n\n\n\n\nWe can store the datapackage like this way:\n\n\n~/.datapackage/\\\npublisher>/\\\npackage>/\\\nversion>/*\n\n\nThis is the way maven/gradle/ivy cache jar locally.\n\n\nSee changes between versions\n\n\nAs a Data Analyst I want to compare different versions of some datapackage locally, so that I can see schema changes clearly and adjust my analytics code to the desired schema version.\n\n\nLow Priority\n\n\nAs a Web Developer of multiple projects, I want to be able to install multiple versions of the same datapackage separately so that all my projects could be developed independently and deployed locally. (virtualenv-like)\n\n\nAs a Developer I want to list all DataPackages requirements for my project in the file and pin the exact versions of any DataPackage that my project depends on so that the project can be deterministically deployed locally and won\u2019t break because of the DataPackage schema changes. (requirements.txt-like)\n\n\n5. Versioning and Changes in Data Packages\n\n\nWhen we talk about versioning we can mean two things:\n\n\n\n\nExplicit versioning: this is like the versioning of releases \u201cv1.0\u201d etc. This is conscious and explicit. Main purpose:\n\n\n*to support other systems depending on this one (they want the data at a known stable state) *\n\n\neasy access to major staging points in the evolution (e.g. i want to see how things were at v1)\n\n\n\n\n\n\nImplicit versioning or \u201crevisioning\u201d: this is like the commits in git or the autosave of a word or google doc. It happens frequently, either with minimum effort or even automatically. Main purpose:\n\n\nUndelete and recovery (you save a every point and can recover if you accidentally write or delete something)\n\n\nCollaboration and merging of changes (in revision control)\n\n\nActivity logging\n\n\n\n\n\n\n\n\nExplicit Versioning - Publisher\n\n\nAs a Publisher I want to tag a version of my data on the command line so that \u2026 [see so that\u2019s below]\n\n\ndpmpy tag {tag-name}\n\n\n=> tag current \u201clatest\u201d on the server as {tag-name}\n\n\n\n\nDo we restrict {tag-name} to semver? I don\u2019t think so atm.\n\n\nAs a {Publisher} I want to tag datapackage to create a snapshot of data on the registry server, so that consumers can refer to it\n\n\nAs a {Publisher} I want to be warned that a tag exists, when I try to overwrite it, so that I don\u2019t accidentally overwrite stable tagged data, which is relied on by consumers.\n\n\nAs a {Publisher} I want to be able to overwrite the previously tagged datapackage, so that I can fix it if I mess up.\n\n\nThe versioning here happens server side\n\n\nIs this confusing for users? I.e. they are doing something local.\n\n\n\n\n\n\n\n\nBackground \u201cso that\u201d user story epics:\n\n\n\n\nAs a {Publisher} I want to version my Data Package and keep multiple versions around including older versions so that I do not break consumer systems when I change my Data Package (whether schema or data) [It is not just the publisher who wants this, it is a consumer - see below]\n\n\nAs a {Publisher} I want to be able to get access to a previous version I tagged so that I can return to it and review it (and use it)\n\n\nso that i can recover old data if i delete it myself or compare how things changed over time\n\n\n\n\n\n\n\n\nExplicit Versioning - Consumer\n\n\nAs a {Consumer} (of a Data Package) I want to know full details when and how the data package schema has changed and when so that I can adjust my scripts to handle it.\n\n\nImportant info to know for each schema change:\n\n\n\n\ntime when published\n\n\nfor any \nchanged\n field - name, what was changed (type, format, \u2026?),\n    \n +maybe everything else that was not changed (full field descriptor)\n\n\nfor any \ndeleted\n field - name,\n    \n +maybe everything else (full field descriptor)\n\n\nfor any \nadded\n field - all data (full field descriptor)\n\n\n\n\nA change in schema would correspond to a major version change in software (see\n \nhttp://semver.org/\n)\n.\n\n\nConcerns about explicit versioning\n: we all have experience with consuming data from e.g. government publishers where the publishers change the data schema breaking client code. I am constatnly looking for a policy\\mechanism to guide publishers to develop stable schema versioning for the data they produce, and help consumers to get some stability guarantees.\n\n\nAutomated versioning / automated tracking\n: Explicit versioning relies on the publisher, and humans can forget or not care enough about others. So to help consumers my suggestion would be to always track schema changes of uploaded packages on the server, and allow users to review those changes on the website. (We might even want to implement auto-tagging or not allowing users to upload a package with the same version but a different schema without forcing)\n\n\nAs a {Consumer} I want to get a sense how outdated is the datapackage, that I have downloaded before, so that I can decide if I should update or not.\n\n\n\n\nI want to preview a DataPackage changelog (list of all available versions\\tags with brief info) online, sorted by creation time, so that I can get a sense how data or schema has changed since some time in the past. Important brief info:\n\n\nTime when published\n\n\nHow many rows added\\deleted for each resource data\n\n\nWhat fields(column names) changed, added or deleted for each resource.\n\n\n\n\n\n\n\n\nAs a {Consumer} I want to view a Datapackage at a particular version online, so that I can present/discuss the particular data timeslice of interest with other people.\n\n\nAs a {Consumer} I want to download a Data package at a particular version so that I know it is compatible with my scripts and system\n\n\n\n\nOnline: I want to pick the version I want from the list, and download it (as zip for ex.)\n\n\nCLI: I want to specify tag or version when using the `install` command.\n\n\n\n\nRevisioning - Implicit Versioning\n\n\n\u2026\n\n\nChange Notifications\n\n\nAs a Consumer I want to be notified of changes to a package i care about so that I can check out what has changed and take action (like downloading the updated data)\n\n\nAs a Consumer I want to see how active the site is to see if I should get involved\n\n\n6. Publishers\n\n\nCreate a New Publisher\n\n\nTODO\n\n\nFind a Publisher (and users?)\n\n\nAs a Consumer I want to browse and find publishers so that I can find interesting publishers and their packages (so that I can use them)\n\n\nView a Publisher Profile\n\n\nview data packages associated to a publisher or user\n\n\nImplementation details: \nhttps://hackmd.io/MwNgrAZmCMAcBMBaYB2eAWR72woghmLNIrAEb4AME+08s6VQA===\n\n\nAs a Consumer I want to see a publisher\u2019s profile so that I can discover their packages and get a sense of how active and good they are\n\n\nAs a Publisher I want to have a profile with a list of my data packages so that:\n\n\n\n\nOthers can find my data packages quickly and easily\n\n\nCan see how many data packages i have\n\n\nI can find a data package i want to look at quickly [they can discover their own data]\n\n\nI can find the link for a data package to send to someone else\n\n\nPeople want to share what they have done. This is probably the number one way the site gets prominence at the start (along with simple google traffic)\n\n\n\n\n\n\nso that I can check that members do not abuse their rights to publish and only publish topical data packages.\n\n\n\n\nAs a Consumer I want to view a publisher\u2019s profile so that I can see who is behind a particular package or to see what other packages they produce [navigate up from a package page] [so that: i can trust on his published data packages to reuse.]\n\n\nDetails\n\n\n\n\nProfile =\n\n\nFull name / title e.g. \u201cWorld Bank\u201d, identifier e.g. world-bank\n\n\npicture, short description text (if we have this - we don\u2019t atm)\n\n\n(esp important to know if this is the world bank or not)\n\n\n\n\n\n\nTotal number of data packages\n\n\nList of data packages\n\n\nView by most recently created (updated?)\n\n\nFor each DataPackage want to see: title, number of resources (?), first 200 character of description, license (see data.okfn.org/data/ for example)\n\n\nDo we limit / paginate this list? No, not for the moment\n\n\n\n\n\n\n[wontfix atm] Activity - this means data packages published, updated\n\n\n[wontfix atm] Quality \u2026 - we don\u2019t have anything on this\n\n\n[wontfix atm] List of users\n\n\n\n\n\n\nWhat are the permissions here?\n\n\nDo we show private data packages? No\n\n\nDo we show them when \u201cowner\u201d viewing or sysadmin? Yes (but flag as \u201cprivate\u201d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat data packages to show? All the packages you own.\n\n\nWhat about pinning? No support for this atm.\n\n\n\n\n\n\n\n\nSearch among publishers packages\n\n\nAs a Consumer i want to search among all data packages owned by a publisher so that I can easily find one data package amongst all the data packages by this publisher.\n\n\nRegistered Users Profile and packages\n\n\nAs a Consumer i want to see the profile and activity of a user so that \u2026\n\n\nAs a Registered User I want to see the data packages i am associated with \nso that\n [like publisher]\n\n\nPublisher and User Leaderboard\n\n\nAs a ??? I want to see who are the top publihers and users so that I can emulate them or ???\n\n\nManage Publisher\n\n\nCreate and Edit Profile\n\n\nAs {Owner ...} I want to edit my profile so that it is updated with new information\n\n\nAdd and Manage Members\n\n\nAs an {Owner of a Publisher in the Registry} I want to invite an existing user to become a member of my publisher\n\n\n\n\nAuto lookup by user name (show username and fullname) - standard as per all sites\n\n\nUser gets a notification on their dashboard + email with link to accept invite\n\n\nIf invite is accepted notify the publisher (?) - actually do not do this.\n\n\n\n\nAs an {Owner of a Publisher in the Registry} I want to invite someone using their email to sign up and become a member of my Publisher so that they are authorized to publish data packages under my Publisher.\n\n\nAs an {Publisher Owner} I want to remove someone from membership in my publisher so they no longer have ability to publish or modify my data packages\n\n\nAs a {Publisher Owner} I want to view all the people in my organization and what roles they have so that I can change these if I want\n\n\nAs a {Publisher Owner} I want to make a user an \u201cowner\u201d so they have full control\n\n\nAs a {Publisher Owner} I want to remove a user as an \u201cowner\u201d so they are just a member and no longer have full control\n\n\n7. Web Hooks and Extensions\n\n\nTODO: how do people build value added services around the system (and push back over the API etc \u2026) - OAuth etc\n\n\n8. Administer Site\n\n\nConfigure Site\n\n\nAs the Admin I want to set key configuration parameters for my site deployment so that I can change key information like the site title\n\n\n\n\nMain config database is the one thing we might need\n\n\n\n\nSee usage metrics\n\n\nAs an Admin I want to see key metrics about usage such as users, API usage, downloads etc so that I know how things are going\n\n\n\n\nTotal users are signed up, how many signed up in last week / month etc\n\n\nTotal publishers \u2026\n\n\nUsers per publisher distribution (?)\n\n\n\n\n\n\nAPI usage\n\n\nDownloads\n\n\nBilling: revenue in relevant periods\n\n\nCosts: how much are we spending on storage\n\n\n\n\nPricing and Billing\n\n\nAs an Admin I want to have a pricing plan and billing system so that I can charge users and make my platform sustainable\n\n\nAs a Publisher I want to know if this site has a pricing plan and what the prices are so that I can work out what this will cost me in the future and have a sense that these guys are sustainable (\u2018free forever\u2019 does not work very well)\n\n\nAs a Publisher I want to sign up for a given pricing plan so that I am entitled to what it allows (e.g. private stuff \u2026)\n\n\nPrivate Data Packages\n\n\ncf npmjs.com\n\n\nAs a Publisher I want to have private data packages that I can share just with my team\n\n\nSell My Data through your site\n\n\nEPIC: As a Publisher i want to sell my data through your site so that I make money and am able to sustain my publishing and my life \u2026", 
            "title": "User Stories"
        }, 
        {
            "location": "/developers/user-stories/#user-stories", 
            "text": "DataHub is the place where  people  can  store, share and publish  their data,  collect, inspect and process  it with  powerful tools , and  discover and use  data shared by others. [order matters]  People = data wranglers = those who use machines (e.g. code, command line tools) to work with their data rather than editing it by hand (as, for example, many analysts do in Excel). (Think people who use Python vs people who use Excel for data work)   Data is not chaotic and is in some sense neat  Can present your data with various visualization tools (graphs, charts, tables etc.)   Easy to publish  Specific data (power) tools and integrations  Can validate your data before publishing  Data API  Data Conversion / Bundling: zip the data, provide sqlite  Generate a node package of your data  (Versioning)     Table of Contents    User Stories  Personas  Stories  1. Get Started  1. Sign in / Sign up [DONE]  Sign up via github (and/or google)  Next Step after Sign Up  Invite User to Join Platform    2. Publish Data Packages  Publish with a Client  Configure Client    Update a Data Package  Delete a Data Package  Purge a Data Package  Validate Data in Data Package  Validate in CLI  Validate on Server    Cache Data Package Resource data (on the server)  Publish with Web Interface  Undelete data package  Render (views) in data package in CLI before upload    3. Find and View Data Packages  View a Data Package Online  (Pre)View a not-yet-published Data Package Online  See How Much a Data Package is Used (Downloaded) {2d}  Browse Data Packages  Search for Data Packages  Download Data Package Descriptor  Download Data Package in One File (e.g. zip)    4. Install a Data Package (locally)  Use DataPackage in Node (package auto-generated)  Import DataPackage into R  Import DataPackage into Pandas  SQL / SQLite database  See changes between versions  Low Priority    5. Versioning and Changes in Data Packages  Explicit Versioning - Publisher  Explicit Versioning - Consumer  Revisioning - Implicit Versioning  Change Notifications    6. Publishers  Create a New Publisher  Find a Publisher (and users?)  View a Publisher Profile  Search among publishers packages  Registered Users Profile and packages    Publisher and User Leaderboard  Manage Publisher  Create and Edit Profile  Add and Manage Members      7. Web Hooks and Extensions  8. Administer Site  Configure Site  See usage metrics  Pricing and Billing    Private Data Packages  Sell My Data through your site", 
            "title": "User Stories"
        }, 
        {
            "location": "/developers/user-stories/#personas", 
            "text": "[Geek] Publisher . Knows how to use a command line or other automated tooling. Wants to publish their data package in order to satisfy their teams requirements to publish data.  Non-Geek Publisher. Tbc \u2026    Consumer : A person or organization looking to use data packages (or data in general)  Data Analyst  Coder (of data driven applications)  \u2026    Admin : A person or organization who runs an instance of a DataHub", 
            "title": "Personas"
        }, 
        {
            "location": "/developers/user-stories/#stories", 
            "text": "", 
            "title": "Stories"
        }, 
        {
            "location": "/developers/user-stories/#1-get-started", 
            "text": "", 
            "title": "1. Get Started"
        }, 
        {
            "location": "/developers/user-stories/#1-sign-in-sign-up-done", 
            "text": "As a Geek Publisher I want to sign up for an account so that I can publish my data package to the registry and to have a publisher account to publish my data package under.  Generally want this to be as minimal, easy and quick as possible   Sign in with a Google account  (?) what about up other social accounts?    Essential profile information (after sign in we prompt for this)  email address  Name  (?) - future. Credit card details for payment - can we integrate with payment system (?)    They need to choose a user name which is url friendly unique human readable name for our app. Can be used in sign in and in many other places.  WHY? Where would we need this? For url on site   for publisher  Same as publisher names (needed for URLs): [a-z-_.]  Explain: they cannot change this later e.g. \"Choose wisely! Once you set this it cannot be changed later!\"    Send the user an email confirming their account is set up and suggesting next steps   Automatically:   Auto-create a publisher for them  Same name as their user name but a publisher  That way they can start publishing straight away \u2026     TODO: (??) should we do *all  of this via the command line client (a la npmjs) *", 
            "title": "1. Sign in / Sign up [DONE]"
        }, 
        {
            "location": "/developers/user-stories/#sign-up-via-github-andor-google", 
            "text": "As a Visitor I want to sign up via github or google so that I don\u2019t have to enter lots of information and remember my password for yet another website   How do we deal with username conflicts? What about publisher name conflicts?  This does not arise in simple username system because we have only pool of usernames", 
            "title": "Sign up via github (and/or google)"
        }, 
        {
            "location": "/developers/user-stories/#next-step-after-sign-up", 
            "text": "As a Geek Publisher I want to know what do next after signing up so that I can get going quickly.  Things to do:   Edit your profile  Download a client / Configure your client (if you have one already)  Instructions on getting relevant auth credentials  Note they will *need* to have set a username / password in their profile    Join a Publisher (understand what a publisher is!)", 
            "title": "Next Step after Sign Up"
        }, 
        {
            "location": "/developers/user-stories/#invite-user-to-join-platform", 
            "text": "As an Admin (or existing Registered User?) I want to invite someone to join the platform so that they can start contributing or using data   Get an invitation email with a sign up link  Some commonality with Publisher invite member below", 
            "title": "Invite User to Join Platform"
        }, 
        {
            "location": "/developers/user-stories/#2-publish-data-packages", 
            "text": "", 
            "title": "2. Publish Data Packages"
        }, 
        {
            "location": "/developers/user-stories/#publish-with-a-client", 
            "text": "As a Geek Publisher I want to import (publish) my data package into the registry so my data has a permanent online home so that I and others can have access  On command line looks like:   cd my/data/package  dpm publish  \u2026 working \u2026  SUCCESS   Notes   Permissions: must be a member of the Publisher  Internally: DataPackageCreate or DataPackageUpdate capability    Handle conflicts: if data package already exists, return 409. Client instructions should be already exists and use \"--force\" or similar to overwrite  API endpoint behind the scenes: POST {api}/package/  TODO: private data packages  And payment!", 
            "title": "Publish with a Client"
        }, 
        {
            "location": "/developers/user-stories/#configure-client", 
            "text": "As a Geek Publisher I want to configure my client so I can start publishing data packages.  TODO: check whether we want to use OAUTH.  TODO: research JWT (JSON Web Tokens)  Locally in $HOME store store:  .dpm/credentials # stores your API key and user name  .dpm/config # stores info like your default publisher", 
            "title": "Configure Client"
        }, 
        {
            "location": "/developers/user-stories/#update-a-data-package", 
            "text": "As a Geek Publisher I want to use a publish command to update a data package that is already in the registry so it appears there   Old version will be lost (!)", 
            "title": "Update a Data Package"
        }, 
        {
            "location": "/developers/user-stories/#delete-a-data-package", 
            "text": "As a Geek Publisher I want to unpublish (delete) a data package so it is no longer visible to anyone", 
            "title": "Delete a Data Package"
        }, 
        {
            "location": "/developers/user-stories/#purge-a-data-package", 
            "text": "As a Geek Publisher I want to permanently delete (purge) a data package so that it no longer takes up storage space", 
            "title": "Purge a Data Package"
        }, 
        {
            "location": "/developers/user-stories/#validate-data-in-data-package", 
            "text": "", 
            "title": "Validate Data in Data Package"
        }, 
        {
            "location": "/developers/user-stories/#validate-in-cli", 
            "text": "As a Publisher [owner/member] I want to validate the data I am about to publish to the registry so that I publish \u201cgood\u201d data and know that I am doing so and do not have to manually check that the published data looks ok (e.g. rendering charts properly) (and if wrong I have to re-upload)  dpmpy datavalidate [file-path]   [file-path] - run this against a given file. Look in the resources to see if this file is there and if so use the schema. Otherwise just do goodtables table \u2026  If no file provided run validate against each resource in turn in the datapackage    Output to stdout.  Default: human-readable - nice version of output from goodtables.  Option for JSON e.g. --json to put machine readable output  check goodtables command line tool and follow if possible. Can probably reuse code      Auto-run this before publish unless explicit suppression (e.g. --skip-datavalidate)  Use goodtables", 
            "title": "Validate in CLI"
        }, 
        {
            "location": "/developers/user-stories/#validate-on-server", 
            "text": "As a Publisher [owner] i want my data to be validated when I publish it so that I know immediately if I have accidentally \u201cbroken\u201d my data or have bugs and can take action to correct  As a Consumer I want to know that the data I am downloading is \u201cgood\u201d and can be relied on so that I don\u2019t have to check it myself or run into annoying bugs later on   Implies showing something in the UI e.g. \u201cData Valid\u201d (like build passing)   Implementation notes to self   Need a new table to store results of validation and a concept of a \u201crun\u201d  Store details of the run [e.g. time to complete, ]    How to automate doing validation (using goodtables we assume) - do we reuse a separate service (goodtables.io in some way) or run ourselves in a process like ECS ???  Display this in frontend", 
            "title": "Validate on Server"
        }, 
        {
            "location": "/developers/user-stories/#cache-data-package-resource-data-on-the-server", 
            "text": "As a Publisher I want to publish a data package where its resource data is stored on my servers but the registry caches a copy of that data so that if my data is lost or gets broken I still have a copy people can use  As a Consumer I want to be able to get the data for a data package even if the original data has been moved or removed so that I can still use is and my app or analysis keeps working   TODO: what does this mean for the UI or command line tools. How does the CLI know about this, how does it use it?", 
            "title": "Cache Data Package Resource data (on the server)"
        }, 
        {
            "location": "/developers/user-stories/#publish-with-web-interface", 
            "text": "As a Publisher I want to publish a data package in the UI so that it is available and published   Publish => they already have datapackage.json and all the data. They just want to be able to upload and store this.   As a Publisher I want to create a data package in the UI so that it is available and published   Create => no datapackage.json - just data files. Need to add key descriptors information, upload data files and have schemas created etc etc.", 
            "title": "Publish with Web Interface"
        }, 
        {
            "location": "/developers/user-stories/#undelete-data-package", 
            "text": "[cli] As a Publisher I want to be able to restore the deleted data package via cli, so that it is back visible and available to view, download (and searchable)  dpmpy undelete  [webui] As a Publisher i want to undelete the deleted data packages, so that the deleted data packages is now visible again.", 
            "title": "Undelete data package"
        }, 
        {
            "location": "/developers/user-stories/#render-views-in-data-package-in-cli-before-upload", 
            "text": "As a Publisher, I want to be able to preview the views (graphs and table (?)) of the current data package using cli prior to publishing so that I can refine the json declarations of datapackage view section to achieve a great looking result.", 
            "title": "Render (views) in data package in CLI before upload"
        }, 
        {
            "location": "/developers/user-stories/#3-find-and-view-data-packages", 
            "text": "", 
            "title": "3. Find and View Data Packages"
        }, 
        {
            "location": "/developers/user-stories/#view-a-data-package-online", 
            "text": "EPIC: As a Consumer I want to view a data package online so I can get a sense of whether this is the dataset I want   Obsess here about \u201cwhether this is the dataset I want\u201d  *Publishers want this too \u2026 *  Also important for SEO if we have good info here   Features   Visualize data in charts - gives one an immediate sense of what this is  One graph section at top of page after README excerpt  One graph for each entry in the \u201cviews\u201d    Interactive table - allows me to see what is in the table  One table for each resource     This user story can be viewed from two perspectives:   From a publisher point of view  From a consumer point of view   As a  publisher  i want to show the world how my published data is so that it immediately catches consumer\u2019s attention (and so I know it looks right - e.g. graph is ok)  As a  consumer  i want to view the data package so that i can get a sense of whether i want this dataset or not.  Acceptance criteria - what does done mean!   A table for each resource  Simple graph spec works => converts to plotly  Multiple time series    Plotly spec graphs work  All core graphs work (not sure how to check every one but representative ones)  Recline graphs specs (are handled - temporary basis)  Loading spinners whilst data is loading so users know what is happening   Bonus:   Complex examples e.g. time series with a log scale \u2026 (e.g. hard drive data \u2026)   Features:  DP view status   Different options to view data as graph.  Recline  Vega-lite  Vega  [Plotly]    General Functionality  Multiple views [wrongly done. We iterate over resource not views]  Table as a view    Interactive table so that consumer can do  Filter  Join", 
            "title": "View a Data Package Online"
        }, 
        {
            "location": "/developers/user-stories/#preview-a-not-yet-published-data-package-online", 
            "text": "As a (potential) Publisher I want to preview a datapackage I have prepared so that I can check it works and share the results (if there is something wrong with others)   Be able to supply a URL to my datapackage (e.g. on github) and have it previewed as it would look on DPR  Be able to upload a datapackage and have it previewed   Rufus: this was a very common use case for me (and others) when using data.okfn.org. Possibly less relevant if the command line tool can do previewing but still relevant IMO (some people may not have command line tool, and it is useful to be able to share a link e.g. when doing core datasets curation and there is something wrong with a datapackage).  Rufus: also need for an online validation tool", 
            "title": "(Pre)View a not-yet-published Data Package Online"
        }, 
        {
            "location": "/developers/user-stories/#see-how-much-a-data-package-is-used-downloaded-2d", 
            "text": "As a Consumer i want to see how much the data has been downloaded so that i can choose most popular (=  probably most reliable and complete) in the case when there are several alternatives for my usecase (maybe from different publishers)", 
            "title": "See How Much a Data Package is Used (Downloaded) {2d}"
        }, 
        {
            "location": "/developers/user-stories/#browse-data-packages", 
            "text": "As a potential Publisher, unaware of datapackages, I want to see real examples of published packages (with the contents datapackage.json), so that I can understand how useful and simple is the datapackage format and the registry itself.  As a Consumer I want to see some example data packages quickly so I get a sense of what is on this site and if it is useful to look further   Browse based on what properties? Most recent, most downloaded?  Most downloaded  Start with: we could just go with core data packages", 
            "title": "Browse Data Packages"
        }, 
        {
            "location": "/developers/user-stories/#search-for-data-packages", 
            "text": "As a Consumer I want to search data packages so that I can find the ones I want   Essential question: what is it you want?  Rufus: in my view generic search is actually *not* important to start with. People do not want to randomly search. More useful is to go via a publisher at the beginning.    Search results should provide enough information to help a user decide whether to dig further e.g. title, short description  For future when we have it: [number of downloads], stars etc     Minimum viable search (based on implementation questions)   Filter by publisher  Free text search against title  Description could be added if we start doing actual scoring as easy to add additional fields    Scoring would be nice but not essential     Implementation questions:   Search:  Should search perform ranking (that requires scoring support)  Free text queries should search against which fields (with what weighting)?    Filtering: On what individual properties of the data package should be able to filter?  Themes and profiles:  Searching for a given profile: not possible atm.  Themes: Should we tag data packages by themes like finance, education and let user find data package by that?\n        * Maybe but not now - maybe in the future    If we follow the go via a publisher at the beginning then should we list the most popular publisher on the home page of user[logged-in/ not logged in]?  If most popular publisher then by what mesaure?  Sort by Most published?  Sort by Most followers?  Sort by most downloads?  Or all show top5 in each facet?       Sub user stories:   [WONTFIX?] As a Consumer i want to find the data packages by profile (ex: spending) so that I can find the kind of data I want quickly and easily and in one big list  As a Consumer i want to search based on description of data package, so that I can find package which related to some key words", 
            "title": "Search for Data Packages"
        }, 
        {
            "location": "/developers/user-stories/#download-data-package-descriptor", 
            "text": "As a Consumer I want to download the data package descriptor (datapackage.json) on its own so that \u2026  *Rufus: I can\u2019t understand why anyone would want to do this *", 
            "title": "Download Data Package Descriptor"
        }, 
        {
            "location": "/developers/user-stories/#download-data-package-in-one-file-eg-zip", 
            "text": "As a Consumer I want to download the data package in one file so that I don\u2019t have to download descriptor and each resource by hand  Only useful if no cli tool and no install command", 
            "title": "Download Data Package in One File (e.g. zip)"
        }, 
        {
            "location": "/developers/user-stories/#4-install-a-data-package-locally", 
            "text": "Let\u2019s move discussion to the github:  https://github.com/frictionlessdata/dpm-py/issues/30  TODO add these details from the requirement doc   Local \u201cData Package\u201d cache storage (`.datapackages` or similar)  Stores copies of packages from Registry  Stores new Data Packages the user has created  This   Ruby lib   implements something similar", 
            "title": "4. Install a Data Package (locally)"
        }, 
        {
            "location": "/developers/user-stories/#use-datapackage-in-node-package-auto-generated", 
            "text": "As a NodeJS developer I want to use data package as a node lib in my project so that I can depend on it using my normal dependency framework   See this  real-world example  of this request for country-list  => auto-building node package and publishing to npm (not that hard to do \u2026)  Convert CSV data to json (that\u2019s what you probably want from node?)  Generate package.json  Push to npm (register the dataset users)  Rufus: My guess here is that to implement here we want something a bit like github integrations \u2013 specific additional hooks which also get some configuration (or do it like travis - github integration plus a tiny config file - in our case rather than a .travis.yml we have a .node.yml or whatever)    Is it configurable for user that enable to push to npm or not?  Yes. Since we need to push to a specific npm user (for each publisher) this will need to be configured (along with authorization - where does that go?)    Is this something done for *all* data packages or does user need to turn something on? Probably want them to turn this on \u2026   Questions:   From where we should push the data package to npm repo.  Is it from dpmpy or from server? Obviously from a server - this needs to be automated. But you can use dpmpy if you want (but I\u2019m not sure we do want to \u2026)    What to do with multiple resources? Ans: include all resources  Do we include datapackage.json into the node package? Yes, include it so they get all the original metadata.   Generic version is:  As a Web Developer I want to download a DataPackage (like currency codes or country names) so that I can use it in the web service I am building [...]", 
            "title": "Use DataPackage in Node (package auto-generated)"
        }, 
        {
            "location": "/developers/user-stories/#import-datapackage-into-r", 
            "text": "As a Consumer [R user] I want to load a Data Package from R so that I can immediately start playing with it   Should we try and publish to CRAN?  Probably not? Why? think it can be quite painful getting permission to publish to CRAN and very easy to load from the registry  On the CRAN website I can't find a way to automate publishing. It seems possible by filling web-form, but to know the status we have to wait and parse email.      Using this library:  https://github.com/ropenscilabs/datapkg  Where can i know about this?  On each data package view page \u2026     Generic version:  As a Data Analyst I want to download a data package, so that I can study it and wrangle with it to infer new data or generate new insights.  As a Data Analyst, I want to update previously downloaded data package, so that I can work with the most recent data.  Question:   Is the version structure would have been problem for the user. As out directory structure is e.g \u201cbits.datapackaged.com/metadata/core/demo-package/_v/latest/*\u201d? If now how to", 
            "title": "Import DataPackage into R"
        }, 
        {
            "location": "/developers/user-stories/#import-datapackage-into-pandas", 
            "text": "TODO - like R", 
            "title": "Import DataPackage into Pandas"
        }, 
        {
            "location": "/developers/user-stories/#sql-sqlite-database", 
            "text": "As a Consumer I want to download a DataPackage\u2019s data one coherent SQLite database so that I can get it easily in one form  Question:   Why does we need to store datapackage data in sqlite. Is not it better to store in file structure?   We can store the datapackage like this way:  ~/.datapackage/\\ publisher>/\\ package>/\\ version>/*  This is the way maven/gradle/ivy cache jar locally.", 
            "title": "SQL / SQLite database"
        }, 
        {
            "location": "/developers/user-stories/#see-changes-between-versions", 
            "text": "As a Data Analyst I want to compare different versions of some datapackage locally, so that I can see schema changes clearly and adjust my analytics code to the desired schema version.", 
            "title": "See changes between versions"
        }, 
        {
            "location": "/developers/user-stories/#low-priority", 
            "text": "As a Web Developer of multiple projects, I want to be able to install multiple versions of the same datapackage separately so that all my projects could be developed independently and deployed locally. (virtualenv-like)  As a Developer I want to list all DataPackages requirements for my project in the file and pin the exact versions of any DataPackage that my project depends on so that the project can be deterministically deployed locally and won\u2019t break because of the DataPackage schema changes. (requirements.txt-like)", 
            "title": "Low Priority"
        }, 
        {
            "location": "/developers/user-stories/#5-versioning-and-changes-in-data-packages", 
            "text": "When we talk about versioning we can mean two things:   Explicit versioning: this is like the versioning of releases \u201cv1.0\u201d etc. This is conscious and explicit. Main purpose:  *to support other systems depending on this one (they want the data at a known stable state) *  easy access to major staging points in the evolution (e.g. i want to see how things were at v1)    Implicit versioning or \u201crevisioning\u201d: this is like the commits in git or the autosave of a word or google doc. It happens frequently, either with minimum effort or even automatically. Main purpose:  Undelete and recovery (you save a every point and can recover if you accidentally write or delete something)  Collaboration and merging of changes (in revision control)  Activity logging", 
            "title": "5. Versioning and Changes in Data Packages"
        }, 
        {
            "location": "/developers/user-stories/#explicit-versioning-publisher", 
            "text": "As a Publisher I want to tag a version of my data on the command line so that \u2026 [see so that\u2019s below]  dpmpy tag {tag-name}  => tag current \u201clatest\u201d on the server as {tag-name}   Do we restrict {tag-name} to semver? I don\u2019t think so atm.  As a {Publisher} I want to tag datapackage to create a snapshot of data on the registry server, so that consumers can refer to it  As a {Publisher} I want to be warned that a tag exists, when I try to overwrite it, so that I don\u2019t accidentally overwrite stable tagged data, which is relied on by consumers.  As a {Publisher} I want to be able to overwrite the previously tagged datapackage, so that I can fix it if I mess up.  The versioning here happens server side  Is this confusing for users? I.e. they are doing something local.     Background \u201cso that\u201d user story epics:   As a {Publisher} I want to version my Data Package and keep multiple versions around including older versions so that I do not break consumer systems when I change my Data Package (whether schema or data) [It is not just the publisher who wants this, it is a consumer - see below]  As a {Publisher} I want to be able to get access to a previous version I tagged so that I can return to it and review it (and use it)  so that i can recover old data if i delete it myself or compare how things changed over time", 
            "title": "Explicit Versioning - Publisher"
        }, 
        {
            "location": "/developers/user-stories/#explicit-versioning-consumer", 
            "text": "As a {Consumer} (of a Data Package) I want to know full details when and how the data package schema has changed and when so that I can adjust my scripts to handle it.  Important info to know for each schema change:   time when published  for any  changed  field - name, what was changed (type, format, \u2026?),\n      +maybe everything else that was not changed (full field descriptor)  for any  deleted  field - name,\n      +maybe everything else (full field descriptor)  for any  added  field - all data (full field descriptor)   A change in schema would correspond to a major version change in software (see   http://semver.org/ ) .  Concerns about explicit versioning : we all have experience with consuming data from e.g. government publishers where the publishers change the data schema breaking client code. I am constatnly looking for a policy\\mechanism to guide publishers to develop stable schema versioning for the data they produce, and help consumers to get some stability guarantees.  Automated versioning / automated tracking : Explicit versioning relies on the publisher, and humans can forget or not care enough about others. So to help consumers my suggestion would be to always track schema changes of uploaded packages on the server, and allow users to review those changes on the website. (We might even want to implement auto-tagging or not allowing users to upload a package with the same version but a different schema without forcing)  As a {Consumer} I want to get a sense how outdated is the datapackage, that I have downloaded before, so that I can decide if I should update or not.   I want to preview a DataPackage changelog (list of all available versions\\tags with brief info) online, sorted by creation time, so that I can get a sense how data or schema has changed since some time in the past. Important brief info:  Time when published  How many rows added\\deleted for each resource data  What fields(column names) changed, added or deleted for each resource.     As a {Consumer} I want to view a Datapackage at a particular version online, so that I can present/discuss the particular data timeslice of interest with other people.  As a {Consumer} I want to download a Data package at a particular version so that I know it is compatible with my scripts and system   Online: I want to pick the version I want from the list, and download it (as zip for ex.)  CLI: I want to specify tag or version when using the `install` command.", 
            "title": "Explicit Versioning - Consumer"
        }, 
        {
            "location": "/developers/user-stories/#revisioning-implicit-versioning", 
            "text": "\u2026", 
            "title": "Revisioning - Implicit Versioning"
        }, 
        {
            "location": "/developers/user-stories/#change-notifications", 
            "text": "As a Consumer I want to be notified of changes to a package i care about so that I can check out what has changed and take action (like downloading the updated data)  As a Consumer I want to see how active the site is to see if I should get involved", 
            "title": "Change Notifications"
        }, 
        {
            "location": "/developers/user-stories/#6-publishers", 
            "text": "", 
            "title": "6. Publishers"
        }, 
        {
            "location": "/developers/user-stories/#create-a-new-publisher", 
            "text": "TODO", 
            "title": "Create a New Publisher"
        }, 
        {
            "location": "/developers/user-stories/#find-a-publisher-and-users", 
            "text": "As a Consumer I want to browse and find publishers so that I can find interesting publishers and their packages (so that I can use them)", 
            "title": "Find a Publisher (and users?)"
        }, 
        {
            "location": "/developers/user-stories/#view-a-publisher-profile", 
            "text": "view data packages associated to a publisher or user  Implementation details:  https://hackmd.io/MwNgrAZmCMAcBMBaYB2eAWR72woghmLNIrAEb4AME+08s6VQA===  As a Consumer I want to see a publisher\u2019s profile so that I can discover their packages and get a sense of how active and good they are  As a Publisher I want to have a profile with a list of my data packages so that:   Others can find my data packages quickly and easily  Can see how many data packages i have  I can find a data package i want to look at quickly [they can discover their own data]  I can find the link for a data package to send to someone else  People want to share what they have done. This is probably the number one way the site gets prominence at the start (along with simple google traffic)    so that I can check that members do not abuse their rights to publish and only publish topical data packages.   As a Consumer I want to view a publisher\u2019s profile so that I can see who is behind a particular package or to see what other packages they produce [navigate up from a package page] [so that: i can trust on his published data packages to reuse.]  Details   Profile =  Full name / title e.g. \u201cWorld Bank\u201d, identifier e.g. world-bank  picture, short description text (if we have this - we don\u2019t atm)  (esp important to know if this is the world bank or not)    Total number of data packages  List of data packages  View by most recently created (updated?)  For each DataPackage want to see: title, number of resources (?), first 200 character of description, license (see data.okfn.org/data/ for example)  Do we limit / paginate this list? No, not for the moment    [wontfix atm] Activity - this means data packages published, updated  [wontfix atm] Quality \u2026 - we don\u2019t have anything on this  [wontfix atm] List of users    What are the permissions here?  Do we show private data packages? No  Do we show them when \u201cowner\u201d viewing or sysadmin? Yes (but flag as \u201cprivate\u201d)         What data packages to show? All the packages you own.  What about pinning? No support for this atm.", 
            "title": "View a Publisher Profile"
        }, 
        {
            "location": "/developers/user-stories/#search-among-publishers-packages", 
            "text": "As a Consumer i want to search among all data packages owned by a publisher so that I can easily find one data package amongst all the data packages by this publisher.", 
            "title": "Search among publishers packages"
        }, 
        {
            "location": "/developers/user-stories/#registered-users-profile-and-packages", 
            "text": "As a Consumer i want to see the profile and activity of a user so that \u2026  As a Registered User I want to see the data packages i am associated with  so that  [like publisher]", 
            "title": "Registered Users Profile and packages"
        }, 
        {
            "location": "/developers/user-stories/#publisher-and-user-leaderboard", 
            "text": "As a ??? I want to see who are the top publihers and users so that I can emulate them or ???", 
            "title": "Publisher and User Leaderboard"
        }, 
        {
            "location": "/developers/user-stories/#manage-publisher", 
            "text": "", 
            "title": "Manage Publisher"
        }, 
        {
            "location": "/developers/user-stories/#create-and-edit-profile", 
            "text": "As {Owner ...} I want to edit my profile so that it is updated with new information", 
            "title": "Create and Edit Profile"
        }, 
        {
            "location": "/developers/user-stories/#add-and-manage-members", 
            "text": "As an {Owner of a Publisher in the Registry} I want to invite an existing user to become a member of my publisher   Auto lookup by user name (show username and fullname) - standard as per all sites  User gets a notification on their dashboard + email with link to accept invite  If invite is accepted notify the publisher (?) - actually do not do this.   As an {Owner of a Publisher in the Registry} I want to invite someone using their email to sign up and become a member of my Publisher so that they are authorized to publish data packages under my Publisher.  As an {Publisher Owner} I want to remove someone from membership in my publisher so they no longer have ability to publish or modify my data packages  As a {Publisher Owner} I want to view all the people in my organization and what roles they have so that I can change these if I want  As a {Publisher Owner} I want to make a user an \u201cowner\u201d so they have full control  As a {Publisher Owner} I want to remove a user as an \u201cowner\u201d so they are just a member and no longer have full control", 
            "title": "Add and Manage Members"
        }, 
        {
            "location": "/developers/user-stories/#7-web-hooks-and-extensions", 
            "text": "TODO: how do people build value added services around the system (and push back over the API etc \u2026) - OAuth etc", 
            "title": "7. Web Hooks and Extensions"
        }, 
        {
            "location": "/developers/user-stories/#8-administer-site", 
            "text": "", 
            "title": "8. Administer Site"
        }, 
        {
            "location": "/developers/user-stories/#configure-site", 
            "text": "As the Admin I want to set key configuration parameters for my site deployment so that I can change key information like the site title   Main config database is the one thing we might need", 
            "title": "Configure Site"
        }, 
        {
            "location": "/developers/user-stories/#see-usage-metrics", 
            "text": "As an Admin I want to see key metrics about usage such as users, API usage, downloads etc so that I know how things are going   Total users are signed up, how many signed up in last week / month etc  Total publishers \u2026  Users per publisher distribution (?)    API usage  Downloads  Billing: revenue in relevant periods  Costs: how much are we spending on storage", 
            "title": "See usage metrics"
        }, 
        {
            "location": "/developers/user-stories/#pricing-and-billing", 
            "text": "As an Admin I want to have a pricing plan and billing system so that I can charge users and make my platform sustainable  As a Publisher I want to know if this site has a pricing plan and what the prices are so that I can work out what this will cost me in the future and have a sense that these guys are sustainable (\u2018free forever\u2019 does not work very well)  As a Publisher I want to sign up for a given pricing plan so that I am entitled to what it allows (e.g. private stuff \u2026)", 
            "title": "Pricing and Billing"
        }, 
        {
            "location": "/developers/user-stories/#private-data-packages", 
            "text": "cf npmjs.com  As a Publisher I want to have private data packages that I can share just with my team", 
            "title": "Private Data Packages"
        }, 
        {
            "location": "/developers/user-stories/#sell-my-data-through-your-site", 
            "text": "EPIC: As a Publisher i want to sell my data through your site so that I make money and am able to sustain my publishing and my life \u2026", 
            "title": "Sell My Data through your site"
        }, 
        {
            "location": "/publishers/", 
            "text": "Publishers\n\n\nThis section of the DataHub documentation is for data publishers. Here you can learn about getting your data ready for loading into DataHub, and how you can interact with your data once it is loaded.\n\n\n\n\nPublishing a Data Package\n\n\nSign up \n get a secret key\n\n\nInstall command line tool\n\n\nConfigure\n\n\nPublish a dataset\n\n\nView it online\n\n\n\n\n\n\n\n\nPublishing a Data Package\n\n\nSign up \n get a secret key\n\n\nYou can sign up using your GitHub account. Once you are signed in, you will be redirected to a dashboard, where you can find your secret key (access token).\n\n\nInstall command line tool\n\n\nNext you need to install \ndpm\n - the data package manager command line tool:\n\n\n$ [sudo] pip install git+https://github.com/frictionlessdata/dpm-py.git\n\n\n\nConfigure\n\n\nYou will need the secret key (access token) to set your configurations:\n\n\n$ dpm configure\n\n\n Username:  \n your user name \n\n\n Your access_token:  \n you secret key \n\n\n Server URL: https://www.datapackaged.com\n\n\n\nNote: server URL may vary depending on application development stage\n\n\nPublish a dataset\n\n\nWe assume you know what a \nData Package\n is.\n\n\nGo to a directory where your data package is located and publish it:\n\n\n$ cd your-data-package-directory/\n$ dpm publish\n\n\n\nView it online\n\n\nOnce your data package is successfully published, you will get an URL to your dataset on the website. Open the URL in your favourite browser and explore it.", 
            "title": "Getting started"
        }, 
        {
            "location": "/publishers/#publishers", 
            "text": "This section of the DataHub documentation is for data publishers. Here you can learn about getting your data ready for loading into DataHub, and how you can interact with your data once it is loaded.   Publishing a Data Package  Sign up   get a secret key  Install command line tool  Configure  Publish a dataset  View it online", 
            "title": "Publishers"
        }, 
        {
            "location": "/publishers/#publishing-a-data-package", 
            "text": "", 
            "title": "Publishing a Data Package"
        }, 
        {
            "location": "/publishers/#sign-up-get-a-secret-key", 
            "text": "You can sign up using your GitHub account. Once you are signed in, you will be redirected to a dashboard, where you can find your secret key (access token).", 
            "title": "Sign up &amp; get a secret key"
        }, 
        {
            "location": "/publishers/#install-command-line-tool", 
            "text": "Next you need to install  dpm  - the data package manager command line tool:  $ [sudo] pip install git+https://github.com/frictionlessdata/dpm-py.git", 
            "title": "Install command line tool"
        }, 
        {
            "location": "/publishers/#configure", 
            "text": "You will need the secret key (access token) to set your configurations:  $ dpm configure  Username:    your user name    Your access_token:    you secret key    Server URL: https://www.datapackaged.com  Note: server URL may vary depending on application development stage", 
            "title": "Configure"
        }, 
        {
            "location": "/publishers/#publish-a-dataset", 
            "text": "We assume you know what a  Data Package  is.  Go to a directory where your data package is located and publish it:  $ cd your-data-package-directory/\n$ dpm publish", 
            "title": "Publish a dataset"
        }, 
        {
            "location": "/publishers/#view-it-online", 
            "text": "Once your data package is successfully published, you will get an URL to your dataset on the website. Open the URL in your favourite browser and explore it.", 
            "title": "View it online"
        }, 
        {
            "location": "/publishers/core-datasets/", 
            "text": "Core Datasets\n\n\nImportant, commonly-used datasets as high quality, easy-to-use \n open data packages\n\n\nCore Datasets are important, commonly-used \n\"core\" datasets\n like GDP or country codes made available as \nhigh-quality\n, \neasy-to-use\n and \nopen\n \ndata packages\n. Find them online here on the DataHub:\n\n\nhttp://datapackaged.com/core/\n\n\nKey features are:\n\n\n\n\nHigh Quality \n Reliable\n -- sourcing, normalizing and quality checking a set of \nkey reference and indicator datasets such as country codes, currencies, GDP and population\n\n\nStandardized \n Bulk\n -- all datasets provided in a \nstandardized\n form and can be accessed in \nbulk as CSV\n together with a simple \nJSON schema\n\n\nVersioned \n Packaged\n -- all data is in \ndata packages\n and is \nversioned\n using git so all changes are visible and data can be \ncollaboratively maintained\n\n\n\n\nThe \"Core Datasets\" effort is part of the broader \nFrictionless Data initiative\n.\n\n\n\n\n\n\nCore Data Curators\n\n\nThe Core Data Curators curate the core datasets.\n\n\nCuration involves identifying and locating core (public) datasets, then packaging them up as high-quality, reliable, and easy-to-use \ndata packages\n (standardized, structured, open).\n\n\nNew team members wanted:\n We are always seeking volunteers to join the Data Curators team. Get to be part of a crack team and develop and hone your data wrangling skills whilst helping to provide high quality data to the community.\n\n\n\n\nAnyone can contribute\n: details on the \nroles and skills needed below\n.\n\n\nGet involved\n: read more below or jump straight to \nthe sign-up section\n.\n\n\nData Curators Guide\n: can't wait to get started as a Data Curator? You can dive straight in and start packaging datasets using the \ncore data curators guide\n.\n\n\n\n\n\n\n\n\n\nWhat Roles and Skills are Needed\n\n\nWe have a variety of roles from identifying new \"core\" datasets, to collecting and packaging the data, to performing quality control.\n\n\nCore Skills\n -- at least one of these skills is strongly recommended:\n\n\n\n\nData Wrangling Experience\n. Many of our source datasets are not complex (just an Excel file or similar) and can be \"wrangled\" in a Spreadsheet program. What we therefore recommend is at least one of:\n\n\nExperience with a Spreadsheet application such as Excel or (preferably) Google Docs including use of formulas and (desirably) macros (you should at least know how you could quickly convert a cell containing '2014' to '2014-01-01' across 1000 rows)\n\n\nCoding for data processing (especially scraping) in one or more of python, javascript, bash\n\n\n\n\n\n\nData sleuthing\n - the ability to dig up data on the web (specific desirable skills: you know how to search by filetype in google, you know where the developer tools are in chrome or firefox, you know how to find the URL a form posts to)\n\n\n\n\nDesirable Skills\n (the more the better!):\n\n\n\n\nData vs Metadata: know difference between data and metadata\n\n\nFamiliarity with Git (and Github)\n\n\nFamiliarity with a command line (preferably bash)\n\n\nKnow what JSON is\n\n\nMac or Unix is your default operating system (will make access to relevant tools that much easier)\n\n\nKnowledge of Web APIs and/or HTML\n\n\nUse of curl or similar command line tool for accessing Web APIs or web pages\n\n\nScraping using a command line tool or (even better) by coding yourself\n\n\nKnow what a Data Package and a Tabular Data Package are\n\n\nKnow what a text editor is (e.g. notepad, textmate, vim, emacs, ...) and know how to use it (useful for both working with data and for editing Data Package metadata)\n\n\n\n\n\n\nGet Involved - Sign Up Now!\n\n\nHere's what you need to know when you sign up:\n\n\n\n\nTime commitment\n: Members of the team commit to at least 8-16h per month (though this will be an average - if you are especially busy with other things one month and do less that is fine)\n\n\nSchedule\n: There is no schedule so you can contribute at any time that is good for you - evenings, weekeneds, lunch-times etc\n\n\nLocation\n: all activity will be carried out online so you can be based anywhere in the world\n\n\nSkills\n: see above\n\n\n\n\nTo register your interest fill in the following form. Any questions, please \nget in touch directly\n.\n\n\nLoading...", 
            "title": "Core Datasets"
        }, 
        {
            "location": "/publishers/core-datasets/#core-datasets", 
            "text": "Important, commonly-used datasets as high quality, easy-to-use   open data packages  Core Datasets are important, commonly-used  \"core\" datasets  like GDP or country codes made available as  high-quality ,  easy-to-use  and  open   data packages . Find them online here on the DataHub:  http://datapackaged.com/core/  Key features are:   High Quality   Reliable  -- sourcing, normalizing and quality checking a set of  key reference and indicator datasets such as country codes, currencies, GDP and population  Standardized   Bulk  -- all datasets provided in a  standardized  form and can be accessed in  bulk as CSV  together with a simple  JSON schema  Versioned   Packaged  -- all data is in  data packages  and is  versioned  using git so all changes are visible and data can be  collaboratively maintained   The \"Core Datasets\" effort is part of the broader  Frictionless Data initiative .", 
            "title": "Core Datasets"
        }, 
        {
            "location": "/publishers/core-datasets/#core-data-curators", 
            "text": "The Core Data Curators curate the core datasets.  Curation involves identifying and locating core (public) datasets, then packaging them up as high-quality, reliable, and easy-to-use  data packages  (standardized, structured, open).  New team members wanted:  We are always seeking volunteers to join the Data Curators team. Get to be part of a crack team and develop and hone your data wrangling skills whilst helping to provide high quality data to the community.   Anyone can contribute : details on the  roles and skills needed below .  Get involved : read more below or jump straight to  the sign-up section .  Data Curators Guide : can't wait to get started as a Data Curator? You can dive straight in and start packaging datasets using the  core data curators guide .", 
            "title": "Core Data Curators"
        }, 
        {
            "location": "/publishers/core-datasets/#what-roles-and-skills-are-needed", 
            "text": "We have a variety of roles from identifying new \"core\" datasets, to collecting and packaging the data, to performing quality control.  Core Skills  -- at least one of these skills is strongly recommended:   Data Wrangling Experience . Many of our source datasets are not complex (just an Excel file or similar) and can be \"wrangled\" in a Spreadsheet program. What we therefore recommend is at least one of:  Experience with a Spreadsheet application such as Excel or (preferably) Google Docs including use of formulas and (desirably) macros (you should at least know how you could quickly convert a cell containing '2014' to '2014-01-01' across 1000 rows)  Coding for data processing (especially scraping) in one or more of python, javascript, bash    Data sleuthing  - the ability to dig up data on the web (specific desirable skills: you know how to search by filetype in google, you know where the developer tools are in chrome or firefox, you know how to find the URL a form posts to)   Desirable Skills  (the more the better!):   Data vs Metadata: know difference between data and metadata  Familiarity with Git (and Github)  Familiarity with a command line (preferably bash)  Know what JSON is  Mac or Unix is your default operating system (will make access to relevant tools that much easier)  Knowledge of Web APIs and/or HTML  Use of curl or similar command line tool for accessing Web APIs or web pages  Scraping using a command line tool or (even better) by coding yourself  Know what a Data Package and a Tabular Data Package are  Know what a text editor is (e.g. notepad, textmate, vim, emacs, ...) and know how to use it (useful for both working with data and for editing Data Package metadata)", 
            "title": "What Roles and Skills are Needed"
        }, 
        {
            "location": "/publishers/core-datasets/#get-involved-sign-up-now", 
            "text": "Here's what you need to know when you sign up:   Time commitment : Members of the team commit to at least 8-16h per month (though this will be an average - if you are especially busy with other things one month and do less that is fine)  Schedule : There is no schedule so you can contribute at any time that is good for you - evenings, weekeneds, lunch-times etc  Location : all activity will be carried out online so you can be based anywhere in the world  Skills : see above   To register your interest fill in the following form. Any questions, please  get in touch directly .  Loading...", 
            "title": "Get Involved - Sign Up Now!"
        }, 
        {
            "location": "/publishers/core-data-curators/", 
            "text": "Core Data Curators Guide\n\n\nThis is a guide for curators of \n\"core datasets\"\n. Curators collect and maintain these important and commonly-used (\u201ccore\u201d) datasets as high-quality, easy-to-use, open \nData Packages\n.\n\n\nQuick Links\n\n\n\n\nDiscussion forum\n - discussion takes place here by default\n\n\nThis is the place to ask questions, get help etc - just open a new topic\n\n\nIntroduction to Core Datasets Project\n\n\nJoin the Core Data Curators Team\n\n\nPackaging Queue (GitHub Issues Tracker)\n\n\nPublish Data Packages Howto on Frictionless Data Site\n\n\n\n\nQuick Start\n\n\n\n\nPlease take 2m to introduce yourself in the \ndiscussion forum\n so that other team members can get to know you\n\n\nRead the contributing guide below so you:\n\n\nunderstand the details of the curator workflow\n\n\ncan work out where you'd like to contribute\n\n\n\n\n\n\nStop: have you read the contributing guide? The next items only make sense if you have!\n\n\nNow you can dive in with one or both of:\n\n\nResearching: start reviewing the \ncurrent queue\n - add new items, comment on existing ones etc\n\n\nPackaging:  check out the \n\u201cReady to Package\u201d\n section of the queue and assign yourself (drop a comment in the issue claiming it)\n\n\n\n\n\n\n\n\nContributor Guide\n\n\n\n\nFig 1: Overview of the Curation Workflow\n\n\nThere are 2 areas of activity:\n\n\n\n\nPreparing datasets as Core Data Packages - finding them, cleaning them, data-packaging them\n\n\nMaintaining Core Data Packages - keeping them up to date with the source dataset, handling changes, responding to user queries\n\n\n\n\nEach of these has sub-steps which we detail below and you can contribute in any and all of these.\n\n\nKey principles of our approach are that:\n\n\n\n\nWe package data rather than create it \u2013 our focus is to take source data and ensure it is of high quality and in a standard form\n\n\nWe preserve a clean separation between the data source, the data package and this registry \u2013 for example, data packages are stored in git repos hosted separately (preferably github)\n\n\n\n\nPreparing Datasets as Core Data Packages\n\n\nThere are different areas where people can contribute:\n\n\n\n\nResearch\n\n\nPackaging up data\n\n\nQuality assurance\n\n\nFinal Publication into the official core datasets list\n\n\n\n\nOften you will contribute in all 4 by taking a dataset all the way from a suggestion to a fully packaged data package published online.\n\n\n1. Research\n\n\nThis involves researching and selecting datasets as core datasets and adding them to the queue for packaging - no coding or data wrangling skill is needed for this\n\n\n\n\nTo propose a dataset for addition you \nopen an issue in the Registry\n with the details of the proposed dataset.\n\n\nIdentify relevant source or sources for the dataset\n\n\nTo propose a dataset you do not have to know where to get the data from (e.g. you could suggest \u201cUS GDP\u201d as a core dataset without yet knowing where to get the data from)\n\n\nDiscuss with Queue Manager(s) (they will spot your submission and start commenting in the GitHub issue)\n\n\nIf good =\n Shortlist for Packaging - add \nLabel \u201cStatus: Ready to Package\u201d\n\n\n\n\n2. Packaging up data\n\n\nOnce we have a suggested dataset marked as \"ready to package\" we can move to packaging it up.\n\n\nHow to package up data is covered in the \ngeneral publishing guide\n.\n\n\n3. Quality Assurance\n\n\nThis involves validating and checking packaged datasets to ensure they are of high quality and ready to publish.\n\n\n\n\nValidate\n the Data Package and \nreview\n the data in the Data Package.\n\n\nIn the review phase, you should be looking at a table with the data you have input before. That will ensure your data package is working without any issues and that it follows the same quality standards that any other package.\n\n\nPost a validation link and a view link in the comments for the issue in the Registry related to your Data Package.\n\n\n\n\n4. Publishing\n\n\nWe have a few extra specific requirements:\n\n\n\n\nAll Data Packages must (ultimately) be stored in a public GitHub repo\n\n\nFirst publish to your own repository\n\n\nThen arrange a move the repository to \ngithub.com/datasets/ organization\n - as the owner of a repository you can initiate a transfer request to github.com/datasets/ which can then be approved\n\n\nAdd to the \ncatalog list\n \nand\n the \ncore list\n \nand\n the associated csv files: \ncatalog-list.csv\n and \ncore-list.csv\n.\n\n\nReload \nhttp://data.okfn.org/data/\n by visiting \nhttp://data.okfn.org/admin/reload/\n\n\nIf you have access, tweet from the @OKFNLabs account a link to the \nhttp://data.okfn.org/data/\n page for the dataset.\n\n\n\n\nMaintaining Data Packages\n\n\nMany data packages package data that changes over time - for example, many time series get updated monthly or daily.\n\n\nWe need people to become the \"maintainer\" for a given dataset and keep it up to date by regularly adding in the new data.\n\n\nList of datasets needing a maintainer\n\n\nCore Data Assessment Criteria\n\n\nFor a dataset to be designated as \"core\" it should meet the following criteria:\n\n\n\n\nQuality - the dataset must be well structured\n\n\nRelevance and importance - the focus at present is on indicators and reference data\n\n\nOngoing support - it should have a maintainer\n\n\nOpenness - data should be \nopen data\n and openly licensed in accordance with the \nOpen Definition\n\n\n\n\n\n\nGuide for Managing Curators\n\n\nIntro Email for New Joiners\n\n\nHi,\n\nWe are delighted to welcome you to the Core Data Curators team of crack data curators.\n\nTo kick-off your core data curatorship we invite you to:\n\n1. Introduce yourself in the forum here:\n\n    http://discuss.okfn.org/t/core-data-curators-introductions/145/24\n\n2. Take a look at the Core Data Curators guide:\n\n    http://docs.datahub.io/publishers/core-data-curators\n\nRegards,\n\nXXX", 
            "title": "Core Data Curators"
        }, 
        {
            "location": "/publishers/core-data-curators/#core-data-curators-guide", 
            "text": "This is a guide for curators of  \"core datasets\" . Curators collect and maintain these important and commonly-used (\u201ccore\u201d) datasets as high-quality, easy-to-use, open  Data Packages .", 
            "title": "Core Data Curators Guide"
        }, 
        {
            "location": "/publishers/core-data-curators/#quick-links", 
            "text": "Discussion forum  - discussion takes place here by default  This is the place to ask questions, get help etc - just open a new topic  Introduction to Core Datasets Project  Join the Core Data Curators Team  Packaging Queue (GitHub Issues Tracker)  Publish Data Packages Howto on Frictionless Data Site", 
            "title": "Quick Links"
        }, 
        {
            "location": "/publishers/core-data-curators/#quick-start", 
            "text": "Please take 2m to introduce yourself in the  discussion forum  so that other team members can get to know you  Read the contributing guide below so you:  understand the details of the curator workflow  can work out where you'd like to contribute    Stop: have you read the contributing guide? The next items only make sense if you have!  Now you can dive in with one or both of:  Researching: start reviewing the  current queue  - add new items, comment on existing ones etc  Packaging:  check out the  \u201cReady to Package\u201d  section of the queue and assign yourself (drop a comment in the issue claiming it)", 
            "title": "Quick Start"
        }, 
        {
            "location": "/publishers/core-data-curators/#contributor-guide", 
            "text": "Fig 1: Overview of the Curation Workflow  There are 2 areas of activity:   Preparing datasets as Core Data Packages - finding them, cleaning them, data-packaging them  Maintaining Core Data Packages - keeping them up to date with the source dataset, handling changes, responding to user queries   Each of these has sub-steps which we detail below and you can contribute in any and all of these.  Key principles of our approach are that:   We package data rather than create it \u2013 our focus is to take source data and ensure it is of high quality and in a standard form  We preserve a clean separation between the data source, the data package and this registry \u2013 for example, data packages are stored in git repos hosted separately (preferably github)", 
            "title": "Contributor Guide"
        }, 
        {
            "location": "/publishers/core-data-curators/#preparing-datasets-as-core-data-packages", 
            "text": "There are different areas where people can contribute:   Research  Packaging up data  Quality assurance  Final Publication into the official core datasets list   Often you will contribute in all 4 by taking a dataset all the way from a suggestion to a fully packaged data package published online.", 
            "title": "Preparing Datasets as Core Data Packages"
        }, 
        {
            "location": "/publishers/core-data-curators/#1-research", 
            "text": "This involves researching and selecting datasets as core datasets and adding them to the queue for packaging - no coding or data wrangling skill is needed for this   To propose a dataset for addition you  open an issue in the Registry  with the details of the proposed dataset.  Identify relevant source or sources for the dataset  To propose a dataset you do not have to know where to get the data from (e.g. you could suggest \u201cUS GDP\u201d as a core dataset without yet knowing where to get the data from)  Discuss with Queue Manager(s) (they will spot your submission and start commenting in the GitHub issue)  If good =  Shortlist for Packaging - add  Label \u201cStatus: Ready to Package\u201d", 
            "title": "1. Research"
        }, 
        {
            "location": "/publishers/core-data-curators/#2-packaging-up-data", 
            "text": "Once we have a suggested dataset marked as \"ready to package\" we can move to packaging it up.  How to package up data is covered in the  general publishing guide .", 
            "title": "2. Packaging up data"
        }, 
        {
            "location": "/publishers/core-data-curators/#3-quality-assurance", 
            "text": "This involves validating and checking packaged datasets to ensure they are of high quality and ready to publish.   Validate  the Data Package and  review  the data in the Data Package.  In the review phase, you should be looking at a table with the data you have input before. That will ensure your data package is working without any issues and that it follows the same quality standards that any other package.  Post a validation link and a view link in the comments for the issue in the Registry related to your Data Package.", 
            "title": "3. Quality Assurance"
        }, 
        {
            "location": "/publishers/core-data-curators/#4-publishing", 
            "text": "We have a few extra specific requirements:   All Data Packages must (ultimately) be stored in a public GitHub repo  First publish to your own repository  Then arrange a move the repository to  github.com/datasets/ organization  - as the owner of a repository you can initiate a transfer request to github.com/datasets/ which can then be approved  Add to the  catalog list   and  the  core list   and  the associated csv files:  catalog-list.csv  and  core-list.csv .  Reload  http://data.okfn.org/data/  by visiting  http://data.okfn.org/admin/reload/  If you have access, tweet from the @OKFNLabs account a link to the  http://data.okfn.org/data/  page for the dataset.", 
            "title": "4. Publishing"
        }, 
        {
            "location": "/publishers/core-data-curators/#maintaining-data-packages", 
            "text": "Many data packages package data that changes over time - for example, many time series get updated monthly or daily.  We need people to become the \"maintainer\" for a given dataset and keep it up to date by regularly adding in the new data.  List of datasets needing a maintainer", 
            "title": "Maintaining Data Packages"
        }, 
        {
            "location": "/publishers/core-data-curators/#core-data-assessment-criteria", 
            "text": "For a dataset to be designated as \"core\" it should meet the following criteria:   Quality - the dataset must be well structured  Relevance and importance - the focus at present is on indicators and reference data  Ongoing support - it should have a maintainer  Openness - data should be  open data  and openly licensed in accordance with the  Open Definition", 
            "title": "Core Data Assessment Criteria"
        }, 
        {
            "location": "/publishers/core-data-curators/#guide-for-managing-curators", 
            "text": "", 
            "title": "Guide for Managing Curators"
        }, 
        {
            "location": "/publishers/core-data-curators/#intro-email-for-new-joiners", 
            "text": "Hi,\n\nWe are delighted to welcome you to the Core Data Curators team of crack data curators.\n\nTo kick-off your core data curatorship we invite you to:\n\n1. Introduce yourself in the forum here:\n\n    http://discuss.okfn.org/t/core-data-curators-introductions/145/24\n\n2. Take a look at the Core Data Curators guide:\n\n    http://docs.datahub.io/publishers/core-data-curators\n\nRegards,\n\nXXX", 
            "title": "Intro Email for New Joiners"
        }, 
        {
            "location": "/publishers/cli/", 
            "text": "DATA: The Data Package Manager CLI\n\n\n\n\nGetting started\n\n\nInstallation\n\n\nCommands\n\n\nConfiguration\n\n\n\n\nUsage\n\n\n\n\nPublish\n\n\nDownload\n\n\nDelete\n\n\nInformation\n\n\nNormalize\n\n\nValidate\n\n\nConfiguration\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\n\n\nGetting started\n\n\nThe data is a command-line tool aimed to help publishers to prepare and upload data to the DataHub. With data you will be able to:\n\n\n\n\nPublish Data Package to DataHub\n\n\nGet Data Package from DataHub\n\n\nRemove uploaded Data Package from DataHub\n\n\nGet information about particular Data Package\n\n\nNormalize Data Package according to the specs\n\n\nValidate your data to ensure its quality\n\n\nSet up configuration file in order to publish\n\n\n\n\nInstallation\n\n\nInstalling binaries without npm\n\n\nOn the \nreleases\n page, you can download pre-built binaries for MacOS and LinuxOS x64. You may need to put the pre-built binary in the bin directory (e.g.: /usr/local/bin/).\n\n\nmv path/to/data-{os-distribution} /usr/local/bin/data\n\n\n\nInstalling from npm\n\n\nYou can also install it from \nnpm\n as follows:\n\nnpm install -g data\n\n\nCommands\n\n\nYou can see the latest commands and get help by doing:\n\n\ndata --help\n\n\n\nThe output of the help command:\n\n\n\u2752 data [options] \ncommand\n \nargs\n\n\nCommands:\n  DataHub:\n    push        [path]        Push data to the DataHub\n    get         [pkg-id]      Get data from DataHub\n    purge       [owner/name]  Permanently deletes data from DataHub\n  Data Package specific:\n    info        [pkg-id]      Get info on data\n    normalize                 Normalize datapackage.json\n    validate                  Validate Data Package structure\n\n  Administrative:\n    config                    Set up configuration\n    help        [cmd]         Show help on cmd\n\nOptions:\n-h, --help              Output usage information\n-v, --version           Output the version\n\n\n\n\nConfiguration\n\n\nData can be configured using \ndata config[ure]\n command. It will ask you to provide a username, secretToken, server and bitStore addresses of DataHub.\n\n\nThe config is stored in \n~/.datahub/config\n, you can edit it with text editor.\nSimple example config file can look like this:\n\n\nusername = myname\naccess_token = mykey\nserver = server URL for publishing Eg: https://www.datapackaged.com\n\n\n\nUsage\n\n\nPublish\n\n\nTo publish a Data Package, go to the Data Package directory (with \ndatapackage.json\n) and\nrun:\n\n\ndata push\n\n\n\nIf your configured \nusername\n and \nsecretToken\n are correct, data will\nupload datapackage.json and all relevant resources to the DataHub server.\n\n\nGet\n\n\nTo get Data Package run the following command:\n\ndata get \npublisher\n/\npackage\n\nNew Data Package will be downloaded into current working directory. \n\n\nDelete\n\n\nTo delete permanently Data Package from DataHub, you can use \ndpm purge\n command:\n\n\ndata purge\n\n\n\nInformation\n\n\nYou can get information about particular Data Package\n\n\ndata info\n\n\n\nNormalize\n\n\nTo normalize Data Package descriptor according to the specs\n\n\ndata norm[alize] [path]\n\n\n\nValidate\n\n\nTo validate Data Package descriptor against schema\n\n\ndata validate [path | URL]\n\n\n\nConfiguration\n\n\nTo set up configuration file:\n\ndata config[ure]\n\n\nLinks\n\n\n\n\nCode repo", 
            "title": "CLI"
        }, 
        {
            "location": "/publishers/cli/#data-the-data-package-manager-cli", 
            "text": "Getting started  Installation  Commands  Configuration   Usage   Publish  Download  Delete  Information  Normalize  Validate  Configuration     Links", 
            "title": "DATA: The Data Package Manager CLI"
        }, 
        {
            "location": "/publishers/cli/#getting-started", 
            "text": "The data is a command-line tool aimed to help publishers to prepare and upload data to the DataHub. With data you will be able to:   Publish Data Package to DataHub  Get Data Package from DataHub  Remove uploaded Data Package from DataHub  Get information about particular Data Package  Normalize Data Package according to the specs  Validate your data to ensure its quality  Set up configuration file in order to publish", 
            "title": "Getting started"
        }, 
        {
            "location": "/publishers/cli/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/publishers/cli/#installing-binaries-without-npm", 
            "text": "On the  releases  page, you can download pre-built binaries for MacOS and LinuxOS x64. You may need to put the pre-built binary in the bin directory (e.g.: /usr/local/bin/).  mv path/to/data-{os-distribution} /usr/local/bin/data", 
            "title": "Installing binaries without npm"
        }, 
        {
            "location": "/publishers/cli/#installing-from-npm", 
            "text": "You can also install it from  npm  as follows: npm install -g data", 
            "title": "Installing from npm"
        }, 
        {
            "location": "/publishers/cli/#commands", 
            "text": "You can see the latest commands and get help by doing:  data --help  The output of the help command:  \u2752 data [options]  command   args \n\nCommands:\n  DataHub:\n    push        [path]        Push data to the DataHub\n    get         [pkg-id]      Get data from DataHub\n    purge       [owner/name]  Permanently deletes data from DataHub\n  Data Package specific:\n    info        [pkg-id]      Get info on data\n    normalize                 Normalize datapackage.json\n    validate                  Validate Data Package structure\n\n  Administrative:\n    config                    Set up configuration\n    help        [cmd]         Show help on cmd\n\nOptions:\n-h, --help              Output usage information\n-v, --version           Output the version", 
            "title": "Commands"
        }, 
        {
            "location": "/publishers/cli/#configuration", 
            "text": "Data can be configured using  data config[ure]  command. It will ask you to provide a username, secretToken, server and bitStore addresses of DataHub.  The config is stored in  ~/.datahub/config , you can edit it with text editor.\nSimple example config file can look like this:  username = myname\naccess_token = mykey\nserver = server URL for publishing Eg: https://www.datapackaged.com", 
            "title": "Configuration"
        }, 
        {
            "location": "/publishers/cli/#usage", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/publishers/cli/#publish", 
            "text": "To publish a Data Package, go to the Data Package directory (with  datapackage.json ) and\nrun:  data push  If your configured  username  and  secretToken  are correct, data will\nupload datapackage.json and all relevant resources to the DataHub server.", 
            "title": "Publish"
        }, 
        {
            "location": "/publishers/cli/#get", 
            "text": "To get Data Package run the following command: data get  publisher / package \nNew Data Package will be downloaded into current working directory.", 
            "title": "Get"
        }, 
        {
            "location": "/publishers/cli/#delete", 
            "text": "To delete permanently Data Package from DataHub, you can use  dpm purge  command:  data purge", 
            "title": "Delete"
        }, 
        {
            "location": "/publishers/cli/#information", 
            "text": "You can get information about particular Data Package  data info", 
            "title": "Information"
        }, 
        {
            "location": "/publishers/cli/#normalize", 
            "text": "To normalize Data Package descriptor according to the specs  data norm[alize] [path]", 
            "title": "Normalize"
        }, 
        {
            "location": "/publishers/cli/#validate", 
            "text": "To validate Data Package descriptor against schema  data validate [path | URL]", 
            "title": "Validate"
        }, 
        {
            "location": "/publishers/cli/#configuration_1", 
            "text": "To set up configuration file: data config[ure]", 
            "title": "Configuration"
        }, 
        {
            "location": "/publishers/cli/#links", 
            "text": "Code repo", 
            "title": "Links"
        }, 
        {
            "location": "/publishers/views/", 
            "text": "Views\n\n\n\n\nIntroduction\n\n\nMotivation\n\n\nConcepts and Background\n\n\n\n\n\n\nExamples of using views\n\n\nSimple graph spec\n\n\nVega graphs\n\n\nMaps\n\n\nTables and Transforms\n\n\n\n\n\n\n\n\nIntroduction\n\n\nMotivation\n\n\nProducers and consumers of data [packages] want to have their data presented in tables and graphs -- \"views\" on the data.\n\n\nWhy? For a range of reasons -- from simple eyeballing to drawing out key insights.\n\n\n\ngraph LR\n  data[Your Data] --> table[Table]\n  data --> grap[Graph]\n\n  classDef implemented fill:lightblue,stroke:#333,stroke-width:4px;\n  class abc implemented;\n\n\n\n\nTo achieve this we need to provide:\n\n\n\n\nA tool-chain to create these views from the data.\n\n\nA descriptive language for specifying views such as tables, graphs, map.\n\n\n\n\nThese requirements are addressed through the introduction of Data Package \"Views\" and associated tooling.\n\n\n\ngraph LR\n\n  subgraph Data Package\n    resource[Resource]\n    view[View]\n    resource -.-> view\n  end\n\n  view --> toolchain\n  toolchain --> svg[\"Rendered Graph (SVG)\"]\n  toolchain --> table[Table]\n\n\n\n\nWe take a \"running code\" approach -- as in the rest of the Frictionless Data effort. We develop the spec by building a working implementation, and iterate around the spec as we develop. As development progresses the spec crystallizes and becomes ever less subject to change.\n\n\nOur current implementation efforts focus on provide javascript tools and building a data package registry (DPR).\n\n\nConcepts and Background\n\n\nTo generate visualizations you usually want the following 3 types of information:\n\n\n\n\nmetadata: e.g. title of graph, credits etc\n\n\ngraph: description / specification of the graph itself\n\n\ndata: specification of data sources for the graph including location and key metadata like types\n\n\n\n\nThe data spec itself often consists of three distinct parts:\n\n\n\n\n\"raw / graph data\": a spec / description of data exactly in the form needed by the visualization system. This is often a very well defined spec e.g. an array of series.\n\n\nlocate/describe: a spec of where to get data from e.g. \nurl\n or \ndata\n attribute plus some information on that data such as format and types.\n\n\ntransform: a spec of how transform external data prior to use e.g. pivoting or filtering it\n\n\n\n\nFrom this description it should be clear that the latter two data specs -- locate/describe and transform -- are actually generic and independent of the specific graphing library. The only thing the graphing library really needs is a clear description of the \"raw\" format which it directly consumes. Thus, we can consider a natural grouping of specs as:\n\n\n\n\ngeneral-metadata - e.g. title of graph, credits etc [provided by e.g. Data Package / define yourself!]\n\n\ndata: sourcing and transform [provided by e.g. Data Resource]\n\n\nsourcing: how to source data from external sources\n\n\ntransform: how to transform data e.g. pivot it, select one field, scale a field etc\n\n\n\n\n\n\ngraph description / specification [provided by e.g. Vega]\n\n\ngraph data (raw): data as directly consumed by graph spec (usually JSON based if we are talking about JS web-based visualization)\n\n\n\n\n\n\n\n\nHowever, in many visualization tools -- including specs like Vega -- these items are combined together. This is understandable as these tools seek to off users a \"complete solution\". However, \ndecoupling these parts and having clearly defined interfaces would offer significant benefits\n:\n\n\n\n\nExtensibility: it would be easier to extend and adapt the system. For example, adding new data import options could be done without changing the graph system.\n\n\nComposability: we can combine different parts together in different ways. For example, data import and transformation could be used for generating data for tabular display as well as graphing.\n\n\nReusability: we want to reuse existing tools and specifications wherever possible. If we keep the specs relatively separate we can reuse the best spec for each job.\n\n\nReliability: when the system is decoupled it is easier to test and check.\n\n\n\n\nIn summary, a smaller pieces, loosely joined makes it easier to adapt and evolve the specs and the associated tooling.\n\n\nExamples of using views\n\n\nIn this section, examples of using Data Package views are provided. Each example has a README section with small tutorial.\n\n\nSimple graph spec\n\n\nSimple graph spec is the easiest and quickest way to specify a view in a Data Package. Using simple graph spec publishers can generate graphs, e.g., line and bar charts.\n\n\n\n\nSimple Graph Spec Tutorial\n\n\n\n\nVega graphs\n\n\nPublishers can also describe graphs using Vega specifications:\n\n\n\n\nVega Graph Spec Tutorial - Yields of Barley\n\n\nVega Graph Spec Tutorial - US presidents\n\n\nVega Graph Spec Tutorial - US Airports\n\n\n\n\nMaps\n\n\nAt the moment, we only support \n.geojson\n format:\n\n\n\n\nGeoJSON Tutorial\n\n\n\n\nTables and Transforms\n\n\nIn the following examples, we demonstrate how transforms can be used in Data Package views. Transformed data will be displayed as table views.\n\n\n\n\nFilter \n Formula\n\n\nSample\n\n\nAggregate", 
            "title": "Views"
        }, 
        {
            "location": "/publishers/views/#views", 
            "text": "Introduction  Motivation  Concepts and Background    Examples of using views  Simple graph spec  Vega graphs  Maps  Tables and Transforms", 
            "title": "Views"
        }, 
        {
            "location": "/publishers/views/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/publishers/views/#motivation", 
            "text": "Producers and consumers of data [packages] want to have their data presented in tables and graphs -- \"views\" on the data.  Why? For a range of reasons -- from simple eyeballing to drawing out key insights.  \ngraph LR\n  data[Your Data] --> table[Table]\n  data --> grap[Graph]\n\n  classDef implemented fill:lightblue,stroke:#333,stroke-width:4px;\n  class abc implemented;  To achieve this we need to provide:   A tool-chain to create these views from the data.  A descriptive language for specifying views such as tables, graphs, map.   These requirements are addressed through the introduction of Data Package \"Views\" and associated tooling.  \ngraph LR\n\n  subgraph Data Package\n    resource[Resource]\n    view[View]\n    resource -.-> view\n  end\n\n  view --> toolchain\n  toolchain --> svg[\"Rendered Graph (SVG)\"]\n  toolchain --> table[Table]  We take a \"running code\" approach -- as in the rest of the Frictionless Data effort. We develop the spec by building a working implementation, and iterate around the spec as we develop. As development progresses the spec crystallizes and becomes ever less subject to change.  Our current implementation efforts focus on provide javascript tools and building a data package registry (DPR).", 
            "title": "Motivation"
        }, 
        {
            "location": "/publishers/views/#concepts-and-background", 
            "text": "To generate visualizations you usually want the following 3 types of information:   metadata: e.g. title of graph, credits etc  graph: description / specification of the graph itself  data: specification of data sources for the graph including location and key metadata like types   The data spec itself often consists of three distinct parts:   \"raw / graph data\": a spec / description of data exactly in the form needed by the visualization system. This is often a very well defined spec e.g. an array of series.  locate/describe: a spec of where to get data from e.g.  url  or  data  attribute plus some information on that data such as format and types.  transform: a spec of how transform external data prior to use e.g. pivoting or filtering it   From this description it should be clear that the latter two data specs -- locate/describe and transform -- are actually generic and independent of the specific graphing library. The only thing the graphing library really needs is a clear description of the \"raw\" format which it directly consumes. Thus, we can consider a natural grouping of specs as:   general-metadata - e.g. title of graph, credits etc [provided by e.g. Data Package / define yourself!]  data: sourcing and transform [provided by e.g. Data Resource]  sourcing: how to source data from external sources  transform: how to transform data e.g. pivot it, select one field, scale a field etc    graph description / specification [provided by e.g. Vega]  graph data (raw): data as directly consumed by graph spec (usually JSON based if we are talking about JS web-based visualization)     However, in many visualization tools -- including specs like Vega -- these items are combined together. This is understandable as these tools seek to off users a \"complete solution\". However,  decoupling these parts and having clearly defined interfaces would offer significant benefits :   Extensibility: it would be easier to extend and adapt the system. For example, adding new data import options could be done without changing the graph system.  Composability: we can combine different parts together in different ways. For example, data import and transformation could be used for generating data for tabular display as well as graphing.  Reusability: we want to reuse existing tools and specifications wherever possible. If we keep the specs relatively separate we can reuse the best spec for each job.  Reliability: when the system is decoupled it is easier to test and check.   In summary, a smaller pieces, loosely joined makes it easier to adapt and evolve the specs and the associated tooling.", 
            "title": "Concepts and Background"
        }, 
        {
            "location": "/publishers/views/#examples-of-using-views", 
            "text": "In this section, examples of using Data Package views are provided. Each example has a README section with small tutorial.", 
            "title": "Examples of using views"
        }, 
        {
            "location": "/publishers/views/#simple-graph-spec", 
            "text": "Simple graph spec is the easiest and quickest way to specify a view in a Data Package. Using simple graph spec publishers can generate graphs, e.g., line and bar charts.   Simple Graph Spec Tutorial", 
            "title": "Simple graph spec"
        }, 
        {
            "location": "/publishers/views/#vega-graphs", 
            "text": "Publishers can also describe graphs using Vega specifications:   Vega Graph Spec Tutorial - Yields of Barley  Vega Graph Spec Tutorial - US presidents  Vega Graph Spec Tutorial - US Airports", 
            "title": "Vega graphs"
        }, 
        {
            "location": "/publishers/views/#maps", 
            "text": "At the moment, we only support  .geojson  format:   GeoJSON Tutorial", 
            "title": "Maps"
        }, 
        {
            "location": "/publishers/views/#tables-and-transforms", 
            "text": "In the following examples, we demonstrate how transforms can be used in Data Package views. Transformed data will be displayed as table views.   Filter   Formula  Sample  Aggregate", 
            "title": "Tables and Transforms"
        }
    ]
}