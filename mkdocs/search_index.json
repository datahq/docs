{
    "docs": [
        {
            "location": "/", 
            "text": "DataHub Documentation\n\n\nWelcome to the DataHub documentation.\n\n\nDataHub is a platform for \npeople\n to \nstore, share and publish\n their data, \ncollect, inspect and process\n it with \npowerful tools\n, and \ndiscover and use\n data shared by others.\n\n\nOur focus is on data wranglers and data scientists: those who use automate their work with data using code and command line tools rather than editing it by hand (as, for example, many analysts do in Excel). Think people who use Python vs people who use Excel for data work.\n\n\nOur goal is to provide simplicity \nand\n power.\n\n\nCurrently this documentation has two sections:\n\n\n\n\nFor developers\n \n \n3 Python, JavaScript and data pipelines? Start here!\n\n\nFor publishers\n: Want to store your data on DataHub? Start here!", 
            "title": "Home"
        }, 
        {
            "location": "/#datahub-documentation", 
            "text": "Welcome to the DataHub documentation.  DataHub is a platform for  people  to  store, share and publish  their data,  collect, inspect and process  it with  powerful tools , and  discover and use  data shared by others.  Our focus is on data wranglers and data scientists: those who use automate their work with data using code and command line tools rather than editing it by hand (as, for example, many analysts do in Excel). Think people who use Python vs people who use Excel for data work.  Our goal is to provide simplicity  and  power.  Currently this documentation has two sections:   For developers     3 Python, JavaScript and data pipelines? Start here!  For publishers : Want to store your data on DataHub? Start here!", 
            "title": "DataHub Documentation"
        }, 
        {
            "location": "/developers/", 
            "text": "Developers\n\n\nThis section of the DataHub documentation is for developers. Here you can learn about the design of the platform and how to get DataHub running locally or on your own servers, and the process for contributing enhancements and bug fixes to the code.\n\n\n\n\nWe use following GitHub repositories for DataHub platform:\n\n\n\n\nDEPLOY\n - Automated deployment\n\n\nFRONTEND\n - Frontend application in node.js\n\n\nASSEMBLER\n - Data assembly line\n\n\nAUTH\n - A generic OAuth2 authentication service and user permission manager.\n\n\nSPECSTORE\n - API server for managing a Source Spec Registry\n\n\nBITSTORE\n - A microservice for storing blobs i.e. files.\n\n\n\n\nRESOLVER\n - A microservice for resolving datapackage URLs into more human readable ones\n\n\n\n\n\n\nDOCS\n - Documentations\n\n\n\n\n\n\n\ngraph TD\n\nsubgraph Repos\n  frontend[Frontend]\n  assembler[Assembler]\n  auth[Auth]\n  specstore[Specstore]\n  bitstore[Bitstore]\n  resolver[Resolver]\n  docs[Docs]\nend\n\nsubgraph Sites\n  dhio[datahub.io]\n  dhdocs[docs.datahub.io]\n  docs --> dhdocs\nend\n\ndeploy((DEPLOY))\ndeploy --> dhio\nfrontend --> deploy\nassembler --> deploy\nauth --> deploy\nspecstore --> deploy\nbitstore --> deploy\nresolver --> deploy\n\n\n\n\n\nInstall\n\n\nWe use several different services to run our platform, please follow the installation instructions here:\n\n\n\n\n\n\nInstall Assembler\n\n\n\n\n\n\nInstall Auth\n\n\n\n\n\n\nInstall Specstore\n\n\n\n\n\n\nInstall Bitstore\n\n\n\n\n\n\nInstall DataHub-CLI\n\n\n\n\n\n\nInstall Resolver\n\n\n\n\n\n\nDeploy\n\n\nFor deployment of the application in a production environment, please see \nthe deploy page\n.\n\n\nDataHub CLI\n\n\nThe DataHub CLI is a Node JS lib and command line interface to interact with an DataHub instance.\n\n\nCLI code", 
            "title": "Getting started"
        }, 
        {
            "location": "/developers/#developers", 
            "text": "This section of the DataHub documentation is for developers. Here you can learn about the design of the platform and how to get DataHub running locally or on your own servers, and the process for contributing enhancements and bug fixes to the code.   We use following GitHub repositories for DataHub platform:   DEPLOY  - Automated deployment  FRONTEND  - Frontend application in node.js  ASSEMBLER  - Data assembly line  AUTH  - A generic OAuth2 authentication service and user permission manager.  SPECSTORE  - API server for managing a Source Spec Registry  BITSTORE  - A microservice for storing blobs i.e. files.   RESOLVER  - A microservice for resolving datapackage URLs into more human readable ones    DOCS  - Documentations    \ngraph TD\n\nsubgraph Repos\n  frontend[Frontend]\n  assembler[Assembler]\n  auth[Auth]\n  specstore[Specstore]\n  bitstore[Bitstore]\n  resolver[Resolver]\n  docs[Docs]\nend\n\nsubgraph Sites\n  dhio[datahub.io]\n  dhdocs[docs.datahub.io]\n  docs --> dhdocs\nend\n\ndeploy((DEPLOY))\ndeploy --> dhio\nfrontend --> deploy\nassembler --> deploy\nauth --> deploy\nspecstore --> deploy\nbitstore --> deploy\nresolver --> deploy", 
            "title": "Developers"
        }, 
        {
            "location": "/developers/#install", 
            "text": "We use several different services to run our platform, please follow the installation instructions here:    Install Assembler    Install Auth    Install Specstore    Install Bitstore    Install DataHub-CLI    Install Resolver", 
            "title": "Install"
        }, 
        {
            "location": "/developers/#deploy", 
            "text": "For deployment of the application in a production environment, please see  the deploy page .", 
            "title": "Deploy"
        }, 
        {
            "location": "/developers/#datahub-cli", 
            "text": "The DataHub CLI is a Node JS lib and command line interface to interact with an DataHub instance.  CLI code", 
            "title": "DataHub CLI"
        }, 
        {
            "location": "/developers/platform/", 
            "text": "Platform\n\n\nThe DataHub platform follows a service oriented architecture. It is built from a set of loosely coupled components, each performing distinct functions related to the platform as a whole.\n\n\n\n\n\n\nPlatform\n\n\nArchitecture\n\n\nInformation Architecture\n\n\nComponents\n\n\nFrontend Web Application\n\n\nViews and Renderer\n\n\n\n\n\n\nAssembler\n\n\nRaw Storage\n\n\nPackage Storage\n\n\nBitStore\n\n\nMetaStore\n\n\nService architecture\n\n\n\n\n\n\nCommand Line Interface\n\n\n\n\n\n\nDomain model\n\n\nProfile\n\n\nPublisher\n\n\nUser\n\n\nPackage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchitecture\n\n\nFig 1: Data Flow through the system\n\n\n\n\ngraph TD\n\ncli((CLI fa:fa-user))\nauth[Auth Service]\ncli --login--> auth\n\ncli --store--> raw[Raw Store API\n+ Storage]  \n\ncli --package-info--> pipeline-store\nraw --data resource--> pipeline-runner\n\npipeline-store -.generate.-> pipeline-runner\n\npipeline-runner --> package[Package Storage]\npackage --api--> frontend[Frontend]\nfrontend --> user[User fa:fa-user]\n\npackage -.publish.->metastore[MetaStore]\npipeline-store -.publish.-> metastore[MetaStore]\nmetastore[MetaStore] --api--> frontend\n\n\n\n\nFig 2: Components Perspective - from the Frontend\n\n\n\n\ngraph TD\n\nsubgraph Web Frontend\n  frontend[Frontend Webapp]\n  browse[Browse \n&\n Search]\n  login[Login \n&\n Signup]\n  view[Views Renderer]\n  frontend --> browse\n  frontend --> login\nend\n\nsubgraph Users and Permissions\n  user[User]\n  permissions[Permissions]\n  authapi[Auth API]\n  authzapi[Authorization API]\n  login --> authapi\n  authapi --> user\n  authzapi --> permissions\nend\n\nsubgraph PkgStore\n  bitstore[\"PkgStore (S3)\"]\n  bitstoreapi[PkgStore API\nput,get]\n  bitstoreapi --> bitstore\n  browse --> bitstoreapi\nend\n\nsubgraph MetaStore\n  metastore[\"MetaStore (ElasticSearch)\"]\n  metaapi[MetaStore API\nread,search,import]\n  metaapi --> metastore\n  browse --> metaapi\nend\n\nsubgraph CLI\n  cli[CLI]\nend\n\n\n\n\nInformation Architecture\n\n\ndatahub.io            # frontend\napi.datahub.io        #\u00a0API - see API page for structure\nrawstore.datahub.io   #\u00a0rawstore - raw bitstore\npkgstore.datahub.io   #\u00a0pkgstore - package bitstore\n\n\n\nComponents\n\n\nFrontend Web Application\n\n\nCore part of platform - Login \n Sign-Up and Browse \n Search Datasets\n\n\nhttps://github.com/datahq/frontend\n\n\nViews and Renderer\n\n\nJS Library responsible for visualization and views.\n\n\nSee \nviews\n section for more about Views.\n\n\nAssembler\n\n\nTODO\n\n\nRaw Storage\n\n\nWe first save all raw files before sending to pipeline-runner.\n\nPipeline-runner\n is a service that runs the data package pipelines. It is used to normalise and modify the data before it is displayed publicly\n\n\n\n\nWe use AWS S3 instance for storing data\n\n\n\n\nPackage Storage\n\n\nWe store files after passing pipeline-runner\n\n\n\n\nWe use AWS S3 instance for storing data\n\n\n\n\nBitStore\n\n\nWe are preserving the data byte by byte.\n\n\n\n\nWe use AWS S3 instance for storing data\n\n\n\n\nMetaStore\n\n\nThe MetaStore provides an integrated, searchable view over key metadata for end user services and users. Initially this metadata will just be metadata on datasets in the Package Store. In future it may expand to provide a unified to include other related metadata such as pipelines. It also includes summary metadata (or the ability to compute summary data) e.g. the total size of all your packages\n\n\nService architecture\n\n\ngraph TD\n\nsubgraph MetaStore\nmetaapi[MetaStore API]\nmetadb[MetaStore DB fa:fa-database]\nend\n\nmetadb --\n metaapi\n\nassembler[Assembler] --should this by via api or direct to DB??--\n metadb\n\nmetaapi --\n frontend[Frontend fa:fa-user]\nmetaapi --\n cli[CLI fa:fa-user]\n\nfrontend -.no dp stuff only access.-\n metaapi\n\n\n\nCommand Line Interface\n\n\nThe command line interface.\n\n\nhttps://github.com/datahq/datahub-cli\n\n\nDomain model\n\n\nThere are two main concepts to understand in DataHub domain model - \nProfile\n and \nPackage\n\n\n\ngraph TD\n\npkg[Data Package]\nresource[Resource]\nfile[File]\nversion[Version]\nuser[User]\npublisher[Publisher]\n\nsubgraph Package\n  pkg --0..*--> resource\n  resource --1..*--> file\n  pkg --> version\nend\n\nsubgraph Profile\n  publisher --1..*--> user\n  publisher --0..*--> pkg\nend\n\n\n\n\nProfile\n\n\nSet of an authenticated and authorized entities like publishers and users. They are responsible for publishing, deleting or maintaining data on platform.\n\n\nImportant:\n Users do not have Data Packages, Publishers do. Users are \nmembers\n of Publishers.\n\n\nPublisher\n\n\nPublisher is an organization which \nowns\n Data Packages. Publisher may have zero or more Data Packages. Publisher may also have one or more user.\n\n\nUser\n\n\nUser is an authenticated entity, that is member of Publisher organization, that can read, edit, create or delete data packages depending on their permissions.\n\n\nPackage\n\n\nA Data Package is a simple way of \u201cpackaging\u201d up and describing data so that it can be easily shared and used. You can imagine as collection of data and and it\ns meta-data (\ndatapackage.json\n), usually covering some concrete topic Eg: \nGold Prices\n or \nPopulation Growth Rate In My country\n etc.\n\n\nEach Data Package may have zero or more resources and one or more versions.\n\n\nResources\n - think like \ntables\n - Each can map to one or more physical files (usually just one). Think of a data table split into multiple CSV files on disk.\n\n\nVersion of a Data Package\n - similar to git commits and tags. People can mean different things by a \nVersion\n:\n\n\n\n\nTag - Same as label or version - a nice human usable label e.g. \nv0.3\n, \nmaster\n, \n2013\n\n\nCommit/Hash - Corresponds to the hash of datapackage.json, with that datapackage.json including all hashes of all data files\n\n\n\n\nWe interpret Version as \nTag\n concept. \nCommit/Hash\n is not supported", 
            "title": "Platform"
        }, 
        {
            "location": "/developers/platform/#platform", 
            "text": "The DataHub platform follows a service oriented architecture. It is built from a set of loosely coupled components, each performing distinct functions related to the platform as a whole.    Platform  Architecture  Information Architecture  Components  Frontend Web Application  Views and Renderer    Assembler  Raw Storage  Package Storage  BitStore  MetaStore  Service architecture    Command Line Interface    Domain model  Profile  Publisher  User  Package", 
            "title": "Platform"
        }, 
        {
            "location": "/developers/platform/#architecture", 
            "text": "Fig 1: Data Flow through the system  \ngraph TD\n\ncli((CLI fa:fa-user))\nauth[Auth Service]\ncli --login--> auth\n\ncli --store--> raw[Raw Store API + Storage]  \n\ncli --package-info--> pipeline-store\nraw --data resource--> pipeline-runner\n\npipeline-store -.generate.-> pipeline-runner\n\npipeline-runner --> package[Package Storage]\npackage --api--> frontend[Frontend]\nfrontend --> user[User fa:fa-user]\n\npackage -.publish.->metastore[MetaStore]\npipeline-store -.publish.-> metastore[MetaStore]\nmetastore[MetaStore] --api--> frontend  Fig 2: Components Perspective - from the Frontend  \ngraph TD\n\nsubgraph Web Frontend\n  frontend[Frontend Webapp]\n  browse[Browse  &  Search]\n  login[Login  &  Signup]\n  view[Views Renderer]\n  frontend --> browse\n  frontend --> login\nend\n\nsubgraph Users and Permissions\n  user[User]\n  permissions[Permissions]\n  authapi[Auth API]\n  authzapi[Authorization API]\n  login --> authapi\n  authapi --> user\n  authzapi --> permissions\nend\n\nsubgraph PkgStore\n  bitstore[\"PkgStore (S3)\"]\n  bitstoreapi[PkgStore API put,get]\n  bitstoreapi --> bitstore\n  browse --> bitstoreapi\nend\n\nsubgraph MetaStore\n  metastore[\"MetaStore (ElasticSearch)\"]\n  metaapi[MetaStore API read,search,import]\n  metaapi --> metastore\n  browse --> metaapi\nend\n\nsubgraph CLI\n  cli[CLI]\nend", 
            "title": "Architecture"
        }, 
        {
            "location": "/developers/platform/#information-architecture", 
            "text": "datahub.io            # frontend\napi.datahub.io        #\u00a0API - see API page for structure\nrawstore.datahub.io   #\u00a0rawstore - raw bitstore\npkgstore.datahub.io   #\u00a0pkgstore - package bitstore", 
            "title": "Information Architecture"
        }, 
        {
            "location": "/developers/platform/#components", 
            "text": "", 
            "title": "Components"
        }, 
        {
            "location": "/developers/platform/#frontend-web-application", 
            "text": "Core part of platform - Login   Sign-Up and Browse   Search Datasets  https://github.com/datahq/frontend", 
            "title": "Frontend Web Application"
        }, 
        {
            "location": "/developers/platform/#views-and-renderer", 
            "text": "JS Library responsible for visualization and views.  See  views  section for more about Views.", 
            "title": "Views and Renderer"
        }, 
        {
            "location": "/developers/platform/#assembler", 
            "text": "TODO", 
            "title": "Assembler"
        }, 
        {
            "location": "/developers/platform/#raw-storage", 
            "text": "We first save all raw files before sending to pipeline-runner. Pipeline-runner  is a service that runs the data package pipelines. It is used to normalise and modify the data before it is displayed publicly   We use AWS S3 instance for storing data", 
            "title": "Raw Storage"
        }, 
        {
            "location": "/developers/platform/#package-storage", 
            "text": "We store files after passing pipeline-runner   We use AWS S3 instance for storing data", 
            "title": "Package Storage"
        }, 
        {
            "location": "/developers/platform/#bitstore", 
            "text": "We are preserving the data byte by byte.   We use AWS S3 instance for storing data", 
            "title": "BitStore"
        }, 
        {
            "location": "/developers/platform/#metastore", 
            "text": "The MetaStore provides an integrated, searchable view over key metadata for end user services and users. Initially this metadata will just be metadata on datasets in the Package Store. In future it may expand to provide a unified to include other related metadata such as pipelines. It also includes summary metadata (or the ability to compute summary data) e.g. the total size of all your packages", 
            "title": "MetaStore"
        }, 
        {
            "location": "/developers/platform/#service-architecture", 
            "text": "graph TD\n\nsubgraph MetaStore\nmetaapi[MetaStore API]\nmetadb[MetaStore DB fa:fa-database]\nend\n\nmetadb --  metaapi\n\nassembler[Assembler] --should this by via api or direct to DB??--  metadb\n\nmetaapi --  frontend[Frontend fa:fa-user]\nmetaapi --  cli[CLI fa:fa-user]\n\nfrontend -.no dp stuff only access.-  metaapi", 
            "title": "Service architecture"
        }, 
        {
            "location": "/developers/platform/#command-line-interface", 
            "text": "The command line interface.  https://github.com/datahq/datahub-cli", 
            "title": "Command Line Interface"
        }, 
        {
            "location": "/developers/platform/#domain-model", 
            "text": "There are two main concepts to understand in DataHub domain model -  Profile  and  Package  \ngraph TD\n\npkg[Data Package]\nresource[Resource]\nfile[File]\nversion[Version]\nuser[User]\npublisher[Publisher]\n\nsubgraph Package\n  pkg --0..*--> resource\n  resource --1..*--> file\n  pkg --> version\nend\n\nsubgraph Profile\n  publisher --1..*--> user\n  publisher --0..*--> pkg\nend", 
            "title": "Domain model"
        }, 
        {
            "location": "/developers/platform/#profile", 
            "text": "Set of an authenticated and authorized entities like publishers and users. They are responsible for publishing, deleting or maintaining data on platform.  Important:  Users do not have Data Packages, Publishers do. Users are  members  of Publishers.", 
            "title": "Profile"
        }, 
        {
            "location": "/developers/platform/#publisher", 
            "text": "Publisher is an organization which  owns  Data Packages. Publisher may have zero or more Data Packages. Publisher may also have one or more user.", 
            "title": "Publisher"
        }, 
        {
            "location": "/developers/platform/#user", 
            "text": "User is an authenticated entity, that is member of Publisher organization, that can read, edit, create or delete data packages depending on their permissions.", 
            "title": "User"
        }, 
        {
            "location": "/developers/platform/#package", 
            "text": "A Data Package is a simple way of \u201cpackaging\u201d up and describing data so that it can be easily shared and used. You can imagine as collection of data and and it s meta-data ( datapackage.json ), usually covering some concrete topic Eg:  Gold Prices  or  Population Growth Rate In My country  etc.  Each Data Package may have zero or more resources and one or more versions.  Resources  - think like  tables  - Each can map to one or more physical files (usually just one). Think of a data table split into multiple CSV files on disk.  Version of a Data Package  - similar to git commits and tags. People can mean different things by a  Version :   Tag - Same as label or version - a nice human usable label e.g.  v0.3 ,  master ,  2013  Commit/Hash - Corresponds to the hash of datapackage.json, with that datapackage.json including all hashes of all data files   We interpret Version as  Tag  concept.  Commit/Hash  is not supported", 
            "title": "Package"
        }, 
        {
            "location": "/developers/deploy/", 
            "text": "DevOps - Production Deployment\n\n\nWe use various cloud services for the platform, for example AWS S3 for storing data and metadata, and the application runs on Docker Cloud.\n\n\nWe have fully automated the deployment of the platform including the setup of all necessary services so that it is one command to deploy. Code and instructions here:\n\n\nhttps://github.com/datahq/deploy\n\n\nBelow we provide a conceptual outline.\n\n\nOutline - Conceptually\n\n\n\ngraph TD\n\n  user[fa:fa-user User] --> frontend[Frontend]\n  frontend --> apiproxy[API Proxy]\n  frontend --> bits[BitStore - S3]\n\n\n\n\nNew Structure\n\n\nThis diagram shows the current deployment architecture.\n\n\n\ngraph LR\n\ncloudflare --> haproxy\n\nhaproxy --> frontend\n\nsubgraph auth\n  postgres\n  authapp\nend\n\nsubgraph rawstore\n  rawobjstore\n  rawapp\nend\n\nsubgraph pkgstore\n  pkgobjstore\n  pkgapp\nend\n\nsubgraph metastore\n  elasticsearch\n  metastore\nend\n\nhaproxy --/auth--> authapp\nhaproxy --/rawstore--> rawapp\n\nhaproxy --> pkgapp\nhaproxy --/metastore--> metastore\n\n\n\n\nOld Structures\n\n\nHeroku\n\n\n\ngraph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> heroku\n  cloudflare --> bits\n  heroku[Heroku - Flask] --> rds[RDS Database]\n  heroku --> bits\n\n\n\n\nAWS Lambda - Flask via Zappa\n\n\nWe are no longer using AWS and Heroku in this way. However, we have kept this for historical purposes and in case we return to any of them.\n\n\n\ngraph TD\n\n  user[fa:fa-user User] --> cloudfront[Cloudfront]\n  cloudfront --> apigateway[API Gateway]\n  apigateway --> lambda[AWS Lambda - Flask via Zappa]\n  cloudfront --> s3assets[S3 Assets]\n  lambda --> rds[RDS Database]\n  lambda --> bits[BitStore]\n  cloudfront --> bits", 
            "title": "Deploy"
        }, 
        {
            "location": "/developers/deploy/#devops-production-deployment", 
            "text": "We use various cloud services for the platform, for example AWS S3 for storing data and metadata, and the application runs on Docker Cloud.  We have fully automated the deployment of the platform including the setup of all necessary services so that it is one command to deploy. Code and instructions here:  https://github.com/datahq/deploy  Below we provide a conceptual outline.", 
            "title": "DevOps - Production Deployment"
        }, 
        {
            "location": "/developers/deploy/#outline-conceptually", 
            "text": "graph TD\n\n  user[fa:fa-user User] --> frontend[Frontend]\n  frontend --> apiproxy[API Proxy]\n  frontend --> bits[BitStore - S3]", 
            "title": "Outline - Conceptually"
        }, 
        {
            "location": "/developers/deploy/#new-structure", 
            "text": "This diagram shows the current deployment architecture.  \ngraph LR\n\ncloudflare --> haproxy\n\nhaproxy --> frontend\n\nsubgraph auth\n  postgres\n  authapp\nend\n\nsubgraph rawstore\n  rawobjstore\n  rawapp\nend\n\nsubgraph pkgstore\n  pkgobjstore\n  pkgapp\nend\n\nsubgraph metastore\n  elasticsearch\n  metastore\nend\n\nhaproxy --/auth--> authapp\nhaproxy --/rawstore--> rawapp\n\nhaproxy --> pkgapp\nhaproxy --/metastore--> metastore", 
            "title": "New Structure"
        }, 
        {
            "location": "/developers/deploy/#old-structures", 
            "text": "", 
            "title": "Old Structures"
        }, 
        {
            "location": "/developers/deploy/#heroku", 
            "text": "graph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> heroku\n  cloudflare --> bits\n  heroku[Heroku - Flask] --> rds[RDS Database]\n  heroku --> bits", 
            "title": "Heroku"
        }, 
        {
            "location": "/developers/deploy/#aws-lambda-flask-via-zappa", 
            "text": "We are no longer using AWS and Heroku in this way. However, we have kept this for historical purposes and in case we return to any of them.  \ngraph TD\n\n  user[fa:fa-user User] --> cloudfront[Cloudfront]\n  cloudfront --> apigateway[API Gateway]\n  apigateway --> lambda[AWS Lambda - Flask via Zappa]\n  cloudfront --> s3assets[S3 Assets]\n  lambda --> rds[RDS Database]\n  lambda --> bits[BitStore]\n  cloudfront --> bits", 
            "title": "AWS Lambda - Flask via Zappa"
        }, 
        {
            "location": "/developers/api/", 
            "text": "DataHub API\n\n\nThe DataHub API provides a range of endpoints to interact with the platform. All endpoints live under the URL \nhttps://api.datahub.io\n where our API is divided into the following sections: \nauth, rawstore, sources, metastore, resolver\n. \n\n\nAuth\n\n\nA generic OAuth2 authentication service and user permission manager. \n\n\nhttps://github.com/datahq/auth#api\n\n\nRawstore\n\n\nDataHub microservice for storing blobs i.e. files. It is a lightweight auth wrapper for n S3-compative object store that integrates with the rest of the DataHub stack and especially the auth service.\n\n\nhttps://github.com/datahq/bitstore#api\n\n\nSources\n\n\nAn API server for managing a Source Spec Registry.\n\n\nhttps://github.com/datahq/specstore#api\n\n\nMetastore\n\n\nA search services for DataHub. \n\n\nhttps://github.com/datahq/metastore#api\n\n\nResolver\n\n\nDataHub microservice for resolving datapackage URLs into more human readable ones.\n\n\nhttps://github.com/datahq/resolver#api", 
            "title": "API"
        }, 
        {
            "location": "/developers/api/#datahub-api", 
            "text": "The DataHub API provides a range of endpoints to interact with the platform. All endpoints live under the URL  https://api.datahub.io  where our API is divided into the following sections:  auth, rawstore, sources, metastore, resolver .", 
            "title": "DataHub API"
        }, 
        {
            "location": "/developers/api/#auth", 
            "text": "A generic OAuth2 authentication service and user permission manager.   https://github.com/datahq/auth#api", 
            "title": "Auth"
        }, 
        {
            "location": "/developers/api/#rawstore", 
            "text": "DataHub microservice for storing blobs i.e. files. It is a lightweight auth wrapper for n S3-compative object store that integrates with the rest of the DataHub stack and especially the auth service.  https://github.com/datahq/bitstore#api", 
            "title": "Rawstore"
        }, 
        {
            "location": "/developers/api/#sources", 
            "text": "An API server for managing a Source Spec Registry.  https://github.com/datahq/specstore#api", 
            "title": "Sources"
        }, 
        {
            "location": "/developers/api/#metastore", 
            "text": "A search services for DataHub.   https://github.com/datahq/metastore#api", 
            "title": "Metastore"
        }, 
        {
            "location": "/developers/api/#resolver", 
            "text": "DataHub microservice for resolving datapackage URLs into more human readable ones.  https://github.com/datahq/resolver#api", 
            "title": "Resolver"
        }, 
        {
            "location": "/developers/publish/", 
            "text": "Publish\n\n\nExplanation of DataHub publishing flow from client and back-end perspectives.\n\n\n\n\ngraph TD\n\n\n  cli((CLI fa:fa-user))\n  auth[Auth Service]\n  cli --login--> auth\n\n\n    cli --store--> raw[Raw Store API\n+ Storage]  \n\n    cli --package-info--> pipeline-store\n  raw --data resource--> pipeline-runner\n\n  pipeline-store -.generate.-> pipeline-runner\n\n  pipeline-runner --> package[Package Storage]\n    package --api--> frontend[Frontend]\n  frontend --> user[User fa:fa-user]\n\n\n\n  package -.publish.->metastore[MetaStore]\n  pipeline-store -.publish.-> metastore[MetaStore]\n  metastore[MetaStore] --api--> frontend\n\n\n\n\n\nClient Perspective\n\n\nPublishing flow takes the following steps and processes to communicate with DataHub API:\n\n\n\nsequenceDiagram\nUpload Agent CLI->>Upload Agent CLI: Check Data Package valid\nUpload Agent CLI-->>Auth(SSO): login\nAuth(SSO)-->>Upload Agent CLI: JWT token\nUpload Agent CLI->>RawStore API: upload using signed url\nRawStore API->>Auth(SSO): Check key / token\nAuth(SSO)->>RawStore API: OK / Not OK\nRawStore API->>Upload Agent CLI: success message\nUpload Agent CLI->>pipeline store: package info\npipeline store->>Upload Agent CLI: OK / Not OK\npipeline store->>pipeline runner: generate\nRawStore API->>pipeline runner: data resource\npipeline runner->>Package Storage: generated\nPackage Storage->>Metadata Storage API: publish\npipeline store->>Metadata Storage API: publish\nMetadata Storage API->>Upload Agent CLI: OK / Not OK\n\n\n\n\n\n\n\n\nUpload API - see \nPOST /source/upload\n in \nsource\n section of \nAPI\n\n\nAuthentication API - see \nGET /auth/check\n in \nauth\n section of \nAPI\n.\n\n\nAuthorization API - see \nGET /auth/authorize\n in \nauth\n section of \nAPI\n.\n\n\n\n\nSee example \ncode snippet in DataHub CLI", 
            "title": "Publish"
        }, 
        {
            "location": "/developers/publish/#publish", 
            "text": "Explanation of DataHub publishing flow from client and back-end perspectives.  \n\ngraph TD\n\n\n  cli((CLI fa:fa-user))\n  auth[Auth Service]\n  cli --login--> auth\n\n\n    cli --store--> raw[Raw Store API + Storage]  \n\n    cli --package-info--> pipeline-store\n  raw --data resource--> pipeline-runner\n\n  pipeline-store -.generate.-> pipeline-runner\n\n  pipeline-runner --> package[Package Storage]\n    package --api--> frontend[Frontend]\n  frontend --> user[User fa:fa-user]\n\n\n\n  package -.publish.->metastore[MetaStore]\n  pipeline-store -.publish.-> metastore[MetaStore]\n  metastore[MetaStore] --api--> frontend", 
            "title": "Publish"
        }, 
        {
            "location": "/developers/publish/#client-perspective", 
            "text": "Publishing flow takes the following steps and processes to communicate with DataHub API:  \nsequenceDiagram\nUpload Agent CLI->>Upload Agent CLI: Check Data Package valid\nUpload Agent CLI-->>Auth(SSO): login\nAuth(SSO)-->>Upload Agent CLI: JWT token\nUpload Agent CLI->>RawStore API: upload using signed url\nRawStore API->>Auth(SSO): Check key / token\nAuth(SSO)->>RawStore API: OK / Not OK\nRawStore API->>Upload Agent CLI: success message\nUpload Agent CLI->>pipeline store: package info\npipeline store->>Upload Agent CLI: OK / Not OK\npipeline store->>pipeline runner: generate\nRawStore API->>pipeline runner: data resource\npipeline runner->>Package Storage: generated\nPackage Storage->>Metadata Storage API: publish\npipeline store->>Metadata Storage API: publish\nMetadata Storage API->>Upload Agent CLI: OK / Not OK    Upload API - see  POST /source/upload  in  source  section of  API  Authentication API - see  GET /auth/check  in  auth  section of  API .  Authorization API - see  GET /auth/authorize  in  auth  section of  API .   See example  code snippet in DataHub CLI", 
            "title": "Client Perspective"
        }, 
        {
            "location": "/developers/views/", 
            "text": "Views\n\n\nPproducers and consumers of data want to have data presented in tables and graphs \n \nviews\n on the data. They want this for a range of reasons, from simple eyeballing to drawing out key insights.\n\n\n\ngraph LR\n  data[Your Data] --> table[Table]\n  data --> grap[Graph]\n  data --> geo[Map]\n\n\n\n\nTo achieve this we need to provide:\n\n\n\n\nA tool-chain to create these views from the data.\n\n\nA descriptive language for specifying views such as tables, graphs, map.\n\n\n\n\nThese requirements are addressed through the introduction of Data Package \nViews\n and associated tooling.\n\n\n\ngraph LR\n\n  subgraph Data Package\n    resource[Resource]\n    view[View]\n    resource -.-> view\n  end\n\n  view --> toolchain\n  toolchain --> svg[\"Rendered Graph (SVG)\"]\n  toolchain --> table[Table]\n  toolchain --> map[Map]\n\n\n\n\nThis section describes the details of how we support \nData Package Views\n in the DataHub.\n\n\nIt consists of two parts, the first describes the general tool chain we have. The second part describes how we use that to generate graphs in the showcase page.\n\n\nQuick Links\n\n\n\n\nData Package Views introduction and spec\n\n\ndatapackage-render-js\n - this is the library that implements conversion from the data package views spec to vega/plotly and then svg or png\n\n\n\n\nTable of Contents\n\n\n\n\n\n\nViews\n\n\nThe Tool Chain\n\n\nGraphs\n\n\nGeo support\n\n\nTable support\n\n\nSummary\n\n\n\n\n\n\nViews in the Showcase\n\n\nAppendix\n\n\n\n\n\n\nThe Tool Chain\n\n\nFigure 1: From Data Package View Spec to Rendered output\n\n\n\ngraph TD\n  pre[Pre-cursor views e.g. Recline] --bespoke conversions--> dpv[Data Package Views]\n  dpv --\"normalize (correct any variations and ensure key fields are present)\"--> dpvn[\"Data Package Views\n(Normalized)\"]\n  dpvn --\"compile in resource \n&\n data ([future] do transforms)\"--> dpvnd[\"Self-Contained View\n(All data and schema inline)\"]\n  dpvnd --compile to native spec--> plotly[Plotly Spec]\n  dpvnd --compile to native spec--> vega[Vega Spec]\n  plotly --render--> html[svg/png/etc]\n  vega --render--> html\n\n\n\n\nIMPORTANT\n: an important \nconvention\n we adopt for the \ncompiling-in\n of data is that resource data should be inlined into an \n_values\n attribute. If the data is tabular this attribute should be an array of \narrays\n (not objects).\n\n\nGraphs\n\n\nFigure 2: Conversion paths\n\n\n\ngraph LR\n  inplotly[\"Plotly DP Spec\"] --> plotly[Plotly JSON]\n  simple[Simple Spec] --> plotly\n  simple .-> vega[Vega JSON]\n  invega[Vega DP Spec] --> vega\n  vegalite[Vega Lite DP Spec] --> vega\n  recline[Recline] .-> simple\n  plotly --plotly lib--> svg[SVG / PNG]\n  vega --vega lib--> svg\n\n  classDef implemented fill:lightblue,stroke:#333,stroke-width:4px;\n  class recline,simple,plotly,svg,inplotly,invega,vega implemented;\n\n\n\n\nNotes:\n\n\n\n\nImplemented paths are shown in lightblue - code for this is in \ndatapackage-render-js\n\n\nLeft-most column (Recline): pre-specs that we can convert to our standard specs\n\n\nSecond-from-left column: DP View spec types.\n\n\nSecond-from-right column: the graphing libraries we can use (which all output to SVG)\n\n\n\n\nGeo support\n\n\nNote\n: support for customizing map is limited to JS atm - there is no real map \nspec\n in JSON yet beyond the trivial version.\n\n\nNote\n: vega has some geo support but geo here means full geojson style mapping.\n\n\n\ngraph LR\n\n  geo[Geo Resource] --> map\n  map[Map Spec] --> leaflet[Leaflet]\n\n  classDef implemented fill:lightblue,stroke:#333,stroke-width:4px;\n  class geo,map,leaflet implemented;\n\n\n\n\nTable support\n\n\n\ngraph LR\n  resource[Tabular Resource] --> table\n  table[Table Spec] --> handsontable[HandsOnTable]\n  table --> html[Simple HTML Table]\n\n  classDef implemented fill:lightblue,stroke:#333,stroke-width:4px;\n  class resource,table,handsontable implemented;\n\n\n\n\nSummary\n\n\nFigure 3: From Data Package View to Rendered output flow (richer version of diagram 1)\n\n\n\n\nViews in the Showcase\n\n\nTo render Data Packages in browsers we use DataHub views written in JavaScript. The module implemented in ReactJS framework and it can render tables, maps and various graphs using third-party libraries.\n\n\nImplementing code can be found in:\n\n\n\n\ndpr-js repo\n - which in turn depends on \ndatapackage-render-js\n\n\n\n\n\n  graph TD\n\n  url[\"metadata URL passed from back-end\"]\n  dp-js[datapackage-js]\n  dprender[datapackage-render-js]\n  table[\"table view\"]\n  chart[\"graph view\"]\n  hot[HandsOnTable]\n  map[LeafletMap]\n  vega[Vega]\n  plotly[Plotly]\n  browser[Browser]\n\n  url --> dp-js\n  dp-js --fetched dp--> dprender\n  dprender --spec--> table\n  table --1..n--> hot\n  dprender --geojson--> map\n  dprender --spec--> chart\n  chart --0..n--> vega\n  chart --0..n--> plotly\n  hot --table--> browser\n  map --map--> browser\n  vega --graph--> browser\n  plotly --graph--> browser\n\n\n\n\nNotice that DataHub views render a table view per tabular resource. If GeoJSON resource is given, it renders a map. Graph views should be specified in \nviews\n property of a Data Package.\n\n\nAppendix\n\n\nThere is a separate page with \nadditional research material regarding views specification and tooling\n.", 
            "title": "Views"
        }, 
        {
            "location": "/developers/views/#views", 
            "text": "Pproducers and consumers of data want to have data presented in tables and graphs    views  on the data. They want this for a range of reasons, from simple eyeballing to drawing out key insights.  \ngraph LR\n  data[Your Data] --> table[Table]\n  data --> grap[Graph]\n  data --> geo[Map]  To achieve this we need to provide:   A tool-chain to create these views from the data.  A descriptive language for specifying views such as tables, graphs, map.   These requirements are addressed through the introduction of Data Package  Views  and associated tooling.  \ngraph LR\n\n  subgraph Data Package\n    resource[Resource]\n    view[View]\n    resource -.-> view\n  end\n\n  view --> toolchain\n  toolchain --> svg[\"Rendered Graph (SVG)\"]\n  toolchain --> table[Table]\n  toolchain --> map[Map]  This section describes the details of how we support  Data Package Views  in the DataHub.  It consists of two parts, the first describes the general tool chain we have. The second part describes how we use that to generate graphs in the showcase page.  Quick Links   Data Package Views introduction and spec  datapackage-render-js  - this is the library that implements conversion from the data package views spec to vega/plotly and then svg or png   Table of Contents    Views  The Tool Chain  Graphs  Geo support  Table support  Summary    Views in the Showcase  Appendix", 
            "title": "Views"
        }, 
        {
            "location": "/developers/views/#the-tool-chain", 
            "text": "Figure 1: From Data Package View Spec to Rendered output  \ngraph TD\n  pre[Pre-cursor views e.g. Recline] --bespoke conversions--> dpv[Data Package Views]\n  dpv --\"normalize (correct any variations and ensure key fields are present)\"--> dpvn[\"Data Package Views (Normalized)\"]\n  dpvn --\"compile in resource  &  data ([future] do transforms)\"--> dpvnd[\"Self-Contained View (All data and schema inline)\"]\n  dpvnd --compile to native spec--> plotly[Plotly Spec]\n  dpvnd --compile to native spec--> vega[Vega Spec]\n  plotly --render--> html[svg/png/etc]\n  vega --render--> html  IMPORTANT : an important  convention  we adopt for the  compiling-in  of data is that resource data should be inlined into an  _values  attribute. If the data is tabular this attribute should be an array of  arrays  (not objects).", 
            "title": "The Tool Chain"
        }, 
        {
            "location": "/developers/views/#graphs", 
            "text": "Figure 2: Conversion paths  \ngraph LR\n  inplotly[\"Plotly DP Spec\"] --> plotly[Plotly JSON]\n  simple[Simple Spec] --> plotly\n  simple .-> vega[Vega JSON]\n  invega[Vega DP Spec] --> vega\n  vegalite[Vega Lite DP Spec] --> vega\n  recline[Recline] .-> simple\n  plotly --plotly lib--> svg[SVG / PNG]\n  vega --vega lib--> svg\n\n  classDef implemented fill:lightblue,stroke:#333,stroke-width:4px;\n  class recline,simple,plotly,svg,inplotly,invega,vega implemented;  Notes:   Implemented paths are shown in lightblue - code for this is in  datapackage-render-js  Left-most column (Recline): pre-specs that we can convert to our standard specs  Second-from-left column: DP View spec types.  Second-from-right column: the graphing libraries we can use (which all output to SVG)", 
            "title": "Graphs"
        }, 
        {
            "location": "/developers/views/#geo-support", 
            "text": "Note : support for customizing map is limited to JS atm - there is no real map  spec  in JSON yet beyond the trivial version.  Note : vega has some geo support but geo here means full geojson style mapping.  \ngraph LR\n\n  geo[Geo Resource] --> map\n  map[Map Spec] --> leaflet[Leaflet]\n\n  classDef implemented fill:lightblue,stroke:#333,stroke-width:4px;\n  class geo,map,leaflet implemented;", 
            "title": "Geo support"
        }, 
        {
            "location": "/developers/views/#table-support", 
            "text": "graph LR\n  resource[Tabular Resource] --> table\n  table[Table Spec] --> handsontable[HandsOnTable]\n  table --> html[Simple HTML Table]\n\n  classDef implemented fill:lightblue,stroke:#333,stroke-width:4px;\n  class resource,table,handsontable implemented;", 
            "title": "Table support"
        }, 
        {
            "location": "/developers/views/#summary", 
            "text": "Figure 3: From Data Package View to Rendered output flow (richer version of diagram 1)", 
            "title": "Summary"
        }, 
        {
            "location": "/developers/views/#views-in-the-showcase", 
            "text": "To render Data Packages in browsers we use DataHub views written in JavaScript. The module implemented in ReactJS framework and it can render tables, maps and various graphs using third-party libraries.  Implementing code can be found in:   dpr-js repo  - which in turn depends on  datapackage-render-js   \n  graph TD\n\n  url[\"metadata URL passed from back-end\"]\n  dp-js[datapackage-js]\n  dprender[datapackage-render-js]\n  table[\"table view\"]\n  chart[\"graph view\"]\n  hot[HandsOnTable]\n  map[LeafletMap]\n  vega[Vega]\n  plotly[Plotly]\n  browser[Browser]\n\n  url --> dp-js\n  dp-js --fetched dp--> dprender\n  dprender --spec--> table\n  table --1..n--> hot\n  dprender --geojson--> map\n  dprender --spec--> chart\n  chart --0..n--> vega\n  chart --0..n--> plotly\n  hot --table--> browser\n  map --map--> browser\n  vega --graph--> browser\n  plotly --graph--> browser  Notice that DataHub views render a table view per tabular resource. If GeoJSON resource is given, it renders a map. Graph views should be specified in  views  property of a Data Package.", 
            "title": "Views in the Showcase"
        }, 
        {
            "location": "/developers/views/#appendix", 
            "text": "There is a separate page with  additional research material regarding views specification and tooling .", 
            "title": "Appendix"
        }, 
        {
            "location": "/developers/views-research/", 
            "text": "Views Research\n\n\n\n\n\n\nViews Research\n\n\nSimple Views - FAQ\n\n\nWhy not vega-lite?\n\n\nWhy not this alternative series notation:\n\n\n\n\n\n\nRecline Views\n\n\ntype\n attribute\n\n\nstate\n attribute\n\n\nExample of the views attribute:\n\n\n\n\n\n\nAnalysis of Vis libraries Graph Specs\n\n\nAnalysis of Vis libraries data objects\n\n\nVega\n\n\nRemote Data\n\n\n\n\n\n\nVega-Lite\n\n\nPlotly\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nAppendix: Plotly Graph spec research\n\n\nData Transform Research\n\n\nPlotly Transforms\n\n\nVega Transforms\n\n\nFiltering\n\n\nGeopath, aggregate, lookup, filter, sort, voronoi and linkpath\n\n\nFurther research on Vega transforms\n\n\nOld analysis\n\n\n\n\n\n\nDP Pipelines transforms\n\n\nconcatenate\n\n\njoin\n\n\n\n\n\n\nVega Dataflow usage for DP views\n\n\nAggregate example\n\n\nFilter example\n\n\nFormula example\n\n\nSample example\n\n\n\n\n\n\nSuggestion on usage from datapackage.json\n\n\nAggregate\n\n\nFilter\n\n\nFormula\n\n\nSample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Views - FAQ\n\n\nWhy not vega-lite?\n\n\n\n\nvega-lite multiple lines approach is a bit weird and counter-intuitive. This matters as this is very common.\n\n\noverall vega-lite retains too much complexity.\n\n\n\n\nVega-lite line example:\n\n\n{\n  \nmark\n: \nline\n,\n  \nencoding\n: {\n    \nx\n: {\nfield\n: \nDate\n, \ntype\n: \ntemporal\n},\n    \ny\n: {\nfield\n: \nVIXClose\n, \ntype\n: \nquantitative\n}\n  }\n}\n\n\n\nWhy not this alternative series notation:\n\n\n  series: [[\nx\n, \ny\n], [\nx\n, \nz\n]]\n\n\n\n\n\npros: explicit about the series \n \n\n\npros: you can plot two different series with different x values (as long as they have some commonality \n )\n\n\ncons: more verbose.\n\n\ncons: 2nd multiple x values point is actually confusing for most users \n (?)\n\n\n\n\nSimplicity is crucial here so those cons outweight the pros.\n\n\nRecline Views\n\n\nTo specify a Recline view, it must have \ntype\n and \nstate\n attributes. \n\n\n\"type\"\n attribute\n\n\n\n\nWe used to use this attribute to identify what graph type we need: plotyl or vega-lite. I suppose it should be used for something else. Right now, if \n\"type\"\n attribute  is set to \nvega-lite\n, we render vega-lite chart. In all other cases we render plotly chart.\n\n\n\n\n\"state\"\n attribute\n\n\n\"state\"\n attribute must be an object and have \n\"graphType\"\n, \n\"group\"\n and \n\"series\"\n attributes.\n\n\n\"graphType\"\n indicates type of the chart - line chart, bar chart, etc. Value must be a string.\n\n\n\"group\"\n is used to specify base axis - right now it is used as abscissa. It must be a string that is usually a primary key in a resource.\n\n\n\"series\"\n is used to specify ordinate - it must be an array of string elements. Each element represents a field name.\n\n\nExample of the \nviews\n attribute:\n\n\nviews\n: [\n    {\n      \ntype\n: \nGraph\n,\n      \nstate\n: {\n        \ngraphType\n: \nlines\n,\n        \ngroup\n: \ndate\n,\n        \nseries\n: [ \nautumn\n ]\n      }\n    }\n  ]\n\n\n\nAnalysis of Vis libraries Graph Specs\n\n\nTODO. Plotly partially covered below.\n\n\nAnalysis of Vis libraries data objects\n\n\nFocus on \ninline\n data structure - i.e. data structure in memory.\n\n\nMotivation for this: need to generate this data structure.\n\n\nVega\n\n\nSee: \nhttps://github.com/vega/vega/wiki/Data#examples\n\n\nData structure is standard \narray of objects\n:\n\n\n[\n  {\nx\n:0, \ny\n:3},\n  {\nx\n:1, \ny\n:5}\n]\n\n// note in Vega docs it is put as ...\n[{\nx\n:0, \ny\n:3}, {\nx\n:1, \ny\n:5}]\n\n\n\nAlso supports simple array of values \n[1,2,3]\n which is implicitly mapped to:\n\n\n[\n  { \ndata\n: 1 },\n  { \ndata\n: 2 },\n  { \ndata\n: 3 }\n]\n\n\n\nNote that internally vega adds \n_id\n to all rows as a unique identifier (cf pandas).\n\n\n// this input\n[{\nx\n:0, \ny\n:3}, {\nx\n:1, \ny\n:5}]\n\n// internally becomes\n[{\n_id\n:0, \nx\n:0, \ny\n:3}, {\n_id\n:1, \nx\n:1, \ny\n:5}]\n\n\n\nYou can also add a \nname\n attribute to name the data table and then the data is put in \nvalues\n:\n\n\n{\n  \nname\n: \ntable\n,\n  \nvalues\n: [12, 23, 47, 6, 52, 19]\n}\n\n\n\nFinally, inside the overall vega spec you put the data inside a \ndata\n property:\n\n\n{\n  \nwidth\n: 400,\n  \nheight\n: 200,\n  \npadding\n: {\ntop\n: 10, \nleft\n: 30, \nbottom\n: 30, \nright\n: 10},\n  \ndata\n: [\n    {\n      \nname\n: \ntable\n,\n      \nvalues\n: [\n        {\nx\n: 1,  \ny\n: 28}, {\nx\n: 2,  \ny\n: 55},\n                ...\n\n\n\nSee \nhttps://vega.github.io/vega-editor/?mode=vega\n\n\nRemote Data\n\n\nLooks like a lot like Resource. Assume that data is mapped to inline structure\n\n\nVega-Lite\n\n\nhttps://vega.github.io/vega-lite/docs/data.html\n\n\nSame as Vega except that:\n\n\n\n\ndata\n is an object not an array \n only one data source allowed\n\n\nWill be given the name \nsource\n when converting to Vega\n\n\n\n\n\n\nOnly one property allowed: \nvalues\n\n\nAnd for remote data: \nurl\n and \nformat\n\n\n\n\n\n\n\n\nPlotly\n\n\nhttp://help.plot.ly/json-chart-schema/\n\n\nhttps://plot.ly/javascript/reference/\n\n\nThe Plotly model does not separate the data out quite as cleanly as vega does. The structure for Plotly json specs is as follows:\n\n\n\n\nOriented around \nseries\n\n\nEach series includes its data \nplus\n the spec for the graph\n\n\nThe data is stored in two attributes \nx\n and \ny\n\n\n\n\n\n\nSeparate \nlayout\n property giving overall layout (e.g. margins, titles etc)\n\n\n\n\nTo give a sense of how it works this is the JS for creating a Plotly graph:\n\n\nPlotly.plot('graphDiv', data, layout);\n\n\n\nExamples\n\n\nFrom \nhttp://help.plot.ly/json-chart-schema/\n and links therein:\n\n\n{\n    \ndata\n: [\n        {\n            \nx\n: [\n                \ngiraffes\n, \n                \norangutans\n, \n                \nmonkeys\n\n            ], \n            \ny\n: [\n                20, \n                14, \n                23\n            ], \n            \ntype\n: \nbar\n\n        }\n    ]\n}\n\n\n\n[\n  {\n    \nname\n: \nSF Zoo\n,\n    \nmarker\n: {\n      \ncolor\n: \nrgba(55, 128, 191, 0.6)\n,\n      \nline\n: {\n        \ncolor\n: \nrgba(55, 128, 191, 1.0)\n,\n        \nwidth\n: 1\n      }\n    },\n    \ny\n: [\n      \ngiraffes\n,\n      \norangutans\n,\n      \nmonkeys\n\n    ],\n    \nx\n: [\n      20,\n      14,\n      23\n    ],\n    \ntype\n: \nbar\n,\n    \norientation\n: \nh\n,\n    \nuid\n: \na4a45d\n\n  },\n  {\n    \nname\n: \nLA Zoo\n,\n    \nmarker\n: {\n      \ncolor\n: \nrgba(255, 153, 51, 0.6)\n,\n      \nline\n: {\n        \ncolor\n: \nrgba(255, 153, 51, 1.0)\n,\n        \nwidth\n: 1\n      }\n    },\n    \ny\n: [\n      \ngiraffes\n,\n      \norangutans\n,\n      \nmonkeys\n\n    ],\n    \nx\n: [\n      12,\n      18,\n      29\n    ],\n    \ntype\n: \nbar\n,\n    \norientation\n: \nh\n,\n    \nuid\n: \nd912bc\n\n  }\n]\n\n\n\nAppendix: Plotly Graph spec research\n\n\nWe would like users to be able to use Plotly JSON chart schema in their Data Package Views specs so they can take full advantage of Plotly\ns capabilities.\n\n\nHere\ns an example - \nhttps://plot.ly/~Dreamshot/8259/\n\n\n{\n    \ndata\n: [\n        {\n            \nname\n: \nCol2\n, \n            \nuid\n: \nbabced\n, \n            \nfillcolor\n: \nrgb(224, 102, 102)\n, \n            \ny\n: [\n                \n17087182\n, \n                \n29354370\n, \n                \n38760373\n, \n                \n40912332\n, \n            ], \n            \nx\n: [\n                \n2000-01-01\n, \n                \n2001-01-01\n, \n                \n2002-01-01\n, \n                \n2003-01-01\n, \n            ], \n            \nfill\n: \ntonexty\n, \n            \ntype\n: \nscatter\n, \n            \nmode\n: \nnone\n\n        }\n    ], \n    \nlayout\n: {\n        \nautosize\n: false, \n        \nyaxis\n: {\n            \nrange\n: [\n                0, \n                1124750578.9473684\n            ], \n            \ntype\n: \nlinear\n, \n            \nautorange\n: true, \n            \ntitle\n: \n\n        }, \n        \ntitle\n: \nTotal Number of Websites\n, \n        \nheight\n: 500, \n        \nwidth\n: 800, \n        \nxaxis\n: {\n            \ntickformat\n: \n%Y\n, \n            \ntitle\n: \n...\n, \n            \nshowgrid\n: false, \n            \nrange\n: [\n                946702800000, \n                1451624400000\n            ], \n            \ntype\n: \ndate\n, \n            \nautorange\n: true\n        }\n    }\n}\n\n\n\nSo, the major requirement will be link the plotly data structure with an external data resources in the Data Package View.\n\n\nKey point:  Plotly data is of form:\n\n\ndata: [\n    {\n      \nname\n: ...\n      x: [...]\n      y: [...],\n      z: [...] // optional\n    },\n    ...\n  ]\n}\n\n\n\nWe just need a way to bind these \n \n\n\ndata: [\n    {\n      name: // by convention this must match the resource - if that is not possible use resource\n      resource: .... // only if name cannot match resource\n      x: \nfield name ...\n  // if this is a string not an array then look it up in the resource ...\n      y: \nfield name ...\n\n      z: \nfield name ...\n\n    },\n    ...\n  ]\n}\n\n\n\nUsing this approach we would support most of Basic, Statistical and 3D charts of Plotly library. We would not support pie chart (labels, values), maps \n\n\n\n\nData manipulations \n not supported\n\n\nIn some examples of Plotly there are manipulations (e.g. filtering) on the raw data. As this is done in Javascript outside of Plotly JSON language we would not be able to support this.\n\n\n\n\nIn the \nplotlyToPlotly\n function:\n\n\nexport function plotlyToPlotly(view) {\n  let plotlySpec = Object.assign({}, view.spec)\n\n  for trace in plotlySpec.data {\n    if(trace.resource) {\n      let resource = findResourceByNameOrIndex(view, trace.resource)\n      const rowsAsObjects = true\n      const rows = getResourceCachedValues(resource, rowsAsObjects)\n      if(trace.xField) {\n        trace.x = rows.map(row =\n row[trace.xField])\n        delete trace.xField\n      }\n      if(trace.yField) {\n        trace.y = rows.map(row =\n row[trace.yField])\n        delete trace.yField\n      }\n      if(trace.zField) {\n        trace.z = rows.map(row =\n row[trace.zField])\n        delete trace.zField\n      }\n\n      delete trace.resource      \n    }\n  }\n\n  return plotlySpec\n}\n\n\n\nData Transform Research\n\n\nPlotly Transforms\n\n\nNo libraries for data transform have been found.\n\n\nVega Transforms\n\n\nhttps://github.com/vega/vega/wiki/Data-Transforms\n - v2\n\n\nhttps://vega.github.io/vega/docs/transforms/\n - v3\n\n\nVega provided Data Transforms can be used to manipulate datasets before rendering a visualisation. E.g., one may need to perform transformations such as aggregation or filtering (there many types, see link above) of a dataset and display the graph only after that. Another situation would be creating a new dataset by applying various calculations on an old one.\n\n\nUsually transforms are defined in \ntransform\n array inside \ndata\n property.\n\n\nTransforms that do not filter or generate new data objects can be used within the transform array of a mark definition to specify post-encoding transforms.\n\n\nExamples:\n\n\nFiltering\n\n\nhttps://vega.github.io/vega-editor/?mode=vega\nspec=parallel_coords\n\n\nThis example filters rows that have both \nHorsepower\n and \nMiles_per_Gallon\n fields.\n\n\n{\n \ndata\n: [\n    {\n      \nname\n: \ncars\n,\n      \nurl\n: \ndata/cars.json\n,\n      \ntransform\n: [\n        {\n          \ntype\n: \nfilter\n,\n          \ntest\n: \ndatum.Horsepower \n datum.Miles_per_Gallon\n\n        }  \n      ]\n    }\n  ] \n}\n\n\n\nGeopath, aggregate, lookup, filter, sort, voronoi and linkpath\n\n\nhttps://vega.github.io/vega-editor/?mode=vega\nspec=airports\n\n\nThis example has a lot of transforms - in some cases there is only transform applied to a dataset, in other cases there are sequence of transforms.\n\n\nIn the first dataset, it applies \ngeopath\n transform which maps GeoJSON features to SVG path strings. It uses \nalberUsa\n projection type (\nmore about projection\n).\n\n\nIn the second dataset, it applies sum operation on \ncount\n field and outputs it as \nflights\n fields.\n\n\nIn the third dataset: \n1) it compares its \niata\n field against \norigin\n field of \ntraffic\n dataset. Matching values are outputed as \ntraffic\n field. \n2) Next, it filters out all values that are null.\n3) After that, it applies \ngeo\n transform as in the first dataset above. \n4) Next, it filters out layout_x and layout_y values that are null. \n5) Then, it sorts dataset by traffic.flights field in descending order.\n6) After that, it applies \nvoronoi\n transform to compute voronoi diagram based on \nlayout_x\n and \nlayout_y\n fields.\n\n\nIn the last dataset:\n1) First, it filters values on which there is a signal called \nhover\n (specified in the Vega spec\ns \nsignals\n property) with \niata\n attribute that matches to the dataset\ns \norigin\n field.\n2) Next, it looks up matching values of \nairports\n dataset\ns \niata\n field against its \norigin\n and \ndestination\n fields. Output fields are saved as \n_source\n and \n_target\n.\n3) Filters \n_source\n and \n_target\n values that are truthy (not null).\n4) Finally, linkpath transform creates visual links between nodes (\nmore about linkpath\n).\n\n\n{\n  \ndata\n: [\n    {\n      \nname\n: \nstates\n,\n      \nurl\n: \ndata/us-10m.json\n,\n      \nformat\n: {\ntype\n: \ntopojson\n, \nfeature\n: \nstates\n},\n      \ntransform\n: [\n        {\n          \ntype\n: \ngeopath\n, \nprojection\n: \nalbersUsa\n,\n          \nscale\n: 1200, \ntranslate\n: [450, 280]\n        }\n      ]\n    },\n    {\n      \nname\n: \ntraffic\n,\n      \nurl\n: \ndata/flights-airport.csv\n,\n      \nformat\n: {\ntype\n: \ncsv\n, \nparse\n: \nauto\n},\n      \ntransform\n: [\n        {\n          \ntype\n: \naggregate\n, \ngroupby\n: [\norigin\n],\n          \nsummarize\n: [{\nfield\n: \ncount\n, \nops\n: [\nsum\n], \nas\n: [\nflights\n]}]\n        }\n      ]\n    },\n    {\n      \nname\n: \nairports\n,\n      \nurl\n: \ndata/airports.csv\n,\n      \nformat\n: {\ntype\n: \ncsv\n, \nparse\n: \nauto\n},\n      \ntransform\n: [\n        {\n          \ntype\n: \nlookup\n, \non\n: \ntraffic\n, \nonKey\n: \norigin\n,\n          \nkeys\n: [\niata\n], \nas\n: [\ntraffic\n]\n        },\n        {\n          \ntype\n: \nfilter\n,\n          \ntest\n: \ndatum.traffic != null\n\n        },\n        {\n          \ntype\n: \ngeo\n, \nprojection\n: \nalbersUsa\n,\n          \nscale\n: 1200, \ntranslate\n: [450, 280],\n          \nlon\n: \nlongitude\n, \nlat\n: \nlatitude\n\n        },\n        {\n          \ntype\n: \nfilter\n,\n          \ntest\n: \ndatum.layout_x != null \n datum.layout_y != null\n\n        },\n        { \ntype\n: \nsort\n, \nby\n: \n-traffic.flights\n },\n        { \ntype\n: \nvoronoi\n, \nx\n: \nlayout_x\n, \ny\n: \nlayout_y\n }\n      ]\n    },\n    {\n      \nname\n: \nroutes\n,\n      \nurl\n: \ndata/flights-airport.csv\n,\n      \nformat\n: {\ntype\n: \ncsv\n, \nparse\n: \nauto\n},\n      \ntransform\n: [\n        { \ntype\n: \nfilter\n, \ntest\n: \nhover \n hover.iata == datum.origin\n },\n        {\n          \ntype\n: \nlookup\n, \non\n: \nairports\n, \nonKey\n: \niata\n,\n          \nkeys\n: [\norigin\n, \ndestination\n], \nas\n: [\n_source\n, \n_target\n]\n        },\n        { \ntype\n: \nfilter\n, \ntest\n: \ndatum._source \n datum._target\n },\n        { \ntype\n: \nlinkpath\n }\n      ]\n    }\n  ]\n}\n\n\n\nFurther research on Vega transforms\n\n\nhttps://github.com/vega/vega-dataflow-examples/\n\n\nIt is quite difficult to me to read the code as there is not enough documentation. I have included here the simplest example:\n\n\nvega-dataflow.js\n contains Dataflow, all transforms and vega\ns utilities.\n\n\n!DOCTYPE HTML\n\n\nhtml\n\n  \nhead\n\n    \ntitle\nDataflow CountPattern\n/title\n\n    \nscript src=\n../../build/vega-dataflow.js\n/script\n\n    \nstyle\n\n      body { margin: 10px; font-family: Helvetica Neue, Arial; font-size: 14px; }\n      textarea { width: 800px; height: 200px; }\n      pre { font-family: Monaco; font-size: 10px; }\n    \n/style\n\n  \n/head\n\n  \nbody\n\n    \ntextarea id=\ntext\n/textarea\nbr/\n\n    \ninput id=\nslider\n type=\nrange\n min=\n2\n max=\n10\n value=\n4\n/\n\n    Frequency Threshold\nbr/\n\n    \npre id=\noutput\n/pre\n\n  \n/body\n\n\n/html\n\n\n\ndf\n is a Dataflow instance where we register (.add) functions and parameters - as below on line 36-38. The same with adding transforms - lines 40-44. We can pass different parameters to the transforms depending on requirements of each of them. Event handlers can added by using \n.on\n method of the Dataflow instance - lines 46-48.\n\n\nvar tx = vega.transforms; // all transforms \nvar out = document.querySelector('#output');\nvar area = document.querySelector('#text');\narea.value = [\n  \nDespite myriad tools for visualizing data, there remains a gap between the notational efficiency of high-level visualization systems and the expressiveness and accessibility of low-level graphical systems.\n\n].join('\\n\\n');\nvar stopwords = \n(i|me|my|myself|we|us|our|ours|ourselves|you|your|yours|yourself|yourselves|he|him|his)\n;\n\nvar get = vega.field('data');\n\nfunction readText(_, pulse) {\n  if (this.value) pulse.rem = this.value;\n  return pulse.source = pulse.add = [vega.ingest(area.value)];\n}\n\nfunction threshold(_) {\n  var freq = _.freq,\n      f = function(t) { return t.count \n= freq; };\n  return (f.fields = ['count'], f);\n}\n\nfunction updatePage() {\n  out.innerText = c1.value.slice()\n    .sort(function(a,b) {\n      return (b.count - a.count)\n        || (b.text \n a.text ? -1 : a.text \n b.text ? 1 : 0);\n    })\n    .map(function(t) {\n      return t.text + ': ' + t.count;\n    })\n    .join('\\n');\n}\n\nvar df = new vega.Dataflow(), // create a new Dataflow instance\n// then add various operators into Dataflow instance:\n    ft = df.add(4), // word frequency threshold\n    ff = df.add(threshold, {freq:ft})\n    rt = df.add(readText),\n    // add a transforms (tx):\n    cp = df.add(tx.CountPattern, {field:get, case:'lower',\n      pattern:'[\\\\w\\']{2,}', stopwords:stopwords, pulse:rt}),\n    cc = df.add(tx.Collect, {pulse:cp}),\n    fc = df.add(tx.Filter, {expr:ff, pulse:cc}),\n    c1 = df.add(tx.Collect, {pulse:fc}),\n    up = df.add(updatePage, {pulse: c1});\ndf.on(df.events(area, 'keyup').debounce(250), rt)\n  .on(df.events('#slider', 'input'), ft, function(_, e) { return +e.target.value; })\n  .run();\n\n\n\n\n\nOld analysis\n\n\nThere are number of transforms and they are located in different libraries. Basics are here \nhttps://github.com/vega/vega-dataflow/tree/master/src/transforms\n\n\nGenerally, all data flow happens in the \nvega-dataflow module\n. There are lots of complicated operations performed to data input and parameters. Some of transform functions are inherited from another functions/classes which makes difficult to separate them:\n\n\nFilter function:\n\n\nexport default function Filter(params) {\n  Transform.call(this, fastmap(), params);\n}\n\nvar prototype = inherits(Filter, Transform);\n\n// more code for prototype\n\nand Transform is:\n\nimport Operator from './Operator';\nimport {inherits} from 'vega-util';\n\n/**\n * Abstract class for operators that process data tuples.\n * Subclasses must provide a {@link transform} method for operator processing.\n * @constructor\n * @param {*} [init] - The initial value for this operator.\n * @param {object} [params] - The parameters for this operator.\n * @param {Operator} [source] - The operator from which to receive pulses.\n */\nexport default function Transform(init, params) {\n  Operator.call(this, init, null, params);\n}\n\nvar prototype = inherits(Transform, Operator);\n\n/**\n * Overrides {@link Operator.evaluate} for transform operators.\n * Marshalls parameter values and then invokes {@link transform}.\n * @param {Pulse} pulse - the current dataflow pulse.\n * @return {Pulse} The output pulse (or StopPropagation). A falsy return\n     value (including undefined) will let the input pulse pass through.\n */\nprototype.evaluate = function(pulse) {\n  var params = this.marshall(pulse.stamp),\n      out = this.transform(params, pulse);\n  params.clear();\n  return out;\n};\n\n/**\n * Process incoming pulses.\n * Subclasses should override this method to implement transforms.\n * @param {Parameters} _ - The operator parameter values.\n * @param {Pulse} pulse - The current dataflow pulse.\n * @return {Pulse} The output pulse (or StopPropagation). A falsy return\n *   value (including undefined) will let the input pulse pass through.\n */\nprototype.transform = function() {};\n\n\nand as we can see Transform inherits from Operator and so on.\n\n\nBut some of the transform functions looks independent:\n\n\nGetting cross product:\n\n\n// filter is an optional  function for selectively including tuples in the cross product.\nfunction cross(input, a, b, filter) {\n  var data = [],\n      t = {},\n      n = input.length,\n      i = 0,\n      j, left;\n\n  for (; i\nn; ++i) {\n    t[a] = left = input[i];\n    for (j=0; j\nn; ++j) {\n      t[b] = input[j];\n      if (filter(t)) {\n        data.push(ingest(t));\n        t = {};\n        t[a] = left;\n      }\n    }\n  }\n\n  return data;\n}\n\n\n\nOther transforms:\n\n\n\n\nLinkpath \nhttps://github.com/vega/vega-encode\n\n\nGeo \nhttps://github.com/vega/vega-geo\n\n\nHierarchical \nhttps://github.com/vega/vega-hierarchy/tree/master/definitions\n\n\nvoronoi \nhttps://github.com/vega/vega-voronoi\n\n\ncrossfilter \nhttps://github.com/vega/vega-crossfilter\n\n\n\n\nDP Pipelines transforms\n\n\nDPP provides number of transforms that can be applied to a dataset. However, those transforms cannot be processed inside browsers as the library requires Python scripts to run.\n\n\nBelow is a copy-paste from DPP docs:\n\n\nconcatenate\n\n\nConcatenates a number of streamed resources and converts them to a single resource.\n\n\nParameters\n:\n\n\n\n\nsources\n - Which resources to concatenate. Same semantics as \nresources\n in \nstream_remote_resources\n.\n\n\n\n\nIf omitted, all resources in datapackage are concatenated.\n\n\nResources to concatenate must appear in consecutive order within the data-package.\n\n\n\n\n\n\ntarget\n - Target resource to hold the concatenated data. Should define at least the following properties:\n\n\n\n\n\n\nname\n - name of the resource\n\n\n\n\npath\n - path in the data-package for this file.\n\n\n\n\nIf omitted, the target resource will receive the name \nconcat\n and will be saved at \ndata/concat.csv\n in the datapackage.\n\n\n\n\nfields\n - Mapping of fields between the sources and the target, so that the keys are the \ntarget\n field names, and values are lists of \nsource\n field names.\n\n\n\n\nThis mapping is used to create the target resources schema.\n\n\nNote that the target field name is \nalways\n assumed to be mapped to itself.\n\n\nExample\n:\n\n\n- run: concatenate\n  parameters: \n    target:\n      name: multi-year-report\n      path: data/multi-year-report.csv\n    sources: 'report-year-20[0-9]{2}'\n    fields:\n      activity: []\n      amount: ['2009_amount', 'Amount', 'AMOUNT [USD]', '$$$']    \n\n\n\nIn this example we concatenate all resources that look like \nreport-year-\nyear\n, and output them to the \nmulti-year-report\n resource.\n\n\nThe output contains two fields:\n\n\n\n\nactivity\n , which is called \nactivity\n in all sources\n\n\namount\n, which has varying names in different resources (e.g. \nAmount\n, \n2009_amount\n, \namount\n etc.)\n\n\n\n\njoin\n\n\nJoins two streamed resources. \n\n\nJoining\n in our case means taking the \ntarget\n resource, and adding fields to each of its rows by looking up data in the \nsource\n resource. \n\n\nA special case for the join operation is when there is no target stream, and all unique rows from the source are used to create it. \nThis mode is called \ndeduplication\n mode - The target resource will be created and  deduplicated rows from the source will be added to it.\n\n\nParameters\n:\n\n\n\n\nsource\n - information regarding the \nsource\n resource\n\n\nname\n - name of the resource\n\n\nkey\n - One of\n\n\nList of field names which should be used as the lookup key\n\n\nString, which would be interpreted as a Python format string used to form the key (e.g. \n{\nfield_name_1\n}:{field_name_2}\n)\n\n\n\n\n\n\ndelete\n - delete from data-package after joining (\nFalse\n by default)\n\n\ntarget\n - Target resource to hold the joined data. Should define at least the following properties:\n\n\nname\n - as in \nsource\n\n\nkey\n - as in \nsource\n, or \nnull\n for creating the target resource and performing \ndeduplication\n.\n\n\nfields\n - mapping of fields from the source resource to the target resource. \n  Keys should be field names in the target resource.\n  Values can define two attributes:\n\n\n\n\nname\n - field name in the source (by default is the same as the target field name)\n\n\n\n\n\n\naggregate\n - aggregation strategy (how to handle multiple \nsource\n rows with the same key). Can take the following options: \n\n\n\n\n\n\nsum\n - summarise aggregated values. \n  For numeric values it\ns the arithmetic sum, for strings the concatenation of strings and for other types will error.\n\n\n\n\n\n\navg\n - calculate the average of aggregated values.\n\n\n\n\n\n\nFor numeric values it\ns the arithmetic average and for other types will err.\n\n\n\n\nmax\n - calculate the maximum of aggregated values.\n\n\n\n\nFor numeric values it\ns the arithmetic maximum, for strings the dictionary maximum and for other types will error.\n\n\n\n\nmin\n - calculate the minimum of aggregated values.\n\n\n\n\nFor numeric values it\ns the arithmetic minimum, for strings the dictionary minimum and for other types will error.\n\n\n\n\n\n\nfirst\n - take the first value encountered\n\n\n\n\n\n\nlast\n - take the last value encountered\n\n\n\n\n\n\ncount\n - count the number of occurrences of a specific key\n  For this method, specifying \nname\n is not required. In case it is specified, \ncount\n will count the number of non-null values for that source field.\n\n\n\n\n\n\nset\n - collect all distinct values of the aggregated field, unordered \n\n\n\n\n\n\narray\n - collect all values of the aggregated field, in order of appearance   \n\n\n\n\n\n\nany\n - pick any value.\n\n\n\n\n\n\nBy default, \naggregate\n takes the \nany\n value.\n\n\n\n\n\n\nIf neither \nname\n or \naggregate\n need to be specified, the mapping can map to the empty object \n{}\n or to \nnull\n.\n- \nfull\n  - Boolean,\n  - If \nTrue\n (the default), failed lookups in the source will result in \nnull\n values at the source.\n  - if \nFalse\n, failed lookups in the source will result in dropping the row from the target.\n\n\nImportant: the \nsource\n resource \nmust\n appear before the \ntarget\n resource in the data-package.\n\n\nExamples\n:\n\n\n- run: join\n  parameters: \n    source:\n      name: world_population\n      key: [\ncountry_code\n]\n      delete: yes\n    target:\n      name: country_gdp_2015\n      key: [\nCC\n]\n    fields:\n      population:\n        name: \ncensus_2015\n        \n    full: true\n\n\n\nThe above example aims to create a package containing the GDP and Population of each country in the world.\n\n\nWe have one resource (\nworld_population\n) with data that looks like:\n\n\n\n\n\n\n\n\ncountry_code\n\n\ncountry_name\n\n\ncensus_2000\n\n\ncensus_2015\n\n\n\n\n\n\n\n\n\n\nUK\n\n\nUnited Kingdom\n\n\n58857004\n\n\n64715810\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd another resource (\ncountry_gdp_2015\n) with data that looks like:\n\n\n\n\n\n\n\n\nCC\n\n\nGDP (\u00a3m)\n\n\nNet Debt (\u00a3m)\n\n\n\n\n\n\n\n\n\n\nUK\n\n\n1832318\n\n\n1606600\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe \njoin\n command will match rows in both datasets based on the \ncountry_code\n / \nCC\n fields, and then copying the value in the \ncensus_2015\n field into a new \npopulation\n field.\n\n\nThe resulting data package will have the \nworld_population\n resource removed and the \ncountry_gdp_2015\n resource looking like:\n\n\n\n\n\n\n\n\nCC\n\n\nGDP (\u00a3m)\n\n\nNet Debt (\u00a3m)\n\n\npopulation\n\n\n\n\n\n\n\n\n\n\nUK\n\n\n1832318\n\n\n1606600\n\n\n64715810\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA more complex example:\n\n\n- run: join\n  parameters: \n    source:\n      name: screen_actor_salaries\n      key: \n{production} ({year})\n\n    target:\n      name: mgm_movies\n      key: \n{title}\n\n    fields:\n      num_actors:\n        aggregate: 'count'\n      average_salary:\n        name: salary\n        aggregate: 'avg'\n      total_salaries:\n        name: salary\n        aggregate: 'sum'\n    full: false\n\n\n\nThis example aims to analyse salaries for screen actors in the MGM studios.\n\n\nOnce more, we have one resource (\nscreen_actor_salaries\n) with data that looks like:\n\n\n\n\n\n\n\n\nyear\n\n\nproduction\n\n\nactor\n\n\nsalary\n\n\n\n\n\n\n\n\n\n\n2016\n\n\nVertigo 2\n\n\nMr. T\n\n\n15000000\n\n\n\n\n\n\n2016\n\n\nVertigo 2\n\n\nRobert Downey Jr.\n\n\n7000000\n\n\n\n\n\n\n2015\n\n\nThe Fall - Resurrection\n\n\nJeniffer Lawrence\n\n\n18000000\n\n\n\n\n\n\n2015\n\n\nAlf - The Return to Melmack\n\n\nThe Rock\n\n\n12000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd another resource (\nmgm_movies\n) with data that looks like:\n\n\n\n\n\n\n\n\ntitle\n\n\ndirector\n\n\nproducer\n\n\n\n\n\n\n\n\n\n\nVertigo 2 (2016)\n\n\nLindsay Lohan\n\n\nLee Ka Shing\n\n\n\n\n\n\niRobot - The Movie (2018)\n\n\nMr. T\n\n\nMr. T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe \njoin\n command will match rows in both datasets based on the movie name and production year. Notice how we overcome incompatible fields by using different key patterns.\n\n\nThe resulting dataset could look like:\n\n\n\n\n\n\n\n\ntitle\n\n\ndirector\n\n\nproducer\n\n\nnum_actors\n\n\naverage_salary\n\n\ntotal_salaries\n\n\n\n\n\n\n\n\n\n\nVertigo 2 (2016)\n\n\nLindsay Lohan\n\n\nLee Ka Shing\n\n\n2\n\n\n11000000\n\n\n22000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVega Dataflow usage for DP views\n\n\nVega has quite a lot of data transform functions available, however, most of them require complicated JSON descriptor to use. Although we may implement them in the future, at the moment we could start with the most basic and essential ones:\n\n\nList of transforms that we could use:\n\n\n\n\nAggregate\n\n\nFilter\n\n\nFormula (applies given formula to dataset)\n\n\nSample\n\n\n\n\nAggregate example\n\n\nWe have dataset with 4 fields - a, b, c and d. Lets apply different aggregation methods on them - count, sum, min and max:\n\n\nconst vegadataflow = require('./build/vega-dataflow.js');\n\nvar tx = vegadataflow.transforms,\n    changeset = vegadataflow.changeset;\n\nvar data = [\n {\n   \na\n: 17.76,\n   \nb\n: 20.14,\n   \nc\n: 17.05,\n   \nd\n: 17.79\n },\n {\n   \na\n: 19.19,\n   \nb\n: 21.29,\n   \nc\n: 19.19,\n   \nd\n: 19.92\n },\n {\n   \na\n: 20.33,\n   \nb\n: 22.9,\n   \nc\n: 19.52,\n   \nd\n: 21.12\n },\n {\n   \na\n: 20.15,\n   \nb\n: 20.72,\n   \nc\n: 19.04,\n   \nd\n: 19.31\n },\n {\n   \na\n: 17.93,\n   \nb\n: 18.09,\n   \nc\n: 16.99,\n   \nd\n: 17.01\n }\n];\n\nvar a = vegadataflow.field('a'),\n    b = vegadataflow.field('b'),\n    c = vegadataflow.field('c'),\n    d = vegadataflow.field('d');\n\nvar df = new vegadataflow.Dataflow(),\n    col = df.add(tx.Collect),\n    agg = df.add(tx.Aggregate, {\n            fields: [a, b, c, d],\n            ops: ['count', 'sum', 'min', 'max'],\n            pulse: col\n          }),\n    out = df.add(tx.Collect, {pulse: agg});\n\ndf.pulse(col, changeset().insert(data)).run();\n\nconsole.dir(out.value);\n\n\n\nOutput:\n\n[ \n  {\n    _id: 7, \n    count_a: 5, \n    sum_b: 103.14, \n    min_c: 16.99, \n    max_d: 21.12 \n  }\n]\n\n\nFilter example\n\n\nUsing the dataset from example above, lets filter values of field \na\n that are not greater than 19:\n\n\nconst vegadataflow = require('./build/vega-dataflow.js');\n\nvar tx = vegadataflow.transforms,\n    changeset = vegadataflow.changeset;\n\nvar data = [\n {\n   \na\n: 17.76,\n   \nb\n: 20.14,\n   \nc\n: 17.05,\n   \nd\n: 17.79\n },\n {\n   \na\n: 19.19,\n   \nb\n: 21.29,\n   \nc\n: 19.19,\n   \nd\n: 19.92\n },\n {\n   \na\n: 20.33,\n   \nb\n: 22.9,\n   \nc\n: 19.52,\n   \nd\n: 21.12\n },\n {\n   \na\n: 20.15,\n   \nb\n: 20.72,\n   \nc\n: 19.04,\n   \nd\n: 19.31\n },\n {\n   \na\n: 17.93,\n   \nb\n: 18.09,\n   \nc\n: 16.99,\n   \nd\n: 17.01\n }\n];\n\nvar a = vegadataflow.field('a');\n\nvar filter1 = vegadataflow.accessor(d =\n { return d.a \n 19 }, ['a']);\n\nvar df = new vegadataflow.Dataflow(),\n    ex = df.add(null),\n    col = df.add(tx.Collect),\n    fil = df.add(tx.Filter, {expr: ex, pulse: col}),\n    out = df.add(tx.Collect, {pulse: fil});\n\ndf.pulse(col, changeset().insert(data));\ndf.update(ex, filter1).run();\n\nconsole.log(out.value);\n\n\n\n\nOutput:\n\n[ \n  { a: 19.19, b: 21.29, c: 19.19, d: 19.92, _id: 3 },\n  { a: 20.33, b: 22.9, c: 19.52, d: 21.12, _id: 4 },\n  { a: 20.15, b: 20.72, c: 19.04, d: 19.31, _id: 5 } \n]\n\n\nFormula example\n\n\nUsing the same dataset, lets apply mapping on a field:\n\n\nconst vegadataflow = require('./build/vega-dataflow.js');\n\nvar tx = vegadataflow.transforms,\n    changeset = vegadataflow.changeset;\n\nvar data = [\n {\n   \na\n: 17.76,\n   \nb\n: 20.14,\n   \nc\n: 17.05,\n   \nd\n: 17.79\n },\n {\n   \na\n: 19.19,\n   \nb\n: 21.29,\n   \nc\n: 19.19,\n   \nd\n: 19.92\n },\n {\n   \na\n: 20.33,\n   \nb\n: 22.9,\n   \nc\n: 19.52,\n   \nd\n: 21.12\n },\n {\n   \na\n: 20.15,\n   \nb\n: 20.72,\n   \nc\n: 19.04,\n   \nd\n: 19.31\n },\n {\n   \na\n: 17.93,\n   \nb\n: 18.09,\n   \nc\n: 16.99,\n   \nd\n: 17.01\n }\n];\n\n\nvar df = new vegadataflow.Dataflow(),\n    e = vegadataflow.field('e'),\n    f = vegadataflow.field('f'),\n    formula1 = vegadataflow.accessor(d =\n { return d.a * 10; }, ['a']),\n    formula2 = vegadataflow.accessor(d =\n { return d.b / 10; }, ['b']),\n    col = df.add(tx.Collect),\n    fa = df.add(tx.Formula, {expr: formula1, as: 'e', pulse: col}),\n    fb = df.add(tx.Formula, {expr: formula2, as: 'f', pulse: fa});\n\ndf.pulse(col, changeset().insert(data)).run();\n\nconsole.log(col.value.map(e));\nconsole.log(col.value.map(f));\n\n\n\nOutput:\n\n[ 177.60000000000002, 191.9, 203.29999999999998, 201.5, 179.3 ]\n[ 2.0140000000000002, 2.129, 2.29, 2.072, 1.809 ]\n\n\nSample example\n\n\nLets create a dataset with 100 rows and take a sample of 10 from it:\n\n\nconst vegadataflow = require('./build/vega-dataflow.js');\n\nvar tx = vegadataflow.transforms,\n    changeset = vegadataflow.changeset;\n\nvar n = 100,\n    sampleSize = 10,\n    data = Array(n),\n    i;\n\nfor(i=0; i\nn; i++) data[i] = {v:Math.random()};\n\nvar df = new vegadataflow.Dataflow(),\n    s = df.add(tx.Sample, {size: sampleSize});\n\ndf.pulse(s, changeset().insert(data)).run();\n\nconsole.log(s.value);\n\n\n\nOutput:\n\n[ \n  { v: 0.3332451883830292, _id: 69 },\n  { v: 0.2874480689159735, _id: 3 },\n  { v: 0.18009915754527817, _id: 41 },\n  { v: 0.10513776386462825, _id: 27 },\n  { v: 0.4972760501252764, _id: 35 },\n  { v: 0.757859721485594, _id: 67 },\n  { v: 0.248170225498199, _id: 64 },\n  { v: 0.431513510601889, _id: 28 },\n  { v: 0.07281378713091247, _id: 37 },\n  { v: 0.9543216903991236, _id: 33 } \n]\n\n\nSuggestion on usage from datapackage.json\n\n\nOur current simple view layout:\n\n\n{\n  name: 'sample',\n  resource: [0],\n  specType: 'simple',\n  spec: {\n    type: 'line',\n    group: 'a',\n    series: ['b', 'c']\n  }\n}\n\n\n\nWe could add \ntransform\n property that would be a specification for transforms to be applied. Each transform would have slightly different properties:\n\n\nAggregate\n\n\n{\n  ...\n  transform: {\n    type: 'aggregate',\n    fields: ['a', 'b'],\n    operations: ['sum', 'min']\n  },\n  ...\n}\n\nFor \naggregate\n transform, a publisher should pass a field name and an operation to be applied. Operations should be one of \nhttps://vega.github.io/vega/docs/transforms/aggregate/\n\n\nFilter\n\n\n{\n  ...\n  transform: {\n    type: 'filter',\n    expr: 'data.fieldName \n 10'\n  },\n  ...\n}\n\nFor \nfilter\n type expression should evaluate to true or false so only truthy values will be kept.\n\n\nFormula\n\n\n{\n  ...\n  transform: {\n    type: 'formula',\n    expr: ['data.fieldName * 2', 'data.fieldName + 10'],\n    as: ['x', 'y']\n  },\n  ...\n}\n\n\n\nFor \nformula\n type, a field will be mapped with given expression and output will be stored in new fields that are specified in \nas\n property.\n\n\nSample\n\n\n  ...\n  transform: {\n    type: 'sample',\n    size: 'some integer'\n  },\n  ...\n\n\n\nIn \nsample\n type, only size of a sample is needed.", 
            "title": "Views Research"
        }, 
        {
            "location": "/developers/views-research/#views-research", 
            "text": "Views Research  Simple Views - FAQ  Why not vega-lite?  Why not this alternative series notation:    Recline Views  type  attribute  state  attribute  Example of the views attribute:    Analysis of Vis libraries Graph Specs  Analysis of Vis libraries data objects  Vega  Remote Data    Vega-Lite  Plotly  Examples      Appendix: Plotly Graph spec research  Data Transform Research  Plotly Transforms  Vega Transforms  Filtering  Geopath, aggregate, lookup, filter, sort, voronoi and linkpath  Further research on Vega transforms  Old analysis    DP Pipelines transforms  concatenate  join    Vega Dataflow usage for DP views  Aggregate example  Filter example  Formula example  Sample example    Suggestion on usage from datapackage.json  Aggregate  Filter  Formula  Sample", 
            "title": "Views Research"
        }, 
        {
            "location": "/developers/views-research/#simple-views-faq", 
            "text": "", 
            "title": "Simple Views - FAQ"
        }, 
        {
            "location": "/developers/views-research/#why-not-vega-lite", 
            "text": "vega-lite multiple lines approach is a bit weird and counter-intuitive. This matters as this is very common.  overall vega-lite retains too much complexity.   Vega-lite line example:  {\n   mark :  line ,\n   encoding : {\n     x : { field :  Date ,  type :  temporal },\n     y : { field :  VIXClose ,  type :  quantitative }\n  }\n}", 
            "title": "Why not vega-lite?"
        }, 
        {
            "location": "/developers/views-research/#why-not-this-alternative-series-notation", 
            "text": "series: [[ x ,  y ], [ x ,  z ]]   pros: explicit about the series     pros: you can plot two different series with different x values (as long as they have some commonality   )  cons: more verbose.  cons: 2nd multiple x values point is actually confusing for most users   (?)   Simplicity is crucial here so those cons outweight the pros.", 
            "title": "Why not this alternative series notation:"
        }, 
        {
            "location": "/developers/views-research/#recline-views", 
            "text": "To specify a Recline view, it must have  type  and  state  attributes.", 
            "title": "Recline Views"
        }, 
        {
            "location": "/developers/views-research/#type-attribute", 
            "text": "We used to use this attribute to identify what graph type we need: plotyl or vega-lite. I suppose it should be used for something else. Right now, if  \"type\"  attribute  is set to  vega-lite , we render vega-lite chart. In all other cases we render plotly chart.", 
            "title": "\"type\" attribute"
        }, 
        {
            "location": "/developers/views-research/#state-attribute", 
            "text": "\"state\"  attribute must be an object and have  \"graphType\" ,  \"group\"  and  \"series\"  attributes.  \"graphType\"  indicates type of the chart - line chart, bar chart, etc. Value must be a string.  \"group\"  is used to specify base axis - right now it is used as abscissa. It must be a string that is usually a primary key in a resource.  \"series\"  is used to specify ordinate - it must be an array of string elements. Each element represents a field name.", 
            "title": "\"state\" attribute"
        }, 
        {
            "location": "/developers/views-research/#example-of-the-views-attribute", 
            "text": "views : [\n    {\n       type :  Graph ,\n       state : {\n         graphType :  lines ,\n         group :  date ,\n         series : [  autumn  ]\n      }\n    }\n  ]", 
            "title": "Example of the views attribute:"
        }, 
        {
            "location": "/developers/views-research/#analysis-of-vis-libraries-graph-specs", 
            "text": "TODO. Plotly partially covered below.", 
            "title": "Analysis of Vis libraries Graph Specs"
        }, 
        {
            "location": "/developers/views-research/#analysis-of-vis-libraries-data-objects", 
            "text": "Focus on  inline  data structure - i.e. data structure in memory.  Motivation for this: need to generate this data structure.", 
            "title": "Analysis of Vis libraries data objects"
        }, 
        {
            "location": "/developers/views-research/#vega", 
            "text": "See:  https://github.com/vega/vega/wiki/Data#examples  Data structure is standard  array of objects :  [\n  { x :0,  y :3},\n  { x :1,  y :5}\n]\n\n// note in Vega docs it is put as ...\n[{ x :0,  y :3}, { x :1,  y :5}]  Also supports simple array of values  [1,2,3]  which is implicitly mapped to:  [\n  {  data : 1 },\n  {  data : 2 },\n  {  data : 3 }\n]  Note that internally vega adds  _id  to all rows as a unique identifier (cf pandas).  // this input\n[{ x :0,  y :3}, { x :1,  y :5}]\n\n// internally becomes\n[{ _id :0,  x :0,  y :3}, { _id :1,  x :1,  y :5}]  You can also add a  name  attribute to name the data table and then the data is put in  values :  {\n   name :  table ,\n   values : [12, 23, 47, 6, 52, 19]\n}  Finally, inside the overall vega spec you put the data inside a  data  property:  {\n   width : 400,\n   height : 200,\n   padding : { top : 10,  left : 30,  bottom : 30,  right : 10},\n   data : [\n    {\n       name :  table ,\n       values : [\n        { x : 1,   y : 28}, { x : 2,   y : 55},\n                ...  See  https://vega.github.io/vega-editor/?mode=vega", 
            "title": "Vega"
        }, 
        {
            "location": "/developers/views-research/#remote-data", 
            "text": "Looks like a lot like Resource. Assume that data is mapped to inline structure", 
            "title": "Remote Data"
        }, 
        {
            "location": "/developers/views-research/#vega-lite", 
            "text": "https://vega.github.io/vega-lite/docs/data.html  Same as Vega except that:   data  is an object not an array   only one data source allowed  Will be given the name  source  when converting to Vega    Only one property allowed:  values  And for remote data:  url  and  format", 
            "title": "Vega-Lite"
        }, 
        {
            "location": "/developers/views-research/#plotly", 
            "text": "http://help.plot.ly/json-chart-schema/  https://plot.ly/javascript/reference/  The Plotly model does not separate the data out quite as cleanly as vega does. The structure for Plotly json specs is as follows:   Oriented around  series  Each series includes its data  plus  the spec for the graph  The data is stored in two attributes  x  and  y    Separate  layout  property giving overall layout (e.g. margins, titles etc)   To give a sense of how it works this is the JS for creating a Plotly graph:  Plotly.plot('graphDiv', data, layout);", 
            "title": "Plotly"
        }, 
        {
            "location": "/developers/views-research/#examples", 
            "text": "From  http://help.plot.ly/json-chart-schema/  and links therein:  {\n     data : [\n        {\n             x : [\n                 giraffes , \n                 orangutans , \n                 monkeys \n            ], \n             y : [\n                20, \n                14, \n                23\n            ], \n             type :  bar \n        }\n    ]\n}  [\n  {\n     name :  SF Zoo ,\n     marker : {\n       color :  rgba(55, 128, 191, 0.6) ,\n       line : {\n         color :  rgba(55, 128, 191, 1.0) ,\n         width : 1\n      }\n    },\n     y : [\n       giraffes ,\n       orangutans ,\n       monkeys \n    ],\n     x : [\n      20,\n      14,\n      23\n    ],\n     type :  bar ,\n     orientation :  h ,\n     uid :  a4a45d \n  },\n  {\n     name :  LA Zoo ,\n     marker : {\n       color :  rgba(255, 153, 51, 0.6) ,\n       line : {\n         color :  rgba(255, 153, 51, 1.0) ,\n         width : 1\n      }\n    },\n     y : [\n       giraffes ,\n       orangutans ,\n       monkeys \n    ],\n     x : [\n      12,\n      18,\n      29\n    ],\n     type :  bar ,\n     orientation :  h ,\n     uid :  d912bc \n  }\n]", 
            "title": "Examples"
        }, 
        {
            "location": "/developers/views-research/#appendix-plotly-graph-spec-research", 
            "text": "We would like users to be able to use Plotly JSON chart schema in their Data Package Views specs so they can take full advantage of Plotly s capabilities.  Here s an example -  https://plot.ly/~Dreamshot/8259/  {\n     data : [\n        {\n             name :  Col2 , \n             uid :  babced , \n             fillcolor :  rgb(224, 102, 102) , \n             y : [\n                 17087182 , \n                 29354370 , \n                 38760373 , \n                 40912332 , \n            ], \n             x : [\n                 2000-01-01 , \n                 2001-01-01 , \n                 2002-01-01 , \n                 2003-01-01 , \n            ], \n             fill :  tonexty , \n             type :  scatter , \n             mode :  none \n        }\n    ], \n     layout : {\n         autosize : false, \n         yaxis : {\n             range : [\n                0, \n                1124750578.9473684\n            ], \n             type :  linear , \n             autorange : true, \n             title :  \n        }, \n         title :  Total Number of Websites , \n         height : 500, \n         width : 800, \n         xaxis : {\n             tickformat :  %Y , \n             title :  ... , \n             showgrid : false, \n             range : [\n                946702800000, \n                1451624400000\n            ], \n             type :  date , \n             autorange : true\n        }\n    }\n}  So, the major requirement will be link the plotly data structure with an external data resources in the Data Package View.  Key point:  Plotly data is of form:  data: [\n    {\n       name : ...\n      x: [...]\n      y: [...],\n      z: [...] // optional\n    },\n    ...\n  ]\n}  We just need a way to bind these     data: [\n    {\n      name: // by convention this must match the resource - if that is not possible use resource\n      resource: .... // only if name cannot match resource\n      x:  field name ...   // if this is a string not an array then look it up in the resource ...\n      y:  field name ... \n      z:  field name ... \n    },\n    ...\n  ]\n}  Using this approach we would support most of Basic, Statistical and 3D charts of Plotly library. We would not support pie chart (labels, values), maps    Data manipulations   not supported  In some examples of Plotly there are manipulations (e.g. filtering) on the raw data. As this is done in Javascript outside of Plotly JSON language we would not be able to support this.   In the  plotlyToPlotly  function:  export function plotlyToPlotly(view) {\n  let plotlySpec = Object.assign({}, view.spec)\n\n  for trace in plotlySpec.data {\n    if(trace.resource) {\n      let resource = findResourceByNameOrIndex(view, trace.resource)\n      const rowsAsObjects = true\n      const rows = getResourceCachedValues(resource, rowsAsObjects)\n      if(trace.xField) {\n        trace.x = rows.map(row =  row[trace.xField])\n        delete trace.xField\n      }\n      if(trace.yField) {\n        trace.y = rows.map(row =  row[trace.yField])\n        delete trace.yField\n      }\n      if(trace.zField) {\n        trace.z = rows.map(row =  row[trace.zField])\n        delete trace.zField\n      }\n\n      delete trace.resource      \n    }\n  }\n\n  return plotlySpec\n}", 
            "title": "Appendix: Plotly Graph spec research"
        }, 
        {
            "location": "/developers/views-research/#data-transform-research", 
            "text": "", 
            "title": "Data Transform Research"
        }, 
        {
            "location": "/developers/views-research/#plotly-transforms", 
            "text": "No libraries for data transform have been found.", 
            "title": "Plotly Transforms"
        }, 
        {
            "location": "/developers/views-research/#vega-transforms", 
            "text": "https://github.com/vega/vega/wiki/Data-Transforms  - v2  https://vega.github.io/vega/docs/transforms/  - v3  Vega provided Data Transforms can be used to manipulate datasets before rendering a visualisation. E.g., one may need to perform transformations such as aggregation or filtering (there many types, see link above) of a dataset and display the graph only after that. Another situation would be creating a new dataset by applying various calculations on an old one.  Usually transforms are defined in  transform  array inside  data  property.  Transforms that do not filter or generate new data objects can be used within the transform array of a mark definition to specify post-encoding transforms.  Examples:", 
            "title": "Vega Transforms"
        }, 
        {
            "location": "/developers/views-research/#filtering", 
            "text": "https://vega.github.io/vega-editor/?mode=vega spec=parallel_coords  This example filters rows that have both  Horsepower  and  Miles_per_Gallon  fields.  {\n  data : [\n    {\n       name :  cars ,\n       url :  data/cars.json ,\n       transform : [\n        {\n           type :  filter ,\n           test :  datum.Horsepower   datum.Miles_per_Gallon \n        }  \n      ]\n    }\n  ] \n}", 
            "title": "Filtering"
        }, 
        {
            "location": "/developers/views-research/#geopath-aggregate-lookup-filter-sort-voronoi-and-linkpath", 
            "text": "https://vega.github.io/vega-editor/?mode=vega spec=airports  This example has a lot of transforms - in some cases there is only transform applied to a dataset, in other cases there are sequence of transforms.  In the first dataset, it applies  geopath  transform which maps GeoJSON features to SVG path strings. It uses  alberUsa  projection type ( more about projection ).  In the second dataset, it applies sum operation on  count  field and outputs it as  flights  fields.  In the third dataset: \n1) it compares its  iata  field against  origin  field of  traffic  dataset. Matching values are outputed as  traffic  field. \n2) Next, it filters out all values that are null.\n3) After that, it applies  geo  transform as in the first dataset above. \n4) Next, it filters out layout_x and layout_y values that are null. \n5) Then, it sorts dataset by traffic.flights field in descending order.\n6) After that, it applies  voronoi  transform to compute voronoi diagram based on  layout_x  and  layout_y  fields.  In the last dataset:\n1) First, it filters values on which there is a signal called  hover  (specified in the Vega spec s  signals  property) with  iata  attribute that matches to the dataset s  origin  field.\n2) Next, it looks up matching values of  airports  dataset s  iata  field against its  origin  and  destination  fields. Output fields are saved as  _source  and  _target .\n3) Filters  _source  and  _target  values that are truthy (not null).\n4) Finally, linkpath transform creates visual links between nodes ( more about linkpath ).  {\n   data : [\n    {\n       name :  states ,\n       url :  data/us-10m.json ,\n       format : { type :  topojson ,  feature :  states },\n       transform : [\n        {\n           type :  geopath ,  projection :  albersUsa ,\n           scale : 1200,  translate : [450, 280]\n        }\n      ]\n    },\n    {\n       name :  traffic ,\n       url :  data/flights-airport.csv ,\n       format : { type :  csv ,  parse :  auto },\n       transform : [\n        {\n           type :  aggregate ,  groupby : [ origin ],\n           summarize : [{ field :  count ,  ops : [ sum ],  as : [ flights ]}]\n        }\n      ]\n    },\n    {\n       name :  airports ,\n       url :  data/airports.csv ,\n       format : { type :  csv ,  parse :  auto },\n       transform : [\n        {\n           type :  lookup ,  on :  traffic ,  onKey :  origin ,\n           keys : [ iata ],  as : [ traffic ]\n        },\n        {\n           type :  filter ,\n           test :  datum.traffic != null \n        },\n        {\n           type :  geo ,  projection :  albersUsa ,\n           scale : 1200,  translate : [450, 280],\n           lon :  longitude ,  lat :  latitude \n        },\n        {\n           type :  filter ,\n           test :  datum.layout_x != null   datum.layout_y != null \n        },\n        {  type :  sort ,  by :  -traffic.flights  },\n        {  type :  voronoi ,  x :  layout_x ,  y :  layout_y  }\n      ]\n    },\n    {\n       name :  routes ,\n       url :  data/flights-airport.csv ,\n       format : { type :  csv ,  parse :  auto },\n       transform : [\n        {  type :  filter ,  test :  hover   hover.iata == datum.origin  },\n        {\n           type :  lookup ,  on :  airports ,  onKey :  iata ,\n           keys : [ origin ,  destination ],  as : [ _source ,  _target ]\n        },\n        {  type :  filter ,  test :  datum._source   datum._target  },\n        {  type :  linkpath  }\n      ]\n    }\n  ]\n}", 
            "title": "Geopath, aggregate, lookup, filter, sort, voronoi and linkpath"
        }, 
        {
            "location": "/developers/views-research/#further-research-on-vega-transforms", 
            "text": "https://github.com/vega/vega-dataflow-examples/  It is quite difficult to me to read the code as there is not enough documentation. I have included here the simplest example:  vega-dataflow.js  contains Dataflow, all transforms and vega s utilities.  !DOCTYPE HTML  html \n   head \n     title Dataflow CountPattern /title \n     script src= ../../build/vega-dataflow.js /script \n     style \n      body { margin: 10px; font-family: Helvetica Neue, Arial; font-size: 14px; }\n      textarea { width: 800px; height: 200px; }\n      pre { font-family: Monaco; font-size: 10px; }\n     /style \n   /head \n   body \n     textarea id= text /textarea br/ \n     input id= slider  type= range  min= 2  max= 10  value= 4 / \n    Frequency Threshold br/ \n     pre id= output /pre \n   /body  /html  df  is a Dataflow instance where we register (.add) functions and parameters - as below on line 36-38. The same with adding transforms - lines 40-44. We can pass different parameters to the transforms depending on requirements of each of them. Event handlers can added by using  .on  method of the Dataflow instance - lines 46-48.  var tx = vega.transforms; // all transforms \nvar out = document.querySelector('#output');\nvar area = document.querySelector('#text');\narea.value = [\n   Despite myriad tools for visualizing data, there remains a gap between the notational efficiency of high-level visualization systems and the expressiveness and accessibility of low-level graphical systems. \n].join('\\n\\n');\nvar stopwords =  (i|me|my|myself|we|us|our|ours|ourselves|you|your|yours|yourself|yourselves|he|him|his) ;\n\nvar get = vega.field('data');\n\nfunction readText(_, pulse) {\n  if (this.value) pulse.rem = this.value;\n  return pulse.source = pulse.add = [vega.ingest(area.value)];\n}\n\nfunction threshold(_) {\n  var freq = _.freq,\n      f = function(t) { return t.count  = freq; };\n  return (f.fields = ['count'], f);\n}\n\nfunction updatePage() {\n  out.innerText = c1.value.slice()\n    .sort(function(a,b) {\n      return (b.count - a.count)\n        || (b.text   a.text ? -1 : a.text   b.text ? 1 : 0);\n    })\n    .map(function(t) {\n      return t.text + ': ' + t.count;\n    })\n    .join('\\n');\n}\n\nvar df = new vega.Dataflow(), // create a new Dataflow instance\n// then add various operators into Dataflow instance:\n    ft = df.add(4), // word frequency threshold\n    ff = df.add(threshold, {freq:ft})\n    rt = df.add(readText),\n    // add a transforms (tx):\n    cp = df.add(tx.CountPattern, {field:get, case:'lower',\n      pattern:'[\\\\w\\']{2,}', stopwords:stopwords, pulse:rt}),\n    cc = df.add(tx.Collect, {pulse:cp}),\n    fc = df.add(tx.Filter, {expr:ff, pulse:cc}),\n    c1 = df.add(tx.Collect, {pulse:fc}),\n    up = df.add(updatePage, {pulse: c1});\ndf.on(df.events(area, 'keyup').debounce(250), rt)\n  .on(df.events('#slider', 'input'), ft, function(_, e) { return +e.target.value; })\n  .run();", 
            "title": "Further research on Vega transforms"
        }, 
        {
            "location": "/developers/views-research/#old-analysis", 
            "text": "There are number of transforms and they are located in different libraries. Basics are here  https://github.com/vega/vega-dataflow/tree/master/src/transforms  Generally, all data flow happens in the  vega-dataflow module . There are lots of complicated operations performed to data input and parameters. Some of transform functions are inherited from another functions/classes which makes difficult to separate them:  Filter function:  export default function Filter(params) {\n  Transform.call(this, fastmap(), params);\n}\n\nvar prototype = inherits(Filter, Transform);\n\n// more code for prototype \nand Transform is: import Operator from './Operator';\nimport {inherits} from 'vega-util';\n\n/**\n * Abstract class for operators that process data tuples.\n * Subclasses must provide a {@link transform} method for operator processing.\n * @constructor\n * @param {*} [init] - The initial value for this operator.\n * @param {object} [params] - The parameters for this operator.\n * @param {Operator} [source] - The operator from which to receive pulses.\n */\nexport default function Transform(init, params) {\n  Operator.call(this, init, null, params);\n}\n\nvar prototype = inherits(Transform, Operator);\n\n/**\n * Overrides {@link Operator.evaluate} for transform operators.\n * Marshalls parameter values and then invokes {@link transform}.\n * @param {Pulse} pulse - the current dataflow pulse.\n * @return {Pulse} The output pulse (or StopPropagation). A falsy return\n     value (including undefined) will let the input pulse pass through.\n */\nprototype.evaluate = function(pulse) {\n  var params = this.marshall(pulse.stamp),\n      out = this.transform(params, pulse);\n  params.clear();\n  return out;\n};\n\n/**\n * Process incoming pulses.\n * Subclasses should override this method to implement transforms.\n * @param {Parameters} _ - The operator parameter values.\n * @param {Pulse} pulse - The current dataflow pulse.\n * @return {Pulse} The output pulse (or StopPropagation). A falsy return\n *   value (including undefined) will let the input pulse pass through.\n */\nprototype.transform = function() {};  and as we can see Transform inherits from Operator and so on.  But some of the transform functions looks independent:  Getting cross product:  // filter is an optional  function for selectively including tuples in the cross product.\nfunction cross(input, a, b, filter) {\n  var data = [],\n      t = {},\n      n = input.length,\n      i = 0,\n      j, left;\n\n  for (; i n; ++i) {\n    t[a] = left = input[i];\n    for (j=0; j n; ++j) {\n      t[b] = input[j];\n      if (filter(t)) {\n        data.push(ingest(t));\n        t = {};\n        t[a] = left;\n      }\n    }\n  }\n\n  return data;\n}  Other transforms:   Linkpath  https://github.com/vega/vega-encode  Geo  https://github.com/vega/vega-geo  Hierarchical  https://github.com/vega/vega-hierarchy/tree/master/definitions  voronoi  https://github.com/vega/vega-voronoi  crossfilter  https://github.com/vega/vega-crossfilter", 
            "title": "Old analysis"
        }, 
        {
            "location": "/developers/views-research/#dp-pipelines-transforms", 
            "text": "DPP provides number of transforms that can be applied to a dataset. However, those transforms cannot be processed inside browsers as the library requires Python scripts to run.  Below is a copy-paste from DPP docs:", 
            "title": "DP Pipelines transforms"
        }, 
        {
            "location": "/developers/views-research/#concatenate", 
            "text": "Concatenates a number of streamed resources and converts them to a single resource.  Parameters :   sources  - Which resources to concatenate. Same semantics as  resources  in  stream_remote_resources .   If omitted, all resources in datapackage are concatenated.  Resources to concatenate must appear in consecutive order within the data-package.    target  - Target resource to hold the concatenated data. Should define at least the following properties:    name  - name of the resource   path  - path in the data-package for this file.   If omitted, the target resource will receive the name  concat  and will be saved at  data/concat.csv  in the datapackage.   fields  - Mapping of fields between the sources and the target, so that the keys are the  target  field names, and values are lists of  source  field names.   This mapping is used to create the target resources schema.  Note that the target field name is  always  assumed to be mapped to itself.  Example :  - run: concatenate\n  parameters: \n    target:\n      name: multi-year-report\n      path: data/multi-year-report.csv\n    sources: 'report-year-20[0-9]{2}'\n    fields:\n      activity: []\n      amount: ['2009_amount', 'Amount', 'AMOUNT [USD]', '$$$']      In this example we concatenate all resources that look like  report-year- year , and output them to the  multi-year-report  resource.  The output contains two fields:   activity  , which is called  activity  in all sources  amount , which has varying names in different resources (e.g.  Amount ,  2009_amount ,  amount  etc.)", 
            "title": "concatenate"
        }, 
        {
            "location": "/developers/views-research/#join", 
            "text": "Joins two streamed resources.   Joining  in our case means taking the  target  resource, and adding fields to each of its rows by looking up data in the  source  resource.   A special case for the join operation is when there is no target stream, and all unique rows from the source are used to create it. \nThis mode is called  deduplication  mode - The target resource will be created and  deduplicated rows from the source will be added to it.  Parameters :   source  - information regarding the  source  resource  name  - name of the resource  key  - One of  List of field names which should be used as the lookup key  String, which would be interpreted as a Python format string used to form the key (e.g.  { field_name_1 }:{field_name_2} )    delete  - delete from data-package after joining ( False  by default)  target  - Target resource to hold the joined data. Should define at least the following properties:  name  - as in  source  key  - as in  source , or  null  for creating the target resource and performing  deduplication .  fields  - mapping of fields from the source resource to the target resource. \n  Keys should be field names in the target resource.\n  Values can define two attributes:   name  - field name in the source (by default is the same as the target field name)    aggregate  - aggregation strategy (how to handle multiple  source  rows with the same key). Can take the following options:     sum  - summarise aggregated values. \n  For numeric values it s the arithmetic sum, for strings the concatenation of strings and for other types will error.    avg  - calculate the average of aggregated values.    For numeric values it s the arithmetic average and for other types will err.   max  - calculate the maximum of aggregated values.   For numeric values it s the arithmetic maximum, for strings the dictionary maximum and for other types will error.   min  - calculate the minimum of aggregated values.   For numeric values it s the arithmetic minimum, for strings the dictionary minimum and for other types will error.    first  - take the first value encountered    last  - take the last value encountered    count  - count the number of occurrences of a specific key\n  For this method, specifying  name  is not required. In case it is specified,  count  will count the number of non-null values for that source field.    set  - collect all distinct values of the aggregated field, unordered     array  - collect all values of the aggregated field, in order of appearance       any  - pick any value.    By default,  aggregate  takes the  any  value.    If neither  name  or  aggregate  need to be specified, the mapping can map to the empty object  {}  or to  null .\n-  full   - Boolean,\n  - If  True  (the default), failed lookups in the source will result in  null  values at the source.\n  - if  False , failed lookups in the source will result in dropping the row from the target.  Important: the  source  resource  must  appear before the  target  resource in the data-package.  Examples :  - run: join\n  parameters: \n    source:\n      name: world_population\n      key: [ country_code ]\n      delete: yes\n    target:\n      name: country_gdp_2015\n      key: [ CC ]\n    fields:\n      population:\n        name:  census_2015         \n    full: true  The above example aims to create a package containing the GDP and Population of each country in the world.  We have one resource ( world_population ) with data that looks like:     country_code  country_name  census_2000  census_2015      UK  United Kingdom  58857004  64715810           And another resource ( country_gdp_2015 ) with data that looks like:     CC  GDP (\u00a3m)  Net Debt (\u00a3m)      UK  1832318  1606600          The  join  command will match rows in both datasets based on the  country_code  /  CC  fields, and then copying the value in the  census_2015  field into a new  population  field.  The resulting data package will have the  world_population  resource removed and the  country_gdp_2015  resource looking like:     CC  GDP (\u00a3m)  Net Debt (\u00a3m)  population      UK  1832318  1606600  64715810           A more complex example:  - run: join\n  parameters: \n    source:\n      name: screen_actor_salaries\n      key:  {production} ({year}) \n    target:\n      name: mgm_movies\n      key:  {title} \n    fields:\n      num_actors:\n        aggregate: 'count'\n      average_salary:\n        name: salary\n        aggregate: 'avg'\n      total_salaries:\n        name: salary\n        aggregate: 'sum'\n    full: false  This example aims to analyse salaries for screen actors in the MGM studios.  Once more, we have one resource ( screen_actor_salaries ) with data that looks like:     year  production  actor  salary      2016  Vertigo 2  Mr. T  15000000    2016  Vertigo 2  Robert Downey Jr.  7000000    2015  The Fall - Resurrection  Jeniffer Lawrence  18000000    2015  Alf - The Return to Melmack  The Rock  12000000           And another resource ( mgm_movies ) with data that looks like:     title  director  producer      Vertigo 2 (2016)  Lindsay Lohan  Lee Ka Shing    iRobot - The Movie (2018)  Mr. T  Mr. T          The  join  command will match rows in both datasets based on the movie name and production year. Notice how we overcome incompatible fields by using different key patterns.  The resulting dataset could look like:     title  director  producer  num_actors  average_salary  total_salaries      Vertigo 2 (2016)  Lindsay Lohan  Lee Ka Shing  2  11000000  22000000", 
            "title": "join"
        }, 
        {
            "location": "/developers/views-research/#vega-dataflow-usage-for-dp-views", 
            "text": "Vega has quite a lot of data transform functions available, however, most of them require complicated JSON descriptor to use. Although we may implement them in the future, at the moment we could start with the most basic and essential ones:  List of transforms that we could use:   Aggregate  Filter  Formula (applies given formula to dataset)  Sample", 
            "title": "Vega Dataflow usage for DP views"
        }, 
        {
            "location": "/developers/views-research/#aggregate-example", 
            "text": "We have dataset with 4 fields - a, b, c and d. Lets apply different aggregation methods on them - count, sum, min and max:  const vegadataflow = require('./build/vega-dataflow.js');\n\nvar tx = vegadataflow.transforms,\n    changeset = vegadataflow.changeset;\n\nvar data = [\n {\n    a : 17.76,\n    b : 20.14,\n    c : 17.05,\n    d : 17.79\n },\n {\n    a : 19.19,\n    b : 21.29,\n    c : 19.19,\n    d : 19.92\n },\n {\n    a : 20.33,\n    b : 22.9,\n    c : 19.52,\n    d : 21.12\n },\n {\n    a : 20.15,\n    b : 20.72,\n    c : 19.04,\n    d : 19.31\n },\n {\n    a : 17.93,\n    b : 18.09,\n    c : 16.99,\n    d : 17.01\n }\n];\n\nvar a = vegadataflow.field('a'),\n    b = vegadataflow.field('b'),\n    c = vegadataflow.field('c'),\n    d = vegadataflow.field('d');\n\nvar df = new vegadataflow.Dataflow(),\n    col = df.add(tx.Collect),\n    agg = df.add(tx.Aggregate, {\n            fields: [a, b, c, d],\n            ops: ['count', 'sum', 'min', 'max'],\n            pulse: col\n          }),\n    out = df.add(tx.Collect, {pulse: agg});\n\ndf.pulse(col, changeset().insert(data)).run();\n\nconsole.dir(out.value);  Output: [ \n  {\n    _id: 7, \n    count_a: 5, \n    sum_b: 103.14, \n    min_c: 16.99, \n    max_d: 21.12 \n  }\n]", 
            "title": "Aggregate example"
        }, 
        {
            "location": "/developers/views-research/#filter-example", 
            "text": "Using the dataset from example above, lets filter values of field  a  that are not greater than 19:  const vegadataflow = require('./build/vega-dataflow.js');\n\nvar tx = vegadataflow.transforms,\n    changeset = vegadataflow.changeset;\n\nvar data = [\n {\n    a : 17.76,\n    b : 20.14,\n    c : 17.05,\n    d : 17.79\n },\n {\n    a : 19.19,\n    b : 21.29,\n    c : 19.19,\n    d : 19.92\n },\n {\n    a : 20.33,\n    b : 22.9,\n    c : 19.52,\n    d : 21.12\n },\n {\n    a : 20.15,\n    b : 20.72,\n    c : 19.04,\n    d : 19.31\n },\n {\n    a : 17.93,\n    b : 18.09,\n    c : 16.99,\n    d : 17.01\n }\n];\n\nvar a = vegadataflow.field('a');\n\nvar filter1 = vegadataflow.accessor(d =  { return d.a   19 }, ['a']);\n\nvar df = new vegadataflow.Dataflow(),\n    ex = df.add(null),\n    col = df.add(tx.Collect),\n    fil = df.add(tx.Filter, {expr: ex, pulse: col}),\n    out = df.add(tx.Collect, {pulse: fil});\n\ndf.pulse(col, changeset().insert(data));\ndf.update(ex, filter1).run();\n\nconsole.log(out.value);  Output: [ \n  { a: 19.19, b: 21.29, c: 19.19, d: 19.92, _id: 3 },\n  { a: 20.33, b: 22.9, c: 19.52, d: 21.12, _id: 4 },\n  { a: 20.15, b: 20.72, c: 19.04, d: 19.31, _id: 5 } \n]", 
            "title": "Filter example"
        }, 
        {
            "location": "/developers/views-research/#formula-example", 
            "text": "Using the same dataset, lets apply mapping on a field:  const vegadataflow = require('./build/vega-dataflow.js');\n\nvar tx = vegadataflow.transforms,\n    changeset = vegadataflow.changeset;\n\nvar data = [\n {\n    a : 17.76,\n    b : 20.14,\n    c : 17.05,\n    d : 17.79\n },\n {\n    a : 19.19,\n    b : 21.29,\n    c : 19.19,\n    d : 19.92\n },\n {\n    a : 20.33,\n    b : 22.9,\n    c : 19.52,\n    d : 21.12\n },\n {\n    a : 20.15,\n    b : 20.72,\n    c : 19.04,\n    d : 19.31\n },\n {\n    a : 17.93,\n    b : 18.09,\n    c : 16.99,\n    d : 17.01\n }\n];\n\n\nvar df = new vegadataflow.Dataflow(),\n    e = vegadataflow.field('e'),\n    f = vegadataflow.field('f'),\n    formula1 = vegadataflow.accessor(d =  { return d.a * 10; }, ['a']),\n    formula2 = vegadataflow.accessor(d =  { return d.b / 10; }, ['b']),\n    col = df.add(tx.Collect),\n    fa = df.add(tx.Formula, {expr: formula1, as: 'e', pulse: col}),\n    fb = df.add(tx.Formula, {expr: formula2, as: 'f', pulse: fa});\n\ndf.pulse(col, changeset().insert(data)).run();\n\nconsole.log(col.value.map(e));\nconsole.log(col.value.map(f));  Output: [ 177.60000000000002, 191.9, 203.29999999999998, 201.5, 179.3 ]\n[ 2.0140000000000002, 2.129, 2.29, 2.072, 1.809 ]", 
            "title": "Formula example"
        }, 
        {
            "location": "/developers/views-research/#sample-example", 
            "text": "Lets create a dataset with 100 rows and take a sample of 10 from it:  const vegadataflow = require('./build/vega-dataflow.js');\n\nvar tx = vegadataflow.transforms,\n    changeset = vegadataflow.changeset;\n\nvar n = 100,\n    sampleSize = 10,\n    data = Array(n),\n    i;\n\nfor(i=0; i n; i++) data[i] = {v:Math.random()};\n\nvar df = new vegadataflow.Dataflow(),\n    s = df.add(tx.Sample, {size: sampleSize});\n\ndf.pulse(s, changeset().insert(data)).run();\n\nconsole.log(s.value);  Output: [ \n  { v: 0.3332451883830292, _id: 69 },\n  { v: 0.2874480689159735, _id: 3 },\n  { v: 0.18009915754527817, _id: 41 },\n  { v: 0.10513776386462825, _id: 27 },\n  { v: 0.4972760501252764, _id: 35 },\n  { v: 0.757859721485594, _id: 67 },\n  { v: 0.248170225498199, _id: 64 },\n  { v: 0.431513510601889, _id: 28 },\n  { v: 0.07281378713091247, _id: 37 },\n  { v: 0.9543216903991236, _id: 33 } \n]", 
            "title": "Sample example"
        }, 
        {
            "location": "/developers/views-research/#suggestion-on-usage-from-datapackagejson", 
            "text": "Our current simple view layout:  {\n  name: 'sample',\n  resource: [0],\n  specType: 'simple',\n  spec: {\n    type: 'line',\n    group: 'a',\n    series: ['b', 'c']\n  }\n}  We could add  transform  property that would be a specification for transforms to be applied. Each transform would have slightly different properties:", 
            "title": "Suggestion on usage from datapackage.json"
        }, 
        {
            "location": "/developers/views-research/#aggregate", 
            "text": "{\n  ...\n  transform: {\n    type: 'aggregate',\n    fields: ['a', 'b'],\n    operations: ['sum', 'min']\n  },\n  ...\n} \nFor  aggregate  transform, a publisher should pass a field name and an operation to be applied. Operations should be one of  https://vega.github.io/vega/docs/transforms/aggregate/", 
            "title": "Aggregate"
        }, 
        {
            "location": "/developers/views-research/#filter", 
            "text": "{\n  ...\n  transform: {\n    type: 'filter',\n    expr: 'data.fieldName   10'\n  },\n  ...\n} \nFor  filter  type expression should evaluate to true or false so only truthy values will be kept.", 
            "title": "Filter"
        }, 
        {
            "location": "/developers/views-research/#formula", 
            "text": "{\n  ...\n  transform: {\n    type: 'formula',\n    expr: ['data.fieldName * 2', 'data.fieldName + 10'],\n    as: ['x', 'y']\n  },\n  ...\n}  For  formula  type, a field will be mapped with given expression and output will be stored in new fields that are specified in  as  property.", 
            "title": "Formula"
        }, 
        {
            "location": "/developers/views-research/#sample", 
            "text": "...\n  transform: {\n    type: 'sample',\n    size: 'some integer'\n  },\n  ...  In  sample  type, only size of a sample is needed.", 
            "title": "Sample"
        }, 
        {
            "location": "/developers/user-stories/", 
            "text": "User Stories\n\n\nDataHub is the place where \npeople\n can \nstore, share and publish\n their data, \ncollect, inspect and process\n it with \npowerful tools\n, and \ndiscover and use\n data shared by others. [order matters]\n\n\nPeople = data wranglers = those who use machines (e.g. code, command line tools) to work with their data rather than editing it by hand (as, for example, many analysts do in Excel). (Think people who use Python vs people who use Excel for data work)\n\n\n\n\nData is not chaotic and is in some sense neat\n\n\nCan present your data with various visualization tools (graphs, charts, tables etc.) \n\n\nEasy to publish\n\n\nSpecific data (power) tools and integrations\n\n\nCan validate your data before publishing\n\n\nData API\n\n\nData Conversion / Bundling: zip the data, provide sqlite\n\n\nGenerate a node package of your data\n\n\n(Versioning)\n\n\n\n\n\n\n\n\nTable of Contents\n\n\n\n\n\n\nUser Stories\n\n\nPersonas\n\n\nStories\n\n\n1. Get Started\n\n\n1. Sign in / Sign up [DONE]\n\n\nSign up via github (and/or google) [DONE]\n\n\nNext Step after Sign Up\n\n\nInvite User to Join Platform\n\n\n\n\n\n\n2. Publish Data Packages\n\n\nPublish with a Client [DONE]\n\n\nConfigure Client [DONE]\n\n\n\n\n\n\nUpdate a Data Package [DONE]\n\n\nDelete a Data Package\n\n\nPurge a Data Package\n\n\nValidate Data in Data Package\n\n\nValidate in CLI [DONE]\n\n\nValidate on Server\n\n\n\n\n\n\nCache Data Package Resource data (on the server)\n\n\nPublish with Web Interface\n\n\nUndelete data package\n\n\nRender (views) in data package in CLI before upload\n\n\n\n\n\n\n3. Find and View Data Packages\n\n\nView a Data Package Online [DONE]\n\n\n(Pre)View a not-yet-published Data Package Online\n\n\nSee How Much a Data Package is Used (Downloaded) {2d}\n\n\nBrowse Data Packages [DONE]\n\n\nSearch for Data Packages [DONE]\n\n\nDownload Data Package Descriptor\n\n\nDownload Data Package in One File (e.g. zip)\n\n\n\n\n\n\n4. Get a Data Package (locally)\n\n\nUse DataPackage in Node (package auto-generated)\n\n\nImport DataPackage into R [DONE?]\n\n\nImport DataPackage into Pandas [DONE?]\n\n\nSQL / SQLite database\n\n\nSee changes between versions\n\n\nLow Priority\n\n\n\n\n\n\n5. Versioning and Changes in Data Packages\n\n\nExplicit Versioning - Publisher\n\n\nExplicit Versioning - Consumer\n\n\nKnow when a package has changed re caching\n\n\n\n\n\n\nRevisioning - Implicit Versioning\n\n\nChange Notifications\n\n\n\n\n\n\n6. Publishers\n\n\nCreate a New Publisher\n\n\nFind a Publisher (and users?)\n\n\nView a Publisher Profile\n\n\nSearch among publishers packages\n\n\nRegistered Users Profile and packages\n\n\n\n\n\n\nPublisher and User Leaderboard\n\n\nManage Publisher\n\n\nCreate and Edit Profile\n\n\nAdd and Manage Members\n\n\n\n\n\n\n\n\n\n\n7. Web Hooks and Extensions\n\n\n8. Administer Site\n\n\nConfigure Site\n\n\nSee usage metrics\n\n\nPricing and Billing\n\n\n\n\n\n\nPrivate Data Packages\n\n\nSell My Data through your site\n\n\n\n\n\n\n\n\n\n\nPersonas\n\n\n\n\n[Geek] Publisher\n. Knows how to use a command line or other automated tooling. Wants to publish their data package in order to satisfy their teams requirements to publish data.\n\n\nNon-Geek Publisher. Tbc \u2026\n\n\n\n\n\n\nConsumer\n: A person or organization looking to use data packages (or data in general)\n\n\nData Analyst\n\n\nCoder (of data driven applications)\n\n\n\u2026\n\n\n\n\n\n\nAdmin\n: A person or organization who runs an instance of a DataHub\n\n\n\n\nStories\n\n\n1. Get Started\n\n\n1. Sign in / Sign up [DONE]\n\n\nAs a Geek Publisher I want to sign up for an account so that I can publish my data package to the registry and to have a publisher account to publish my data package under.\n\n\nGenerally want this to be as minimal, easy and quick as possible\n\n\n\n\nSign in with a Google account\n\n\n(?) what about up other social accounts?\n\n\n\n\n\n\nEssential profile information (after sign in we prompt for this)\n\n\nemail address\n\n\nName\n\n\n(?) - future. Credit card details for payment - can we integrate with payment system (?)\n\n\n\n\n\n\nThey need to choose a user name which is url friendly unique human readable name for our app. Can be used in sign in and in many other places.\n\n\nWHY? Where would we need this? For url on site \n for publisher\n\n\nSame as publisher names (needed for URLs): [a-z-_.]\n\n\nExplain: they cannot change this later e.g. \nChoose wisely! Once you set this it cannot be changed later!\n\n\n\n\n\n\nSend the user an email confirming their account is set up and suggesting next steps\n\n\n\n\nAutomatically:\n\n\n\n\nAuto-create a publisher for them\n\n\nSame name as their user name but a publisher\n\n\nThat way they can start publishing straight away \u2026\n\n\n\n\n\n\n\n\nTODO: (??) should we do *all\n of this via the command line client (a la npmjs) *\n\n\nSign up via github (and/or google) [DONE]\n\n\nAs a Visitor I want to sign up via github or google so that I don\u2019t have to enter lots of information and remember my password for yet another website\n\n\n\n\nHow do we deal with username conflicts? What about publisher name conflicts?\n\n\nThis does not arise in simple username system because we have only pool of usernames\n\n\n\n\n\n\n\n\nNext Step after Sign Up\n\n\nAs a Geek Publisher I want to know what do next after signing up so that I can get going quickly.\n\n\nThings to do:\n\n\n\n\nEdit your profile\n\n\nDownload a client / Configure your client (if you have one already)\n\n\nInstructions on getting relevant auth credentials\n\n\nNote they will \nneed\n to have set a username / password in their profile\n\n\n\n\n\n\nJoin a Publisher (understand what a publisher is!)\n\n\n\n\nInvite User to Join Platform\n\n\nAs an Admin (or existing Registered User?) I want to invite someone to join the platform so that they can start contributing or using data\n\n\n\n\nGet an invitation email with a sign up link\n\n\nSome commonality with Publisher invite member below\n\n\n\n\n2. Publish Data Packages\n\n\nPublish with a Client [DONE]\n\n\nAs a Geek Publisher I want to import (publish) my data package into the registry so my data has a permanent online home so that I and others can have access\n\n\nOn command line looks like:\n\n\n$ cd my/data/package\n$ data publish\n\n\n \u2026 working \u2026\n\n\n\n SUCCESS\n\n\n\nNotes\n\n\n\n\nPermissions: must be a member of the Publisher\n\n\nInternally: DataPackageCreate or DataPackageUpdate capability\n\n\n\n\n\n\nHandle conflicts: if data package already exists, return 409. Client instructions should be already exists and use \nforce\n or similar to overwrite\n\n\nAPI endpoint behind the scenes: POST {api}/package/\n\n\nTODO: private data packages\n\n\nAnd payment!\n\n\n\n\n\n\n\n\nConfigure Client [DONE]\n\n\nAs a Geek Publisher I want to configure my client so I can start publishing data packages.\n\n\nLocally in $HOME store store something like:\n\n\n.dpm/credentials # stores your API key and user name\n\n.dpm/config      # stores info like your default publisher\n\n\n\nUpdate a Data Package [DONE]\n\n\nAs a Geek Publisher I want to use a publish command to update a data package that is already in the registry so it appears there\n\n\n\n\nOld version will be lost (!)\n\n\n\n\nDelete a Data Package\n\n\nAs a Geek Publisher I want to unpublish (delete) a data package so it is no longer visible to anyone\n\n\nPurge a Data Package\n\n\nAs a Geek Publisher I want to permanently delete (purge) a data package so that it no longer takes up storage space\n\n\nValidate Data in Data Package\n\n\nValidate in CLI [DONE]\n\n\nAs a Publisher [owner/member] I want to validate the data I am about to publish to the registry so that I publish \u201cgood\u201d data and know that I am doing so and do not have to manually check that the published data looks ok (e.g. rendering charts properly) (and if wrong I have to re-upload)\n\n\ndata datavalidate [file-path]\n\n\n\n\n\n[file-path] - run this against a given file. Look in the resources to see if this file is there and if so use the schema. Otherwise just do goodtables table \u2026\n\n\nIf no file provided run validate against each resource in turn in the datapackage\n\n\n\n\n\n\nOutput to stdout.\n\n\nDefault: human-readable - nice version of output from goodtables.\n\n\nOption for JSON e.g. \njson to put machine readable output\n\n\ncheck goodtables command line tool and follow if possible. Can probably reuse code\n\n\n\n\n\n\n\n\n\n\nAuto-run this before publish unless explicit suppression (e.g. \nskip-datavalidate)\n\n\nUse goodtables (?)\n\n\n\n\nValidate on Server\n\n\nAs a Publisher [owner] i want my data to be validated when I publish it so that I know immediately if I have accidentally \u201cbroken\u201d my data or have bugs and can take action to correct\n\n\nAs a Consumer I want to know that the data I am downloading is \u201cgood\u201d and can be relied on so that I don\u2019t have to check it myself or run into annoying bugs later on\n\n\n\n\nImplies showing something in the UI e.g. \u201cData Valid\u201d (like build passing)\n\n\n\n\nImplementation notes to self\n\n\n\n\nNeed a new table to store results of validation and a concept of a \u201crun\u201d\n\n\nStore details of the run [e.g. time to complete, ]\n\n\n\n\n\n\nHow to automate doing validation (using goodtables we assume) - do we reuse a separate service (goodtables.io in some way) or run ourselves in a process like ECS ???\n\n\nDisplay this in frontend\n\n\n\n\nCache Data Package Resource data (on the server)\n\n\nAs a Publisher I want to publish a data package where its resource data is stored on my servers but the registry caches a copy of that data so that if my data is lost or gets broken I still have a copy people can use\n\n\nAs a Consumer I want to be able to get the data for a data package even if the original data has been moved or removed so that I can still use is and my app or analysis keeps working\n\n\n\n\nTODO: what does this mean for the UI or command line tools. How does the CLI know about this, how does it use it?\n\n\n\n\nPublish with Web Interface\n\n\nAs a Publisher I want to publish a data package in the UI so that it is available and published\n\n\n\n\nPublish =\n they already have datapackage.json and all the data. They just want to be able to upload and store this.\n\n\n\n\nAs a Publisher I want to create a data package in the UI so that it is available and published\n\n\n\n\nCreate =\n no datapackage.json - just data files. Need to add key descriptors information, upload data files and have schemas created etc etc.\n\n\n\n\nUndelete data package\n\n\n[cli] As a Publisher I want to be able to restore the deleted data package via cli, so that it is back visible and available to view, download (and searchable)\n\n\ndpmpy undelete\n\n\n\n[webui] As a Publisher i want to undelete the deleted data packages, so that the deleted data packages is now visible again.\n\n\nRender (views) in data package in CLI before upload\n\n\nAs a Publisher, I want to be able to preview the views (graphs and table (?)) of the current data package using cli prior to publishing so that I can refine the json declarations of datapackage view section to achieve a great looking result.\n\n\n3. Find and View Data Packages\n\n\nView a Data Package Online [DONE]\n\n\nEPIC: As a Consumer I want to view a data package online so I can get a sense of whether this is the dataset I want\n\n\n\n\nObsess here about \u201cwhether this is the dataset I want\u201d\n\n\n*Publishers want this too \u2026 *\n\n\nAlso important for SEO if we have good info here\n\n\n\n\nFeatures\n\n\n\n\nVisualize data in charts - gives one an immediate sense of what this is\n\n\nOne graph section at top of page after README excerpt\n\n\nOne graph for each entry in the \u201cviews\u201d\n\n\n\n\n\n\nInteractive table - allows me to see what is in the table\n\n\nOne table for each resource\n\n\n\n\n\n\n\n\nThis user story can be viewed from two perspectives:\n\n\n\n\nFrom a publisher point of view\n\n\nFrom a consumer point of view\n\n\n\n\nAs a \npublisher\n i want to show the world how my published data is so that it immediately catches consumer\u2019s attention (and so I know it looks right - e.g. graph is ok)\n\n\nAs a \nconsumer\n i want to view the data package so that i can get a sense of whether i want this dataset or not.\n\n\nAcceptance criteria - what does done mean!\n\n\n\n\nA table for each resource\n\n\nSimple graph spec works =\n converts to plotly\n\n\nMultiple time series\n\n\n\n\n\n\nPlotly spec graphs work\n\n\nAll core graphs work (not sure how to check every one but representative ones)\n\n\nRecline graphs specs (are handled - temporary basis)\n\n\nLoading spinners whilst data is loading so users know what is happening\n\n\n\n\nBonus:\n\n\n\n\nComplex examples e.g. time series with a log scale \u2026 (e.g. hard drive data \u2026)\n\n\n\n\nFeatures: \nDP view status\n\n\n\n\nDifferent options to view data as graph.\n\n\nRecline\n\n\nVega-lite\n\n\nVega\n\n\n[Plotly]\n\n\n\n\n\n\nGeneral Functionality\n\n\nMultiple views [wrongly done. We iterate over resource not views]\n\n\nTable as a view\n\n\n\n\n\n\nInteractive table so that consumer can do\n\n\nFilter\n\n\nJoin\n\n\n\n\n\n\n\n\n(Pre)View a not-yet-published Data Package Online\n\n\nAs a (potential) Publisher I want to preview a datapackage I have prepared so that I can check it works and share the results (if there is something wrong with others)\n\n\n\n\nBe able to supply a URL to my datapackage (e.g. on github) and have it previewed as it would look on DPR\n\n\nBe able to upload a datapackage and have it previewed\n\n\n\n\nRufus: this was a very common use case for me (and others) when using data.okfn.org. Possibly less relevant if the command line tool can do previewing but still relevant IMO (some people may not have command line tool, and it is useful to be able to share a link e.g. when doing core datasets curation and there is something wrong with a datapackage).\n\n\nRufus: also need for an online validation tool\n\n\nSee How Much a Data Package is Used (Downloaded) {2d}\n\n\nAs a Consumer i want to see how much the data has been downloaded so that i can choose most popular (=\n probably most reliable and complete) in the case when there are several alternatives for my usecase (maybe from different publishers)\n\n\nBrowse Data Packages [DONE]\n\n\nAs a potential Publisher, unaware of datapackages, I want to see real examples of published packages (with the contents datapackage.json), so that I can understand how useful and simple is the datapackage format and the registry itself.\n\n\nAs a Consumer I want to see some example data packages quickly so I get a sense of what is on this site and if it is useful to look further\n\n\n\n\nBrowse based on what properties? Most recent, most downloaded?\n\n\nMost downloaded\n\n\nStart with: we could just go with core data packages\n\n\n\n\n\n\n\n\nSearch for Data Packages [DONE]\n\n\nAs a Consumer I want to search data packages so that I can find the ones I want\n\n\n\n\nEssential question: what is it you want?\n\n\nRufus: in my view generic search is actually \nnot\n important to start with. People do not want to randomly search. More useful is to go via a publisher at the beginning.\n\n\n\n\n\n\nSearch results should provide enough information to help a user decide whether to dig further e.g. title, short description\n\n\nFor future when we have it: [number of downloads], stars etc\n\n\n\n\n\n\n\n\nMinimum viable search (based on implementation questions)\n\n\n\n\nFilter by publisher\n\n\nFree text search against title\n\n\nDescription could be added if we start doing actual scoring as easy to add additional fields\n\n\n\n\n\n\nScoring would be nice but not essential\n\n\n\n\n\n\n\n\nImplementation questions:\n\n\n\n\nSearch:\n\n\nShould search perform ranking (that requires scoring support)\n\n\nFree text queries should search against which fields (with what weighting)?\n\n\n\n\n\n\nFiltering: On what individual properties of the data package should be able to filter?\n\n\nThemes and profiles:\n\n\nSearching for a given profile: not possible atm.\n\n\nThemes: Should we tag data packages by themes like finance, education and let user find data package by that?\n        * Maybe but not now - maybe in the future\n\n\n\n\n\n\nIf we follow the go via a publisher at the beginning then should we list the most popular publisher on the home page of user[logged-in/ not logged in]?\n\n\nIf most popular publisher then by what mesaure?\n\n\nSort by Most published?\n\n\nSort by Most followers?\n\n\nSort by most downloads?\n\n\nOr all show top5 in each facet?\n\n\n\n\n\n\n\n\n\n\n\n\nSub user stories:\n\n\n\n\n[WONTFIX?] As a Consumer i want to find the data packages by profile (ex: spending) so that I can find the kind of data I want quickly and easily and in one big list\n\n\nAs a Consumer i want to search based on description of data package, so that I can find package which related to some key words\n\n\n\n\nDownload Data Package Descriptor\n\n\nAs a Consumer I want to download the data package descriptor (datapackage.json) on its own so that \u2026\n\n\n*Rufus: I can\u2019t understand why anyone would want to do this *\n\n\nDownload Data Package in One File (e.g. zip)\n\n\nAs a Consumer I want to download the data package in one file so that I don\u2019t have to download descriptor and each resource by hand\n\n\nOnly useful if no cli tool and no install command\n\n\n4. Get a Data Package (locally)\n\n\nLet\u2019s move discussion to the github: *\nhttps://github.com/frictionlessdata/dpm-py/issues/30\n\n\nTODO add these details from the requirement doc\n\n\n\n\nLocal \u201cData Package\u201d cache storage (\n.datapackages\n or similar)\n\n\nStores copies of packages from Registry\n\n\nStores new Data Packages the user has created\n\n\nThis\n \nRuby lib\n \nimplements something similar\n\n\n\n\n\n\n\n\nUse DataPackage in Node (package auto-generated)\n\n\nAs a NodeJS developer I want to use data package as a node lib in my project so that I can depend on it using my normal dependency framework\n\n\n\n\nSee this \nreal-world example\n of this request for country-list\n\n\n=\n auto-building node package and publishing to npm (not that hard to do \u2026)\n\n\nConvert CSV data to json (that\u2019s what you probably want from node?)\n\n\nGenerate package.json\n\n\nPush to npm (register the dataset users)\n\n\nRufus: My guess here is that to implement here we want something a bit like github integrations \u2013 specific additional hooks which also get some configuration (or do it like travis - github integration plus a tiny config file - in our case rather than a .travis.yml we have a .node.yml or whatever)\n\n\n\n\n\n\nIs it configurable for user that enable to push to npm or not?\n\n\nYes. Since we need to push to a specific npm user (for each publisher) this will need to be configured (along with authorization - where does that go?)\n\n\n\n\n\n\nIs this something done for \nall\n data packages or does user need to turn something on? Probably want them to turn this on \u2026\n\n\n\n\nQuestions:\n\n\n\n\nFrom where we should push the data package to npm repo.\n\n\nIs it from dpmpy or from server? Obviously from a server - this needs to be automated. But you can use dpmpy if you want (but I\u2019m not sure we do want to \u2026)\n\n\n\n\n\n\nWhat to do with multiple resources? Ans: include all resources\n\n\nDo we include datapackage.json into the node package? Yes, include it so they get all the original metadata.\n\n\n\n\nGeneric version is:\n\n\nAs a Web Developer I want to download a DataPackage (like currency codes or country names) so that I can use it in the web service I am building [\n]\n\n\nImport DataPackage into R [DONE?]\n\n\nAs a Consumer [R user] I want to load a Data Package from R so that I can immediately start playing with it\n\n\n\n\nShould we try and publish to CRAN?\n\n\nProbably not? Why? think it can be quite painful getting permission to publish to CRAN and very easy to load from the registry\n\n\nOn the CRAN website I can\nt find a way to automate publishing. It seems possible by filling web-form, but to know the status we have to wait and parse email.\n\n\n\n\n\n\n\n\n\n\nUsing this library: \nhttps://github.com/ropenscilabs/datapkg\n\n\nWhere can i know about this?\n\n\nOn each data package view page \u2026\n\n\n\n\n\n\n\n\nGeneric version:\n\n\nAs a Data Analyst I want to download a data package, so that I can study it and wrangle with it to infer new data or generate new insights.\n\n\nAs a Data Analyst, I want to update previously downloaded data package, so that I can work with the most recent data.\n\n\nImport DataPackage into Pandas [DONE?]\n\n\nTODO - like R\n\n\nSQL / SQLite database\n\n\nAs a Consumer I want to download a DataPackage\u2019s data one coherent SQLite database so that I can get it easily in one form\n\n\nQuestion:\n\n\n\n\nWhy does we need to store datapackage data in sqlite. Is not it better to store in file structure?\n\n\n\n\nWe can store the datapackage like this way:\n\n\n~/.datapackage/\npublisher\n/\npackage\n/\nversion\n/*\n\n\n\nThis is the way maven/gradle/ivy cache jar locally.\n\n\nSee changes between versions\n\n\nAs a Data Analyst I want to compare different versions of some datapackage locally, so that I can see schema changes clearly and adjust my analytics code to the desired schema version.\n\n\nLow Priority\n\n\nAs a Web Developer of multiple projects, I want to be able to install multiple versions of the same datapackage separately so that all my projects could be developed independently and deployed locally. (virtualenv-like)\n\n\nAs a Developer I want to list all DataPackages requirements for my project in the file and pin the exact versions of any DataPackage that my project depends on so that the project can be deterministically deployed locally and won\u2019t break because of the DataPackage schema changes. (requirements.txt-like)\n\n\n5. Versioning and Changes in Data Packages\n\n\nWhen we talk about versioning we can mean two things:\n\n\n\n\nExplicit versioning: this is like the versioning Of releases \u201cv1.0\u201d etc. This is conscious and explicit. Main purpose:\n\n\nto support other systems depending on this one (they want the data at a known stable state)\n\n\neasy access to major staging points in the evolution (e.g. i want to see how things were at v1)\n\n\n\n\n\n\nImplicit versioning or \u201crevisioning\u201d: this is like the commits in git or the autosave of a word or google doc. It happens frequently, either with minimum effort or even automatically. Main purpose:\n\n\nUndelete and recovery (you save a every point and can recover if you accidentally write or delete something)\n\n\nCollaboration and merging of changes (in revision control)\n\n\nActivity logging\n\n\n\n\n\n\n\n\nExplicit Versioning - Publisher\n\n\nAs a Publisher I want to tag a version of my data on the command line so that \u2026 [see so that\u2019s below]\n\n\ndpmpy tag {tag-name}\n\n\n=\n tag current \u201clatest\u201d on the server as {tag-name}\n\n\n\n\nDo we restrict {tag-name} to semver? I don\u2019t think so atm.\n\n\nAs a {Publisher} I want to tag datapackage to create a snapshot of data on the registry server, so that consumers can refer to it\n\n\nAs a {Publisher} I want to be warned that a tag exists, when I try to overwrite it, so that I don\u2019t accidentally overwrite stable tagged data, which is relied on by consumers.\n\n\nAs a {Publisher} I want to be able to overwrite the previously tagged datapackage, so that I can fix it if I mess up.\n\n\nThe versioning here happens server side\n\n\nIs this confusing for users? I.e. they are doing something local.\n\n\n\n\n\n\n\n\nBackground \u201cso that\u201d user story epics:\n\n\n\n\nAs a {Publisher} I want to version my Data Package and keep multiple versions around including older versions so that I do not break consumer systems when I change my Data Package (whether schema or data) [It is not just the publisher who wants this, it is a consumer - see below]\n\n\nAs a {Publisher} I want to be able to get access to a previous version I tagged so that I can return to it and review it (and use it)\n\n\nso that i can recover old data if i delete it myself or compare how things changed over time\n\n\n\n\n\n\n\n\nExplicit Versioning - Consumer\n\n\nAs a {Consumer} (of a Data Package) I want to know full details when and how the data package schema has changed and when so that I can adjust my scripts to handle it.\n\n\nImportant info to know for each schema change:\n\n\n\n\ntime when published\n\n\nfor any \nchanged\n field - name, what was changed (type, format, \u2026?),\n    \n +maybe everything else that was not changed (full field descriptor)\n\n\nfor any \ndeleted\n field - name,\n    \n +maybe everything else (full field descriptor)\n\n\nfor any \nadded\n field - all data (full field descriptor)\n\n\n\n\n*A change in schema would correspond to a major version change in software (see \nhttp://semver.org/\n)\n\n\nConcerns about explicit versioning\n: we all have experience with consuming data from e.g. government publishers where the publishers change the data schema breaking client code. I am constatnly looking for a policy/mechanism to guide publishers to develop stable schema versioning for the data they produce, and help consumers to get some stability guarantees.\n\n\nAutomated versioning / automated tracking\n: Explicit versioning relies on the publisher, and humans can forget or not care enough about others. So to help consumers my suggestion would be to always track schema changes of uploaded packages on the server, and allow users to review those changes on the website. (We might even want to implement auto-tagging or not allowing users to upload a package with the same version but a different schema without forcing)\n\n\nAs a {Consumer} I want to get a sense how outdated is the datapackage, that I have downloaded before, so that I can decide if I should update or not.\n\n\n\n\nI want to preview a DataPackage changelog (list of all available versions/tags with brief info) online, sorted by creation time, so that I can get a sense how data or schema has changed since some time in the past. Important brief info:\n\n\nTime when published\n\n\nHow many rows added/deleted for each resource data\n\n\nWhat fields(column names) changed, added or deleted for each resource.\n\n\n\n\n\n\n\n\nAs a {Consumer} I want to view a Datapackage at a particular version online, so that I can present/discuss the particular data timeslice of interest with other people.\n\n\nAs a {Consumer} I want to download a Data package at a particular version so that I know it is compatible with my scripts and system\n\n\n\n\nOnline: I want to pick the version I want from the list, and download it (as zip for ex.)\n\n\nCLI: I want to specify tag or version when using the \ninstall\n command.\n\n\n\n\nKnow when a package has changed re caching\n\n\nExcerpted from: \nhttps://github.com/okfn/data.okfn.org-new/issues/7\n\n\nFrom @trickvi on June 20, 2013 12:37\n\n\nI would like to be able to use data.okfn.org as an intermediary between my software and the data packages it uses and be able to quickly check whether there\ns a new version available of the data (e.g. if I\nve cached the package on a local machine).\n\n\nThere are ways to do it with the current setup:\n\n\n\n\nDownload the datapackage.json descriptor file, parse it and get the version there and check it against my local version. Problems:\n\n\nThis solution relies on humans and that they update their version but there might not be any consistency in it since the data package standard describes the version attribute as: \na version string conforming to the Semantic Versioning requirement\n\n\nI have to fetch the whole datapackage.json (it\ns not big I know but why download all that extra data I might not even want)\n\n\nGo around data.okfn.org and look directly at the github repository. Problems:\n\n\nI have to find out where the repo is, use git and do a lot of extra stuff (I don\nt care how the data packages are stored, I just want a simple interface to fetch them)\n\n\nWhat would be the point of data.okfn.org/data? In my mind it collects data packages and provides a consistent interface to get the data packages irrespective of how its stored.\n\n\n\n\nI propose data.okfn.org provides an internal system to allow users to quickly check whether a new version might be released. This does not have to be an API. We could leverage HTTP\ns caching mechanism using an ETag header that would contain some hash value. This hash value can e.g. be the the sha value of heads ref objects served via the Github API:\n\n\nhttps://api.github.com/repos/datasets/cpi/git/refs/heads/master\n\n\n\nSoftware that works with data packages could then implement a caching strategy and just send a request with an If-None-Match header along with a GET request for datapackage.json to either get a new version of the descriptor (and look at the version in that file) or just serve the data from its cache.\n\n\nCopied from original issue: frictionlessdata/ideas#51\n\n\nRevisioning - Implicit Versioning\n\n\n\u2026\n\n\nChange Notifications\n\n\nAs a Consumer I want to be notified of changes to a package i care about so that I can check out what has changed and take action (like downloading the updated data)\n\n\nAs a Consumer I want to see how active the site is to see if I should get involved\n\n\n6. Publishers\n\n\nCreate a New Publisher\n\n\nTODO\n\n\nFind a Publisher (and users?)\n\n\nAs a Consumer I want to browse and find publishers so that I can find interesting publishers and their packages (so that I can use them)\n\n\nView a Publisher Profile\n\n\nview data packages associated to a publisher or user\n\n\nImplementation details: \nhttps://hackmd.io/MwNgrAZmCMAcBMBaYB2eAWR72woghmLNIrAEb4AME+08s6VQA===\n\n\nAs a Consumer I want to see a publisher\u2019s profile so that I can discover their packages and get a sense of how active and good they are\n\n\nAs a Publisher I want to have a profile with a list of my data packages so that:\n\n\n\n\nOthers can find my data packages quickly and easily\n\n\nCan see how many data packages i have\n\n\nI can find a data package i want to look at quickly [they can discover their own data]\n\n\nI can find the link for a data package to send to someone else\n\n\nPeople want to share what they have done. This is probably the number one way the site gets prominence at the start (along with simple google traffic)\n\n\n\n\n\n\nso that I can check that members do not abuse their rights to publish and only publish topical data packages.\n\n\n\n\nAs a Consumer I want to view a publisher\u2019s profile so that I can see who is behind a particular package or to see what other packages they produce [navigate up from a package page] [so that: i can trust on his published data packages to reuse.]\n\n\nDetails\n\n\n\n\nProfile =\n\n\nFull name / title e.g. \u201cWorld Bank\u201d, identifier e.g. world-bank\n\n\npicture, short description text (if we have this - we don\u2019t atm)\n\n\n(esp important to know if this is the world bank or not)\n\n\n\n\n\n\nTotal number of data packages\n\n\nList of data packages\n\n\nView by most recently created (updated?)\n\n\nFor each DataPackage want to see: title, number of resources (?), first 200 character of description, license (see data.okfn.org/data/ for example)\n\n\nDo we limit / paginate this list? No, not for the moment\n\n\n\n\n\n\n[wontfix atm] Activity - this means data packages published, updated\n\n\n[wontfix atm] Quality \u2026 - we don\u2019t have anything on this\n\n\n[wontfix atm] List of users\n\n\n\n\n\n\nWhat are the permissions here?\n\n\nDo we show private data packages? No\n\n\nDo we show them when \u201cowner\u201d viewing or sysadmin? Yes (but flag as \u201cprivate\u201d)\n\n\n\n\n\n\n\n\n\n\nWhat data packages to show? All the packages you own.\n\n\nWhat about pinning? No support for this atm.\n\n\n\n\n\n\n\n\nSearch among publishers packages\n\n\nAs a Consumer i want to search among all data packages owned by a publisher so that I can easily find one data package amongst all the data packages by this publisher.\n\n\nRegistered Users Profile and packages\n\n\nAs a Consumer i want to see the profile and activity of a user so that \u2026\n\n\nAs a Registered User I want to see the data packages i am associated with \nso that\n [like publisher]\n\n\nPublisher and User Leaderboard\n\n\nAs a ??? I want to see who are the top publihers and users so that I can emulate them or ???\n\n\nManage Publisher\n\n\nCreate and Edit Profile\n\n\nAs {Owner \n} I want to edit my profile so that it is updated with new information\n\n\nAdd and Manage Members\n\n\nAs an {Owner of a Publisher in the Registry} I want to invite an existing user to become a member of my publisher\n\n\n\n\nAuto lookup by user name (show username and fullname) - standard as per all sites\n\n\nUser gets a notification on their dashboard + email with link to accept invite\n\n\nIf invite is accepted notify the publisher (?) - actually do not do this.\n\n\n\n\nAs an {Owner of a Publisher in the Registry} I want to invite someone using their email to sign up and become a member of my Publisher so that they are authorized to publish data packages under my Publisher.\n\n\nAs an {Publisher Owner} I want to remove someone from membership in my publisher so they no longer have ability to publish or modify my data packages\n\n\nAs a {Publisher Owner} I want to view all the people in my organization and what roles they have so that I can change these if I want\n\n\nAs a {Publisher Owner} I want to make a user an \u201cowner\u201d so they have full control\n\n\nAs a {Publisher Owner} I want to remove a user as an \u201cowner\u201d so they are just a member and no longer have full control\n\n\n7. Web Hooks and Extensions\n\n\nTODO: how do people build value added services around the system (and push back over the API etc \u2026) - OAuth etc\n\n\n8. Administer Site\n\n\nConfigure Site\n\n\nAs the Admin I want to set key configuration parameters for my site deployment so that I can change key information like the site title\n\n\n\n\nMain config database is the one thing we might need\n\n\n\n\nSee usage metrics\n\n\nAs an Admin I want to see key metrics about usage such as users, API usage, downloads etc so that I know how things are going\n\n\n\n\nTotal users are signed up, how many signed up in last week / month etc\n\n\nTotal publishers \u2026\n\n\nUsers per publisher distribution (?)\n\n\n\n\n\n\nAPI usage\n\n\nDownloads\n\n\nBilling: revenue in relevant periods\n\n\nCosts: how much are we spending on storage\n\n\n\n\nPricing and Billing\n\n\nAs an Admin I want to have a pricing plan and billing system so that I can charge users and make my platform sustainable\n\n\nAs a Publisher I want to know if this site has a pricing plan and what the prices are so that I can work out what this will cost me in the future and have a sense that these guys are sustainable (\u2018free forever\u2019 does not work very well)\n\n\nAs a Publisher I want to sign up for a given pricing plan so that I am entitled to what it allows (e.g. private stuff \u2026)\n\n\nPrivate Data Packages\n\n\ncf npmjs.com\n\n\nAs a Publisher I want to have private data packages that I can share just with my team\n\n\nSell My Data through your site\n\n\nEPIC: As a Publisher i want to sell my data through your site so that I make money and am able to sustain my publishing and my life \u2026", 
            "title": "User Stories"
        }, 
        {
            "location": "/developers/user-stories/#user-stories", 
            "text": "DataHub is the place where  people  can  store, share and publish  their data,  collect, inspect and process  it with  powerful tools , and  discover and use  data shared by others. [order matters]  People = data wranglers = those who use machines (e.g. code, command line tools) to work with their data rather than editing it by hand (as, for example, many analysts do in Excel). (Think people who use Python vs people who use Excel for data work)   Data is not chaotic and is in some sense neat  Can present your data with various visualization tools (graphs, charts, tables etc.)   Easy to publish  Specific data (power) tools and integrations  Can validate your data before publishing  Data API  Data Conversion / Bundling: zip the data, provide sqlite  Generate a node package of your data  (Versioning)     Table of Contents    User Stories  Personas  Stories  1. Get Started  1. Sign in / Sign up [DONE]  Sign up via github (and/or google) [DONE]  Next Step after Sign Up  Invite User to Join Platform    2. Publish Data Packages  Publish with a Client [DONE]  Configure Client [DONE]    Update a Data Package [DONE]  Delete a Data Package  Purge a Data Package  Validate Data in Data Package  Validate in CLI [DONE]  Validate on Server    Cache Data Package Resource data (on the server)  Publish with Web Interface  Undelete data package  Render (views) in data package in CLI before upload    3. Find and View Data Packages  View a Data Package Online [DONE]  (Pre)View a not-yet-published Data Package Online  See How Much a Data Package is Used (Downloaded) {2d}  Browse Data Packages [DONE]  Search for Data Packages [DONE]  Download Data Package Descriptor  Download Data Package in One File (e.g. zip)    4. Get a Data Package (locally)  Use DataPackage in Node (package auto-generated)  Import DataPackage into R [DONE?]  Import DataPackage into Pandas [DONE?]  SQL / SQLite database  See changes between versions  Low Priority    5. Versioning and Changes in Data Packages  Explicit Versioning - Publisher  Explicit Versioning - Consumer  Know when a package has changed re caching    Revisioning - Implicit Versioning  Change Notifications    6. Publishers  Create a New Publisher  Find a Publisher (and users?)  View a Publisher Profile  Search among publishers packages  Registered Users Profile and packages    Publisher and User Leaderboard  Manage Publisher  Create and Edit Profile  Add and Manage Members      7. Web Hooks and Extensions  8. Administer Site  Configure Site  See usage metrics  Pricing and Billing    Private Data Packages  Sell My Data through your site", 
            "title": "User Stories"
        }, 
        {
            "location": "/developers/user-stories/#personas", 
            "text": "[Geek] Publisher . Knows how to use a command line or other automated tooling. Wants to publish their data package in order to satisfy their teams requirements to publish data.  Non-Geek Publisher. Tbc \u2026    Consumer : A person or organization looking to use data packages (or data in general)  Data Analyst  Coder (of data driven applications)  \u2026    Admin : A person or organization who runs an instance of a DataHub", 
            "title": "Personas"
        }, 
        {
            "location": "/developers/user-stories/#stories", 
            "text": "", 
            "title": "Stories"
        }, 
        {
            "location": "/developers/user-stories/#1-get-started", 
            "text": "", 
            "title": "1. Get Started"
        }, 
        {
            "location": "/developers/user-stories/#1-sign-in-sign-up-done", 
            "text": "As a Geek Publisher I want to sign up for an account so that I can publish my data package to the registry and to have a publisher account to publish my data package under.  Generally want this to be as minimal, easy and quick as possible   Sign in with a Google account  (?) what about up other social accounts?    Essential profile information (after sign in we prompt for this)  email address  Name  (?) - future. Credit card details for payment - can we integrate with payment system (?)    They need to choose a user name which is url friendly unique human readable name for our app. Can be used in sign in and in many other places.  WHY? Where would we need this? For url on site   for publisher  Same as publisher names (needed for URLs): [a-z-_.]  Explain: they cannot change this later e.g.  Choose wisely! Once you set this it cannot be changed later!    Send the user an email confirming their account is set up and suggesting next steps   Automatically:   Auto-create a publisher for them  Same name as their user name but a publisher  That way they can start publishing straight away \u2026     TODO: (??) should we do *all  of this via the command line client (a la npmjs) *", 
            "title": "1. Sign in / Sign up [DONE]"
        }, 
        {
            "location": "/developers/user-stories/#sign-up-via-github-andor-google-done", 
            "text": "As a Visitor I want to sign up via github or google so that I don\u2019t have to enter lots of information and remember my password for yet another website   How do we deal with username conflicts? What about publisher name conflicts?  This does not arise in simple username system because we have only pool of usernames", 
            "title": "Sign up via github (and/or google) [DONE]"
        }, 
        {
            "location": "/developers/user-stories/#next-step-after-sign-up", 
            "text": "As a Geek Publisher I want to know what do next after signing up so that I can get going quickly.  Things to do:   Edit your profile  Download a client / Configure your client (if you have one already)  Instructions on getting relevant auth credentials  Note they will  need  to have set a username / password in their profile    Join a Publisher (understand what a publisher is!)", 
            "title": "Next Step after Sign Up"
        }, 
        {
            "location": "/developers/user-stories/#invite-user-to-join-platform", 
            "text": "As an Admin (or existing Registered User?) I want to invite someone to join the platform so that they can start contributing or using data   Get an invitation email with a sign up link  Some commonality with Publisher invite member below", 
            "title": "Invite User to Join Platform"
        }, 
        {
            "location": "/developers/user-stories/#2-publish-data-packages", 
            "text": "", 
            "title": "2. Publish Data Packages"
        }, 
        {
            "location": "/developers/user-stories/#publish-with-a-client-done", 
            "text": "As a Geek Publisher I want to import (publish) my data package into the registry so my data has a permanent online home so that I and others can have access  On command line looks like:  $ cd my/data/package\n$ data publish  \u2026 working \u2026   SUCCESS  Notes   Permissions: must be a member of the Publisher  Internally: DataPackageCreate or DataPackageUpdate capability    Handle conflicts: if data package already exists, return 409. Client instructions should be already exists and use  force  or similar to overwrite  API endpoint behind the scenes: POST {api}/package/  TODO: private data packages  And payment!", 
            "title": "Publish with a Client [DONE]"
        }, 
        {
            "location": "/developers/user-stories/#configure-client-done", 
            "text": "As a Geek Publisher I want to configure my client so I can start publishing data packages.  Locally in $HOME store store something like:  .dpm/credentials # stores your API key and user name\n\n.dpm/config      # stores info like your default publisher", 
            "title": "Configure Client [DONE]"
        }, 
        {
            "location": "/developers/user-stories/#update-a-data-package-done", 
            "text": "As a Geek Publisher I want to use a publish command to update a data package that is already in the registry so it appears there   Old version will be lost (!)", 
            "title": "Update a Data Package [DONE]"
        }, 
        {
            "location": "/developers/user-stories/#delete-a-data-package", 
            "text": "As a Geek Publisher I want to unpublish (delete) a data package so it is no longer visible to anyone", 
            "title": "Delete a Data Package"
        }, 
        {
            "location": "/developers/user-stories/#purge-a-data-package", 
            "text": "As a Geek Publisher I want to permanently delete (purge) a data package so that it no longer takes up storage space", 
            "title": "Purge a Data Package"
        }, 
        {
            "location": "/developers/user-stories/#validate-data-in-data-package", 
            "text": "", 
            "title": "Validate Data in Data Package"
        }, 
        {
            "location": "/developers/user-stories/#validate-in-cli-done", 
            "text": "As a Publisher [owner/member] I want to validate the data I am about to publish to the registry so that I publish \u201cgood\u201d data and know that I am doing so and do not have to manually check that the published data looks ok (e.g. rendering charts properly) (and if wrong I have to re-upload)  data datavalidate [file-path]   [file-path] - run this against a given file. Look in the resources to see if this file is there and if so use the schema. Otherwise just do goodtables table \u2026  If no file provided run validate against each resource in turn in the datapackage    Output to stdout.  Default: human-readable - nice version of output from goodtables.  Option for JSON e.g.  json to put machine readable output  check goodtables command line tool and follow if possible. Can probably reuse code      Auto-run this before publish unless explicit suppression (e.g.  skip-datavalidate)  Use goodtables (?)", 
            "title": "Validate in CLI [DONE]"
        }, 
        {
            "location": "/developers/user-stories/#validate-on-server", 
            "text": "As a Publisher [owner] i want my data to be validated when I publish it so that I know immediately if I have accidentally \u201cbroken\u201d my data or have bugs and can take action to correct  As a Consumer I want to know that the data I am downloading is \u201cgood\u201d and can be relied on so that I don\u2019t have to check it myself or run into annoying bugs later on   Implies showing something in the UI e.g. \u201cData Valid\u201d (like build passing)   Implementation notes to self   Need a new table to store results of validation and a concept of a \u201crun\u201d  Store details of the run [e.g. time to complete, ]    How to automate doing validation (using goodtables we assume) - do we reuse a separate service (goodtables.io in some way) or run ourselves in a process like ECS ???  Display this in frontend", 
            "title": "Validate on Server"
        }, 
        {
            "location": "/developers/user-stories/#cache-data-package-resource-data-on-the-server", 
            "text": "As a Publisher I want to publish a data package where its resource data is stored on my servers but the registry caches a copy of that data so that if my data is lost or gets broken I still have a copy people can use  As a Consumer I want to be able to get the data for a data package even if the original data has been moved or removed so that I can still use is and my app or analysis keeps working   TODO: what does this mean for the UI or command line tools. How does the CLI know about this, how does it use it?", 
            "title": "Cache Data Package Resource data (on the server)"
        }, 
        {
            "location": "/developers/user-stories/#publish-with-web-interface", 
            "text": "As a Publisher I want to publish a data package in the UI so that it is available and published   Publish =  they already have datapackage.json and all the data. They just want to be able to upload and store this.   As a Publisher I want to create a data package in the UI so that it is available and published   Create =  no datapackage.json - just data files. Need to add key descriptors information, upload data files and have schemas created etc etc.", 
            "title": "Publish with Web Interface"
        }, 
        {
            "location": "/developers/user-stories/#undelete-data-package", 
            "text": "[cli] As a Publisher I want to be able to restore the deleted data package via cli, so that it is back visible and available to view, download (and searchable)  dpmpy undelete  [webui] As a Publisher i want to undelete the deleted data packages, so that the deleted data packages is now visible again.", 
            "title": "Undelete data package"
        }, 
        {
            "location": "/developers/user-stories/#render-views-in-data-package-in-cli-before-upload", 
            "text": "As a Publisher, I want to be able to preview the views (graphs and table (?)) of the current data package using cli prior to publishing so that I can refine the json declarations of datapackage view section to achieve a great looking result.", 
            "title": "Render (views) in data package in CLI before upload"
        }, 
        {
            "location": "/developers/user-stories/#3-find-and-view-data-packages", 
            "text": "", 
            "title": "3. Find and View Data Packages"
        }, 
        {
            "location": "/developers/user-stories/#view-a-data-package-online-done", 
            "text": "EPIC: As a Consumer I want to view a data package online so I can get a sense of whether this is the dataset I want   Obsess here about \u201cwhether this is the dataset I want\u201d  *Publishers want this too \u2026 *  Also important for SEO if we have good info here   Features   Visualize data in charts - gives one an immediate sense of what this is  One graph section at top of page after README excerpt  One graph for each entry in the \u201cviews\u201d    Interactive table - allows me to see what is in the table  One table for each resource     This user story can be viewed from two perspectives:   From a publisher point of view  From a consumer point of view   As a  publisher  i want to show the world how my published data is so that it immediately catches consumer\u2019s attention (and so I know it looks right - e.g. graph is ok)  As a  consumer  i want to view the data package so that i can get a sense of whether i want this dataset or not.  Acceptance criteria - what does done mean!   A table for each resource  Simple graph spec works =  converts to plotly  Multiple time series    Plotly spec graphs work  All core graphs work (not sure how to check every one but representative ones)  Recline graphs specs (are handled - temporary basis)  Loading spinners whilst data is loading so users know what is happening   Bonus:   Complex examples e.g. time series with a log scale \u2026 (e.g. hard drive data \u2026)   Features:  DP view status   Different options to view data as graph.  Recline  Vega-lite  Vega  [Plotly]    General Functionality  Multiple views [wrongly done. We iterate over resource not views]  Table as a view    Interactive table so that consumer can do  Filter  Join", 
            "title": "View a Data Package Online [DONE]"
        }, 
        {
            "location": "/developers/user-stories/#preview-a-not-yet-published-data-package-online", 
            "text": "As a (potential) Publisher I want to preview a datapackage I have prepared so that I can check it works and share the results (if there is something wrong with others)   Be able to supply a URL to my datapackage (e.g. on github) and have it previewed as it would look on DPR  Be able to upload a datapackage and have it previewed   Rufus: this was a very common use case for me (and others) when using data.okfn.org. Possibly less relevant if the command line tool can do previewing but still relevant IMO (some people may not have command line tool, and it is useful to be able to share a link e.g. when doing core datasets curation and there is something wrong with a datapackage).  Rufus: also need for an online validation tool", 
            "title": "(Pre)View a not-yet-published Data Package Online"
        }, 
        {
            "location": "/developers/user-stories/#see-how-much-a-data-package-is-used-downloaded-2d", 
            "text": "As a Consumer i want to see how much the data has been downloaded so that i can choose most popular (=  probably most reliable and complete) in the case when there are several alternatives for my usecase (maybe from different publishers)", 
            "title": "See How Much a Data Package is Used (Downloaded) {2d}"
        }, 
        {
            "location": "/developers/user-stories/#browse-data-packages-done", 
            "text": "As a potential Publisher, unaware of datapackages, I want to see real examples of published packages (with the contents datapackage.json), so that I can understand how useful and simple is the datapackage format and the registry itself.  As a Consumer I want to see some example data packages quickly so I get a sense of what is on this site and if it is useful to look further   Browse based on what properties? Most recent, most downloaded?  Most downloaded  Start with: we could just go with core data packages", 
            "title": "Browse Data Packages [DONE]"
        }, 
        {
            "location": "/developers/user-stories/#search-for-data-packages-done", 
            "text": "As a Consumer I want to search data packages so that I can find the ones I want   Essential question: what is it you want?  Rufus: in my view generic search is actually  not  important to start with. People do not want to randomly search. More useful is to go via a publisher at the beginning.    Search results should provide enough information to help a user decide whether to dig further e.g. title, short description  For future when we have it: [number of downloads], stars etc     Minimum viable search (based on implementation questions)   Filter by publisher  Free text search against title  Description could be added if we start doing actual scoring as easy to add additional fields    Scoring would be nice but not essential     Implementation questions:   Search:  Should search perform ranking (that requires scoring support)  Free text queries should search against which fields (with what weighting)?    Filtering: On what individual properties of the data package should be able to filter?  Themes and profiles:  Searching for a given profile: not possible atm.  Themes: Should we tag data packages by themes like finance, education and let user find data package by that?\n        * Maybe but not now - maybe in the future    If we follow the go via a publisher at the beginning then should we list the most popular publisher on the home page of user[logged-in/ not logged in]?  If most popular publisher then by what mesaure?  Sort by Most published?  Sort by Most followers?  Sort by most downloads?  Or all show top5 in each facet?       Sub user stories:   [WONTFIX?] As a Consumer i want to find the data packages by profile (ex: spending) so that I can find the kind of data I want quickly and easily and in one big list  As a Consumer i want to search based on description of data package, so that I can find package which related to some key words", 
            "title": "Search for Data Packages [DONE]"
        }, 
        {
            "location": "/developers/user-stories/#download-data-package-descriptor", 
            "text": "As a Consumer I want to download the data package descriptor (datapackage.json) on its own so that \u2026  *Rufus: I can\u2019t understand why anyone would want to do this *", 
            "title": "Download Data Package Descriptor"
        }, 
        {
            "location": "/developers/user-stories/#download-data-package-in-one-file-eg-zip", 
            "text": "As a Consumer I want to download the data package in one file so that I don\u2019t have to download descriptor and each resource by hand  Only useful if no cli tool and no install command", 
            "title": "Download Data Package in One File (e.g. zip)"
        }, 
        {
            "location": "/developers/user-stories/#4-get-a-data-package-locally", 
            "text": "Let\u2019s move discussion to the github: * https://github.com/frictionlessdata/dpm-py/issues/30  TODO add these details from the requirement doc   Local \u201cData Package\u201d cache storage ( .datapackages  or similar)  Stores copies of packages from Registry  Stores new Data Packages the user has created  This   Ruby lib   implements something similar", 
            "title": "4. Get a Data Package (locally)"
        }, 
        {
            "location": "/developers/user-stories/#use-datapackage-in-node-package-auto-generated", 
            "text": "As a NodeJS developer I want to use data package as a node lib in my project so that I can depend on it using my normal dependency framework   See this  real-world example  of this request for country-list  =  auto-building node package and publishing to npm (not that hard to do \u2026)  Convert CSV data to json (that\u2019s what you probably want from node?)  Generate package.json  Push to npm (register the dataset users)  Rufus: My guess here is that to implement here we want something a bit like github integrations \u2013 specific additional hooks which also get some configuration (or do it like travis - github integration plus a tiny config file - in our case rather than a .travis.yml we have a .node.yml or whatever)    Is it configurable for user that enable to push to npm or not?  Yes. Since we need to push to a specific npm user (for each publisher) this will need to be configured (along with authorization - where does that go?)    Is this something done for  all  data packages or does user need to turn something on? Probably want them to turn this on \u2026   Questions:   From where we should push the data package to npm repo.  Is it from dpmpy or from server? Obviously from a server - this needs to be automated. But you can use dpmpy if you want (but I\u2019m not sure we do want to \u2026)    What to do with multiple resources? Ans: include all resources  Do we include datapackage.json into the node package? Yes, include it so they get all the original metadata.   Generic version is:  As a Web Developer I want to download a DataPackage (like currency codes or country names) so that I can use it in the web service I am building [ ]", 
            "title": "Use DataPackage in Node (package auto-generated)"
        }, 
        {
            "location": "/developers/user-stories/#import-datapackage-into-r-done", 
            "text": "As a Consumer [R user] I want to load a Data Package from R so that I can immediately start playing with it   Should we try and publish to CRAN?  Probably not? Why? think it can be quite painful getting permission to publish to CRAN and very easy to load from the registry  On the CRAN website I can t find a way to automate publishing. It seems possible by filling web-form, but to know the status we have to wait and parse email.      Using this library:  https://github.com/ropenscilabs/datapkg  Where can i know about this?  On each data package view page \u2026     Generic version:  As a Data Analyst I want to download a data package, so that I can study it and wrangle with it to infer new data or generate new insights.  As a Data Analyst, I want to update previously downloaded data package, so that I can work with the most recent data.", 
            "title": "Import DataPackage into R [DONE?]"
        }, 
        {
            "location": "/developers/user-stories/#import-datapackage-into-pandas-done", 
            "text": "TODO - like R", 
            "title": "Import DataPackage into Pandas [DONE?]"
        }, 
        {
            "location": "/developers/user-stories/#sql-sqlite-database", 
            "text": "As a Consumer I want to download a DataPackage\u2019s data one coherent SQLite database so that I can get it easily in one form  Question:   Why does we need to store datapackage data in sqlite. Is not it better to store in file structure?   We can store the datapackage like this way:  ~/.datapackage/ publisher / package / version /*  This is the way maven/gradle/ivy cache jar locally.", 
            "title": "SQL / SQLite database"
        }, 
        {
            "location": "/developers/user-stories/#see-changes-between-versions", 
            "text": "As a Data Analyst I want to compare different versions of some datapackage locally, so that I can see schema changes clearly and adjust my analytics code to the desired schema version.", 
            "title": "See changes between versions"
        }, 
        {
            "location": "/developers/user-stories/#low-priority", 
            "text": "As a Web Developer of multiple projects, I want to be able to install multiple versions of the same datapackage separately so that all my projects could be developed independently and deployed locally. (virtualenv-like)  As a Developer I want to list all DataPackages requirements for my project in the file and pin the exact versions of any DataPackage that my project depends on so that the project can be deterministically deployed locally and won\u2019t break because of the DataPackage schema changes. (requirements.txt-like)", 
            "title": "Low Priority"
        }, 
        {
            "location": "/developers/user-stories/#5-versioning-and-changes-in-data-packages", 
            "text": "When we talk about versioning we can mean two things:   Explicit versioning: this is like the versioning Of releases \u201cv1.0\u201d etc. This is conscious and explicit. Main purpose:  to support other systems depending on this one (they want the data at a known stable state)  easy access to major staging points in the evolution (e.g. i want to see how things were at v1)    Implicit versioning or \u201crevisioning\u201d: this is like the commits in git or the autosave of a word or google doc. It happens frequently, either with minimum effort or even automatically. Main purpose:  Undelete and recovery (you save a every point and can recover if you accidentally write or delete something)  Collaboration and merging of changes (in revision control)  Activity logging", 
            "title": "5. Versioning and Changes in Data Packages"
        }, 
        {
            "location": "/developers/user-stories/#explicit-versioning-publisher", 
            "text": "As a Publisher I want to tag a version of my data on the command line so that \u2026 [see so that\u2019s below]  dpmpy tag {tag-name}  =  tag current \u201clatest\u201d on the server as {tag-name}   Do we restrict {tag-name} to semver? I don\u2019t think so atm.  As a {Publisher} I want to tag datapackage to create a snapshot of data on the registry server, so that consumers can refer to it  As a {Publisher} I want to be warned that a tag exists, when I try to overwrite it, so that I don\u2019t accidentally overwrite stable tagged data, which is relied on by consumers.  As a {Publisher} I want to be able to overwrite the previously tagged datapackage, so that I can fix it if I mess up.  The versioning here happens server side  Is this confusing for users? I.e. they are doing something local.     Background \u201cso that\u201d user story epics:   As a {Publisher} I want to version my Data Package and keep multiple versions around including older versions so that I do not break consumer systems when I change my Data Package (whether schema or data) [It is not just the publisher who wants this, it is a consumer - see below]  As a {Publisher} I want to be able to get access to a previous version I tagged so that I can return to it and review it (and use it)  so that i can recover old data if i delete it myself or compare how things changed over time", 
            "title": "Explicit Versioning - Publisher"
        }, 
        {
            "location": "/developers/user-stories/#explicit-versioning-consumer", 
            "text": "As a {Consumer} (of a Data Package) I want to know full details when and how the data package schema has changed and when so that I can adjust my scripts to handle it.  Important info to know for each schema change:   time when published  for any  changed  field - name, what was changed (type, format, \u2026?),\n      +maybe everything else that was not changed (full field descriptor)  for any  deleted  field - name,\n      +maybe everything else (full field descriptor)  for any  added  field - all data (full field descriptor)   *A change in schema would correspond to a major version change in software (see  http://semver.org/ )  Concerns about explicit versioning : we all have experience with consuming data from e.g. government publishers where the publishers change the data schema breaking client code. I am constatnly looking for a policy/mechanism to guide publishers to develop stable schema versioning for the data they produce, and help consumers to get some stability guarantees.  Automated versioning / automated tracking : Explicit versioning relies on the publisher, and humans can forget or not care enough about others. So to help consumers my suggestion would be to always track schema changes of uploaded packages on the server, and allow users to review those changes on the website. (We might even want to implement auto-tagging or not allowing users to upload a package with the same version but a different schema without forcing)  As a {Consumer} I want to get a sense how outdated is the datapackage, that I have downloaded before, so that I can decide if I should update or not.   I want to preview a DataPackage changelog (list of all available versions/tags with brief info) online, sorted by creation time, so that I can get a sense how data or schema has changed since some time in the past. Important brief info:  Time when published  How many rows added/deleted for each resource data  What fields(column names) changed, added or deleted for each resource.     As a {Consumer} I want to view a Datapackage at a particular version online, so that I can present/discuss the particular data timeslice of interest with other people.  As a {Consumer} I want to download a Data package at a particular version so that I know it is compatible with my scripts and system   Online: I want to pick the version I want from the list, and download it (as zip for ex.)  CLI: I want to specify tag or version when using the  install  command.", 
            "title": "Explicit Versioning - Consumer"
        }, 
        {
            "location": "/developers/user-stories/#know-when-a-package-has-changed-re-caching", 
            "text": "Excerpted from:  https://github.com/okfn/data.okfn.org-new/issues/7  From @trickvi on June 20, 2013 12:37  I would like to be able to use data.okfn.org as an intermediary between my software and the data packages it uses and be able to quickly check whether there s a new version available of the data (e.g. if I ve cached the package on a local machine).  There are ways to do it with the current setup:   Download the datapackage.json descriptor file, parse it and get the version there and check it against my local version. Problems:  This solution relies on humans and that they update their version but there might not be any consistency in it since the data package standard describes the version attribute as:  a version string conforming to the Semantic Versioning requirement  I have to fetch the whole datapackage.json (it s not big I know but why download all that extra data I might not even want)  Go around data.okfn.org and look directly at the github repository. Problems:  I have to find out where the repo is, use git and do a lot of extra stuff (I don t care how the data packages are stored, I just want a simple interface to fetch them)  What would be the point of data.okfn.org/data? In my mind it collects data packages and provides a consistent interface to get the data packages irrespective of how its stored.   I propose data.okfn.org provides an internal system to allow users to quickly check whether a new version might be released. This does not have to be an API. We could leverage HTTP s caching mechanism using an ETag header that would contain some hash value. This hash value can e.g. be the the sha value of heads ref objects served via the Github API:  https://api.github.com/repos/datasets/cpi/git/refs/heads/master  Software that works with data packages could then implement a caching strategy and just send a request with an If-None-Match header along with a GET request for datapackage.json to either get a new version of the descriptor (and look at the version in that file) or just serve the data from its cache.  Copied from original issue: frictionlessdata/ideas#51", 
            "title": "Know when a package has changed re caching"
        }, 
        {
            "location": "/developers/user-stories/#revisioning-implicit-versioning", 
            "text": "\u2026", 
            "title": "Revisioning - Implicit Versioning"
        }, 
        {
            "location": "/developers/user-stories/#change-notifications", 
            "text": "As a Consumer I want to be notified of changes to a package i care about so that I can check out what has changed and take action (like downloading the updated data)  As a Consumer I want to see how active the site is to see if I should get involved", 
            "title": "Change Notifications"
        }, 
        {
            "location": "/developers/user-stories/#6-publishers", 
            "text": "", 
            "title": "6. Publishers"
        }, 
        {
            "location": "/developers/user-stories/#create-a-new-publisher", 
            "text": "TODO", 
            "title": "Create a New Publisher"
        }, 
        {
            "location": "/developers/user-stories/#find-a-publisher-and-users", 
            "text": "As a Consumer I want to browse and find publishers so that I can find interesting publishers and their packages (so that I can use them)", 
            "title": "Find a Publisher (and users?)"
        }, 
        {
            "location": "/developers/user-stories/#view-a-publisher-profile", 
            "text": "view data packages associated to a publisher or user  Implementation details:  https://hackmd.io/MwNgrAZmCMAcBMBaYB2eAWR72woghmLNIrAEb4AME+08s6VQA===  As a Consumer I want to see a publisher\u2019s profile so that I can discover their packages and get a sense of how active and good they are  As a Publisher I want to have a profile with a list of my data packages so that:   Others can find my data packages quickly and easily  Can see how many data packages i have  I can find a data package i want to look at quickly [they can discover their own data]  I can find the link for a data package to send to someone else  People want to share what they have done. This is probably the number one way the site gets prominence at the start (along with simple google traffic)    so that I can check that members do not abuse their rights to publish and only publish topical data packages.   As a Consumer I want to view a publisher\u2019s profile so that I can see who is behind a particular package or to see what other packages they produce [navigate up from a package page] [so that: i can trust on his published data packages to reuse.]  Details   Profile =  Full name / title e.g. \u201cWorld Bank\u201d, identifier e.g. world-bank  picture, short description text (if we have this - we don\u2019t atm)  (esp important to know if this is the world bank or not)    Total number of data packages  List of data packages  View by most recently created (updated?)  For each DataPackage want to see: title, number of resources (?), first 200 character of description, license (see data.okfn.org/data/ for example)  Do we limit / paginate this list? No, not for the moment    [wontfix atm] Activity - this means data packages published, updated  [wontfix atm] Quality \u2026 - we don\u2019t have anything on this  [wontfix atm] List of users    What are the permissions here?  Do we show private data packages? No  Do we show them when \u201cowner\u201d viewing or sysadmin? Yes (but flag as \u201cprivate\u201d)      What data packages to show? All the packages you own.  What about pinning? No support for this atm.", 
            "title": "View a Publisher Profile"
        }, 
        {
            "location": "/developers/user-stories/#search-among-publishers-packages", 
            "text": "As a Consumer i want to search among all data packages owned by a publisher so that I can easily find one data package amongst all the data packages by this publisher.", 
            "title": "Search among publishers packages"
        }, 
        {
            "location": "/developers/user-stories/#registered-users-profile-and-packages", 
            "text": "As a Consumer i want to see the profile and activity of a user so that \u2026  As a Registered User I want to see the data packages i am associated with  so that  [like publisher]", 
            "title": "Registered Users Profile and packages"
        }, 
        {
            "location": "/developers/user-stories/#publisher-and-user-leaderboard", 
            "text": "As a ??? I want to see who are the top publihers and users so that I can emulate them or ???", 
            "title": "Publisher and User Leaderboard"
        }, 
        {
            "location": "/developers/user-stories/#manage-publisher", 
            "text": "", 
            "title": "Manage Publisher"
        }, 
        {
            "location": "/developers/user-stories/#create-and-edit-profile", 
            "text": "As {Owner  } I want to edit my profile so that it is updated with new information", 
            "title": "Create and Edit Profile"
        }, 
        {
            "location": "/developers/user-stories/#add-and-manage-members", 
            "text": "As an {Owner of a Publisher in the Registry} I want to invite an existing user to become a member of my publisher   Auto lookup by user name (show username and fullname) - standard as per all sites  User gets a notification on their dashboard + email with link to accept invite  If invite is accepted notify the publisher (?) - actually do not do this.   As an {Owner of a Publisher in the Registry} I want to invite someone using their email to sign up and become a member of my Publisher so that they are authorized to publish data packages under my Publisher.  As an {Publisher Owner} I want to remove someone from membership in my publisher so they no longer have ability to publish or modify my data packages  As a {Publisher Owner} I want to view all the people in my organization and what roles they have so that I can change these if I want  As a {Publisher Owner} I want to make a user an \u201cowner\u201d so they have full control  As a {Publisher Owner} I want to remove a user as an \u201cowner\u201d so they are just a member and no longer have full control", 
            "title": "Add and Manage Members"
        }, 
        {
            "location": "/developers/user-stories/#7-web-hooks-and-extensions", 
            "text": "TODO: how do people build value added services around the system (and push back over the API etc \u2026) - OAuth etc", 
            "title": "7. Web Hooks and Extensions"
        }, 
        {
            "location": "/developers/user-stories/#8-administer-site", 
            "text": "", 
            "title": "8. Administer Site"
        }, 
        {
            "location": "/developers/user-stories/#configure-site", 
            "text": "As the Admin I want to set key configuration parameters for my site deployment so that I can change key information like the site title   Main config database is the one thing we might need", 
            "title": "Configure Site"
        }, 
        {
            "location": "/developers/user-stories/#see-usage-metrics", 
            "text": "As an Admin I want to see key metrics about usage such as users, API usage, downloads etc so that I know how things are going   Total users are signed up, how many signed up in last week / month etc  Total publishers \u2026  Users per publisher distribution (?)    API usage  Downloads  Billing: revenue in relevant periods  Costs: how much are we spending on storage", 
            "title": "See usage metrics"
        }, 
        {
            "location": "/developers/user-stories/#pricing-and-billing", 
            "text": "As an Admin I want to have a pricing plan and billing system so that I can charge users and make my platform sustainable  As a Publisher I want to know if this site has a pricing plan and what the prices are so that I can work out what this will cost me in the future and have a sense that these guys are sustainable (\u2018free forever\u2019 does not work very well)  As a Publisher I want to sign up for a given pricing plan so that I am entitled to what it allows (e.g. private stuff \u2026)", 
            "title": "Pricing and Billing"
        }, 
        {
            "location": "/developers/user-stories/#private-data-packages", 
            "text": "cf npmjs.com  As a Publisher I want to have private data packages that I can share just with my team", 
            "title": "Private Data Packages"
        }, 
        {
            "location": "/developers/user-stories/#sell-my-data-through-your-site", 
            "text": "EPIC: As a Publisher i want to sell my data through your site so that I make money and am able to sustain my publishing and my life \u2026", 
            "title": "Sell My Data through your site"
        }, 
        {
            "location": "/publishers/", 
            "text": "Publishing\n\n\nThis guide is all about how to put your data online using the DataHub.\n\n\nQuickstart\n\n\nAs a new user you want to know quickly how to get going so that you can get your data online as soon as possible.\n\n\nTo put your data online you use our \ndata\n command line tool:\n\n\n\n\nHow do you get the \ndata\n tool? =\n follow the \ninstallation instructions\n\n\nHow do you use the \ndata\n tool? =\n Run \ndata login\n (if your first time) then \ndata push\n\n\n\n\nIn summary:\n\n\n\ngraph TD                                                                                 \nclient[Get the `data` tool]\nlogin[Login - on the command line]\npush[Push data using `data` tool]\n\nclient --> login\nlogin --> push\n\n\n\n\nDesktop app\n: we are  planning a Desktop app as well as the command line tool. If you\nre interested \nlet us know\n.\n\n\nAPI access\n: you can push via an API. However, the process is not yet well documented \n so if you want to use it \nlet us know and we can walk you through it\n.", 
            "title": "Getting started"
        }, 
        {
            "location": "/publishers/#publishing", 
            "text": "This guide is all about how to put your data online using the DataHub.", 
            "title": "Publishing"
        }, 
        {
            "location": "/publishers/#quickstart", 
            "text": "As a new user you want to know quickly how to get going so that you can get your data online as soon as possible.  To put your data online you use our  data  command line tool:   How do you get the  data  tool? =  follow the  installation instructions  How do you use the  data  tool? =  Run  data login  (if your first time) then  data push   In summary:  \ngraph TD                                                                                 \nclient[Get the `data` tool]\nlogin[Login - on the command line]\npush[Push data using `data` tool]\n\nclient --> login\nlogin --> push  Desktop app : we are  planning a Desktop app as well as the command line tool. If you re interested  let us know .  API access : you can push via an API. However, the process is not yet well documented   so if you want to use it  let us know and we can walk you through it .", 
            "title": "Quickstart"
        }, 
        {
            "location": "/publishers/core-datasets/", 
            "text": "Core Datasets\n\n\nImportant, commonly-used datasets as high quality, easy-to-use \n open data packages\n\n\nCore Datasets are important, commonly-used \ncore\n datasets\n like GDP or country codes made available as \nhigh-quality\n, \neasy-to-use\n and \nopen\n \ndata packages\n. Find them online here on the DataHub:\n\n\nhttp://datapackaged.com/core/\n\n\nKey features are:\n\n\n\n\nHigh Quality \n Reliable\n \n sourcing, normalizing and quality checking a set of \nkey reference and indicator datasets such as country codes, currencies, GDP and population\n\n\nStandardized \n Bulk\n \n all datasets provided in a \nstandardized\n form and can be accessed in \nbulk as CSV\n together with a simple \nJSON schema\n\n\nVersioned \n Packaged\n \n all data is in \ndata packages\n and is \nversioned\n using git so all changes are visible and data can be \ncollaboratively maintained\n\n\n\n\nThe \nCore Datasets\n effort is part of the broader \nFrictionless Data initiative\n.\n\n\n\n\n\n\nCore Data Curators\n\n\nThe Core Data Curators curate the core datasets.\n\n\nCuration involves identifying and locating core (public) datasets, then packaging them up as high-quality, reliable, and easy-to-use \ndata packages\n (standardized, structured, open).\n\n\nNew team members wanted:\n We are always seeking volunteers to join the Data Curators team. Get to be part of a crack team and develop and hone your data wrangling skills whilst helping to provide high quality data to the community.\n\n\n\n\nAnyone can contribute\n: details on the \nroles and skills needed below\n.\n\n\nGet involved\n: read more below or jump straight to \nthe sign-up section\n.\n\n\nData Curators Guide\n: can\nt wait to get started as a Data Curator? You can dive straight in and start packaging datasets using the \ncore data curators guide\n.\n\n\n\n\n\n\n\n\n\nWhat Roles and Skills are Needed\n\n\nWe have a variety of roles from identifying new \ncore\n datasets, to collecting and packaging the data, to performing quality control.\n\n\nCore Skills\n \n at least one of these skills is strongly recommended:\n\n\n\n\nData Wrangling Experience\n. Many of our source datasets are not complex (just an Excel file or similar) and can be \nwrangled\n in a Spreadsheet program. What we therefore recommend is at least one of:\n\n\nExperience with a Spreadsheet application such as Excel or (preferably) Google Docs including use of formulas and (desirably) macros (you should at least know how you could quickly convert a cell containing \n2014\n to \n2014-01-01\n across 1000 rows)\n\n\nCoding for data processing (especially scraping) in one or more of python, javascript, bash\n\n\n\n\n\n\nData sleuthing\n - the ability to dig up data on the web (specific desirable skills: you know how to search by filetype in google, you know where the developer tools are in chrome or firefox, you know how to find the URL a form posts to)\n\n\n\n\nDesirable Skills\n (the more the better!):\n\n\n\n\nData vs Metadata: know difference between data and metadata\n\n\nFamiliarity with Git (and Github)\n\n\nFamiliarity with a command line (preferably bash)\n\n\nKnow what JSON is\n\n\nMac or Unix is your default operating system (will make access to relevant tools that much easier)\n\n\nKnowledge of Web APIs and/or HTML\n\n\nUse of curl or similar command line tool for accessing Web APIs or web pages\n\n\nScraping using a command line tool or (even better) by coding yourself\n\n\nKnow what a Data Package and a Tabular Data Package are\n\n\nKnow what a text editor is (e.g. notepad, textmate, vim, emacs, \n) and know how to use it (useful for both working with data and for editing Data Package metadata)\n\n\n\n\n\n\nGet Involved - Sign Up Now!\n\n\nHere\ns what you need to know when you sign up:\n\n\n\n\nTime commitment\n: Members of the team commit to at least 8-16h per month (though this will be an average - if you are especially busy with other things one month and do less that is fine)\n\n\nSchedule\n: There is no schedule so you can contribute at any time that is good for you - evenings, weekeneds, lunch-times etc\n\n\nLocation\n: all activity will be carried out online so you can be based anywhere in the world\n\n\nSkills\n: see above\n\n\n\n\nTo register your interest fill in the following form. Any questions, please \nget in touch directly\n.\n\n\nLoading...", 
            "title": "Core Datasets"
        }, 
        {
            "location": "/publishers/core-datasets/#core-datasets", 
            "text": "Important, commonly-used datasets as high quality, easy-to-use   open data packages  Core Datasets are important, commonly-used  core  datasets  like GDP or country codes made available as  high-quality ,  easy-to-use  and  open   data packages . Find them online here on the DataHub:  http://datapackaged.com/core/  Key features are:   High Quality   Reliable    sourcing, normalizing and quality checking a set of  key reference and indicator datasets such as country codes, currencies, GDP and population  Standardized   Bulk    all datasets provided in a  standardized  form and can be accessed in  bulk as CSV  together with a simple  JSON schema  Versioned   Packaged    all data is in  data packages  and is  versioned  using git so all changes are visible and data can be  collaboratively maintained   The  Core Datasets  effort is part of the broader  Frictionless Data initiative .", 
            "title": "Core Datasets"
        }, 
        {
            "location": "/publishers/core-datasets/#core-data-curators", 
            "text": "The Core Data Curators curate the core datasets.  Curation involves identifying and locating core (public) datasets, then packaging them up as high-quality, reliable, and easy-to-use  data packages  (standardized, structured, open).  New team members wanted:  We are always seeking volunteers to join the Data Curators team. Get to be part of a crack team and develop and hone your data wrangling skills whilst helping to provide high quality data to the community.   Anyone can contribute : details on the  roles and skills needed below .  Get involved : read more below or jump straight to  the sign-up section .  Data Curators Guide : can t wait to get started as a Data Curator? You can dive straight in and start packaging datasets using the  core data curators guide .", 
            "title": "Core Data Curators"
        }, 
        {
            "location": "/publishers/core-datasets/#what-roles-and-skills-are-needed", 
            "text": "We have a variety of roles from identifying new  core  datasets, to collecting and packaging the data, to performing quality control.  Core Skills    at least one of these skills is strongly recommended:   Data Wrangling Experience . Many of our source datasets are not complex (just an Excel file or similar) and can be  wrangled  in a Spreadsheet program. What we therefore recommend is at least one of:  Experience with a Spreadsheet application such as Excel or (preferably) Google Docs including use of formulas and (desirably) macros (you should at least know how you could quickly convert a cell containing  2014  to  2014-01-01  across 1000 rows)  Coding for data processing (especially scraping) in one or more of python, javascript, bash    Data sleuthing  - the ability to dig up data on the web (specific desirable skills: you know how to search by filetype in google, you know where the developer tools are in chrome or firefox, you know how to find the URL a form posts to)   Desirable Skills  (the more the better!):   Data vs Metadata: know difference between data and metadata  Familiarity with Git (and Github)  Familiarity with a command line (preferably bash)  Know what JSON is  Mac or Unix is your default operating system (will make access to relevant tools that much easier)  Knowledge of Web APIs and/or HTML  Use of curl or similar command line tool for accessing Web APIs or web pages  Scraping using a command line tool or (even better) by coding yourself  Know what a Data Package and a Tabular Data Package are  Know what a text editor is (e.g. notepad, textmate, vim, emacs,  ) and know how to use it (useful for both working with data and for editing Data Package metadata)", 
            "title": "What Roles and Skills are Needed"
        }, 
        {
            "location": "/publishers/core-datasets/#get-involved-sign-up-now", 
            "text": "Here s what you need to know when you sign up:   Time commitment : Members of the team commit to at least 8-16h per month (though this will be an average - if you are especially busy with other things one month and do less that is fine)  Schedule : There is no schedule so you can contribute at any time that is good for you - evenings, weekeneds, lunch-times etc  Location : all activity will be carried out online so you can be based anywhere in the world  Skills : see above   To register your interest fill in the following form. Any questions, please  get in touch directly .  Loading...", 
            "title": "Get Involved - Sign Up Now!"
        }, 
        {
            "location": "/publishers/core-data-curators/", 
            "text": "Core Data Curators Guide\n\n\nThis is a guide for curators of \ncore datasets\n. Curators collect and maintain these important and commonly-used (\u201ccore\u201d) datasets as high-quality, easy-to-use, open \nData Packages\n.\n\n\nQuick Links\n\n\n\n\nDiscussion forum\n - discussion takes place here by default\n\n\nThis is the place to ask questions, get help etc - just open a new topic\n\n\nIntroduction to Core Datasets Project\n\n\nJoin the Core Data Curators Team\n\n\nPackaging Queue (GitHub Issues Tracker)\n\n\nPublish Data Packages Howto on Frictionless Data Site\n\n\n\n\nQuick Start\n\n\n\n\nPlease take 2m to introduce yourself in the \ndiscussion forum\n so that other team members can get to know you\n\n\nRead the contributing guide below so you:\n\n\nunderstand the details of the curator workflow\n\n\ncan work out where you\nd like to contribute\n\n\n\n\n\n\nStop: have you read the contributing guide? The next items only make sense if you have!\n\n\nNow you can dive in with one or both of:\n\n\nResearching: start reviewing the \ncurrent queue\n - add new items, comment on existing ones etc\n\n\nPackaging:  check out the \n\u201cReady to Package\u201d\n section of the queue and assign yourself (drop a comment in the issue claiming it)\n\n\n\n\n\n\n\n\nContributor Guide\n\n\n\n\nFig 1: Overview of the Curation Workflow\n\n\nThere are 2 areas of activity:\n\n\n\n\nPreparing datasets as Core Data Packages - finding them, cleaning them, data-packaging them\n\n\nMaintaining Core Data Packages - keeping them up to date with the source dataset, handling changes, responding to user queries\n\n\n\n\nEach of these has sub-steps which we detail below and you can contribute in any and all of these.\n\n\nKey principles of our approach are that:\n\n\n\n\nWe package data rather than create it \u2013 our focus is to take source data and ensure it is of high quality and in a standard form\n\n\nWe preserve a clean separation between the data source, the data package and this registry \u2013 for example, data packages are stored in git repos hosted separately (preferably github)\n\n\n\n\nPreparing Datasets as Core Data Packages\n\n\nThere are different areas where people can contribute:\n\n\n\n\nResearch\n\n\nPackaging up data\n\n\nQuality assurance\n\n\nFinal Publication into the official core datasets list\n\n\n\n\nOften you will contribute in all 4 by taking a dataset all the way from a suggestion to a fully packaged data package published online.\n\n\n1. Research\n\n\nThis involves researching and selecting datasets as core datasets and adding them to the queue for packaging - no coding or data wrangling skill is needed for this\n\n\n\n\nTo propose a dataset for addition you \nopen an issue in the Registry\n with the details of the proposed dataset.\n\n\nIdentify relevant source or sources for the dataset\n\n\nTo propose a dataset you do not have to know where to get the data from (e.g. you could suggest \u201cUS GDP\u201d as a core dataset without yet knowing where to get the data from)\n\n\nDiscuss with Queue Manager(s) (they will spot your submission and start commenting in the GitHub issue)\n\n\nIf good =\n Shortlist for Packaging - add \nLabel \u201cStatus: Ready to Package\u201d\n\n\n\n\n2. Packaging up data\n\n\nOnce we have a suggested dataset marked as \nready to package\n we can move to packaging it up.\n\n\nHow to package up data is covered in the \ngeneral publishing guide\n.\n\n\n3. Quality Assurance\n\n\nThis involves validating and checking packaged datasets to ensure they are of high quality and ready to publish.\n\n\n\n\nValidate\n the Data Package and \nreview\n the data in the Data Package.\n\n\nIn the review phase, you should be looking at a table with the data you have input before. That will ensure your data package is working without any issues and that it follows the same quality standards that any other package.\n\n\nPost a validation link and a view link in the comments for the issue in the Registry related to your Data Package.\n\n\n\n\n4. Publishing\n\n\nWe have a few extra specific requirements:\n\n\n\n\nAll Data Packages must (ultimately) be stored in a public GitHub repo\n\n\nFirst publish to your own repository\n\n\nThen arrange a move the repository to \ngithub.com/datasets/ organization\n - as the owner of a repository you can initiate a transfer request to github.com/datasets/ which can then be approved\n\n\nAdd to the \ncatalog list\n \nand\n the \ncore list\n \nand\n the associated csv files: \ncatalog-list.csv\n and \ncore-list.csv\n.\n\n\nReload \nhttp://data.okfn.org/data/\n by visiting \nhttp://data.okfn.org/admin/reload/\n\n\nIf you have access, tweet from the @OKFNLabs account a link to the \nhttp://data.okfn.org/data/\n page for the dataset.\n\n\n\n\nMaintaining Data Packages\n\n\nMany data packages package data that changes over time - for example, many time series get updated monthly or daily.\n\n\nWe need people to become the \nmaintainer\n for a given dataset and keep it up to date by regularly adding in the new data.\n\n\nList of datasets needing a maintainer\n\n\nCore Data Assessment Criteria\n\n\nFor a dataset to be designated as \ncore\n it should meet the following criteria:\n\n\n\n\nQuality - the dataset must be well structured\n\n\nRelevance and importance - the focus at present is on indicators and reference data\n\n\nOngoing support - it should have a maintainer\n\n\nOpenness - data should be \nopen data\n and openly licensed in accordance with the \nOpen Definition\n\n\n\n\n\n\nGuide for Managing Curators\n\n\nIntro Email for New Joiners\n\n\nHi,\n\nWe are delighted to welcome you to the Core Data Curators team of crack data curators.\n\nTo kick-off your core data curatorship we invite you to:\n\n1. Introduce yourself in the forum here:\n\n    http://discuss.okfn.org/t/core-data-curators-introductions/145/24\n\n2. Take a look at the Core Data Curators guide:\n\n    http://docs.datahub.io/publishers/core-data-curators\n\nRegards,\n\nXXX", 
            "title": "Core Data Curators"
        }, 
        {
            "location": "/publishers/core-data-curators/#core-data-curators-guide", 
            "text": "This is a guide for curators of  core datasets . Curators collect and maintain these important and commonly-used (\u201ccore\u201d) datasets as high-quality, easy-to-use, open  Data Packages .", 
            "title": "Core Data Curators Guide"
        }, 
        {
            "location": "/publishers/core-data-curators/#quick-links", 
            "text": "Discussion forum  - discussion takes place here by default  This is the place to ask questions, get help etc - just open a new topic  Introduction to Core Datasets Project  Join the Core Data Curators Team  Packaging Queue (GitHub Issues Tracker)  Publish Data Packages Howto on Frictionless Data Site", 
            "title": "Quick Links"
        }, 
        {
            "location": "/publishers/core-data-curators/#quick-start", 
            "text": "Please take 2m to introduce yourself in the  discussion forum  so that other team members can get to know you  Read the contributing guide below so you:  understand the details of the curator workflow  can work out where you d like to contribute    Stop: have you read the contributing guide? The next items only make sense if you have!  Now you can dive in with one or both of:  Researching: start reviewing the  current queue  - add new items, comment on existing ones etc  Packaging:  check out the  \u201cReady to Package\u201d  section of the queue and assign yourself (drop a comment in the issue claiming it)", 
            "title": "Quick Start"
        }, 
        {
            "location": "/publishers/core-data-curators/#contributor-guide", 
            "text": "Fig 1: Overview of the Curation Workflow  There are 2 areas of activity:   Preparing datasets as Core Data Packages - finding them, cleaning them, data-packaging them  Maintaining Core Data Packages - keeping them up to date with the source dataset, handling changes, responding to user queries   Each of these has sub-steps which we detail below and you can contribute in any and all of these.  Key principles of our approach are that:   We package data rather than create it \u2013 our focus is to take source data and ensure it is of high quality and in a standard form  We preserve a clean separation between the data source, the data package and this registry \u2013 for example, data packages are stored in git repos hosted separately (preferably github)", 
            "title": "Contributor Guide"
        }, 
        {
            "location": "/publishers/core-data-curators/#preparing-datasets-as-core-data-packages", 
            "text": "There are different areas where people can contribute:   Research  Packaging up data  Quality assurance  Final Publication into the official core datasets list   Often you will contribute in all 4 by taking a dataset all the way from a suggestion to a fully packaged data package published online.", 
            "title": "Preparing Datasets as Core Data Packages"
        }, 
        {
            "location": "/publishers/core-data-curators/#1-research", 
            "text": "This involves researching and selecting datasets as core datasets and adding them to the queue for packaging - no coding or data wrangling skill is needed for this   To propose a dataset for addition you  open an issue in the Registry  with the details of the proposed dataset.  Identify relevant source or sources for the dataset  To propose a dataset you do not have to know where to get the data from (e.g. you could suggest \u201cUS GDP\u201d as a core dataset without yet knowing where to get the data from)  Discuss with Queue Manager(s) (they will spot your submission and start commenting in the GitHub issue)  If good =  Shortlist for Packaging - add  Label \u201cStatus: Ready to Package\u201d", 
            "title": "1. Research"
        }, 
        {
            "location": "/publishers/core-data-curators/#2-packaging-up-data", 
            "text": "Once we have a suggested dataset marked as  ready to package  we can move to packaging it up.  How to package up data is covered in the  general publishing guide .", 
            "title": "2. Packaging up data"
        }, 
        {
            "location": "/publishers/core-data-curators/#3-quality-assurance", 
            "text": "This involves validating and checking packaged datasets to ensure they are of high quality and ready to publish.   Validate  the Data Package and  review  the data in the Data Package.  In the review phase, you should be looking at a table with the data you have input before. That will ensure your data package is working without any issues and that it follows the same quality standards that any other package.  Post a validation link and a view link in the comments for the issue in the Registry related to your Data Package.", 
            "title": "3. Quality Assurance"
        }, 
        {
            "location": "/publishers/core-data-curators/#4-publishing", 
            "text": "We have a few extra specific requirements:   All Data Packages must (ultimately) be stored in a public GitHub repo  First publish to your own repository  Then arrange a move the repository to  github.com/datasets/ organization  - as the owner of a repository you can initiate a transfer request to github.com/datasets/ which can then be approved  Add to the  catalog list   and  the  core list   and  the associated csv files:  catalog-list.csv  and  core-list.csv .  Reload  http://data.okfn.org/data/  by visiting  http://data.okfn.org/admin/reload/  If you have access, tweet from the @OKFNLabs account a link to the  http://data.okfn.org/data/  page for the dataset.", 
            "title": "4. Publishing"
        }, 
        {
            "location": "/publishers/core-data-curators/#maintaining-data-packages", 
            "text": "Many data packages package data that changes over time - for example, many time series get updated monthly or daily.  We need people to become the  maintainer  for a given dataset and keep it up to date by regularly adding in the new data.  List of datasets needing a maintainer", 
            "title": "Maintaining Data Packages"
        }, 
        {
            "location": "/publishers/core-data-curators/#core-data-assessment-criteria", 
            "text": "For a dataset to be designated as  core  it should meet the following criteria:   Quality - the dataset must be well structured  Relevance and importance - the focus at present is on indicators and reference data  Ongoing support - it should have a maintainer  Openness - data should be  open data  and openly licensed in accordance with the  Open Definition", 
            "title": "Core Data Assessment Criteria"
        }, 
        {
            "location": "/publishers/core-data-curators/#guide-for-managing-curators", 
            "text": "", 
            "title": "Guide for Managing Curators"
        }, 
        {
            "location": "/publishers/core-data-curators/#intro-email-for-new-joiners", 
            "text": "Hi,\n\nWe are delighted to welcome you to the Core Data Curators team of crack data curators.\n\nTo kick-off your core data curatorship we invite you to:\n\n1. Introduce yourself in the forum here:\n\n    http://discuss.okfn.org/t/core-data-curators-introductions/145/24\n\n2. Take a look at the Core Data Curators guide:\n\n    http://docs.datahub.io/publishers/core-data-curators\n\nRegards,\n\nXXX", 
            "title": "Intro Email for New Joiners"
        }, 
        {
            "location": "/publishers/cli/", 
            "text": "data\n - the DataHub CLI\n\n\n\n\n\n\ndata - the DataHub CLI\n\n\nGetting started\n\n\nInstallation\n\n\nInstalling binaries\n\n\nInstalling via npm\n\n\n\n\n\n\nUsage\n\n\npush\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\ndata\n is the command-line tool to prepare, push and get data from the DataHub. With \ndata\n you will be able to:\n\n\n\n\nPush data to the DataHub\n\n\nGet data from the DataHub\n\n\nGet information about particular data files and datasets (both on the DataHub and off)\n\n\nValidate your data to ensure its quality\n\n\n\n\nInstallation\n\n\nThere are two options for installation:\n\n\n\n\nInstalling pre-built binaries. These have no dependencies and will work \nout of the box\n\n\nInstall via npm: if you have node (\n= v7) and npm installed you can install via npm\n\n\n\n\nInstalling binaries\n\n\n\n\nGo to the \nreleases page\n\n\nDownload the pre-built binary for your platform (MacOS and LinuxOS x64 at present)\n\n\n\n\nMove the binary into your \n$PATH\n e.g. on Mac you could move to \n/usr/local/bin/\n\n\nmv data-{os-distribution} /usr/local/bin/data\n\n\n\n\n\n\n\nInstalling via npm\n\n\nYou can also install it from \nnpm\n as follows:\n\n\nnpm install -g datahub-cli\n\n\n\nUsage\n\n\nYou can see the latest commands and get help by doing:\n\n\ndata help\n\n\n\npush\n\n\nPutting data online is one command: \npush\n.\n\n\ndata push [FILE-or-DATA-PACKAGE-PATH]\n\n\n\nLogin befor pushing\n: you will need to login (or signup) before pushing:\n\n\ndata login\n\n\n\nThis will carry out login / signup entirely from the command line.", 
            "title": "CLI"
        }, 
        {
            "location": "/publishers/cli/#data-the-datahub-cli", 
            "text": "data - the DataHub CLI  Getting started  Installation  Installing binaries  Installing via npm    Usage  push", 
            "title": "data - the DataHub CLI"
        }, 
        {
            "location": "/publishers/cli/#getting-started", 
            "text": "data  is the command-line tool to prepare, push and get data from the DataHub. With  data  you will be able to:   Push data to the DataHub  Get data from the DataHub  Get information about particular data files and datasets (both on the DataHub and off)  Validate your data to ensure its quality", 
            "title": "Getting started"
        }, 
        {
            "location": "/publishers/cli/#installation", 
            "text": "There are two options for installation:   Installing pre-built binaries. These have no dependencies and will work  out of the box  Install via npm: if you have node ( = v7) and npm installed you can install via npm", 
            "title": "Installation"
        }, 
        {
            "location": "/publishers/cli/#installing-binaries", 
            "text": "Go to the  releases page  Download the pre-built binary for your platform (MacOS and LinuxOS x64 at present)   Move the binary into your  $PATH  e.g. on Mac you could move to  /usr/local/bin/  mv data-{os-distribution} /usr/local/bin/data", 
            "title": "Installing binaries"
        }, 
        {
            "location": "/publishers/cli/#installing-via-npm", 
            "text": "You can also install it from  npm  as follows:  npm install -g datahub-cli", 
            "title": "Installing via npm"
        }, 
        {
            "location": "/publishers/cli/#usage", 
            "text": "You can see the latest commands and get help by doing:  data help", 
            "title": "Usage"
        }, 
        {
            "location": "/publishers/cli/#push", 
            "text": "Putting data online is one command:  push .  data push [FILE-or-DATA-PACKAGE-PATH]  Login befor pushing : you will need to login (or signup) before pushing:  data login  This will carry out login / signup entirely from the command line.", 
            "title": "push"
        }, 
        {
            "location": "/publishers/views/", 
            "text": "Views\n\n\n\n\n\n\nViews\n\n\nIntroduction\n\n\nFeatures\n\n\nHow it works\n\n\nExamples\n\n\nSimple graph spec\n\n\nVega graphs\n\n\nMaps\n\n\nTables and Transforms\n\n\n\n\n\n\n\n\n\n\nMore Information\n\n\n\n\n\n\nIntroduction\n\n\nProducers and consumers of data want to have data presented in tables and graphs \n \nviews\n on the data. They want this for a range of reasons, from simple eyeballing to drawing out key insights,\n\n\n\ngraph LR\n  data[Your Data] --> table[Table]\n  data --> grap[Graph]\n  data --> geo[Map]\n\n\n\n\nThis guide shows you how you can create beautiful views for your data on the DataHub.\n\n\nFeatures\n\n\n\n\nSimple things are simple: adding a bar chart or line chart is fast and easy \n seconds to do and requiring minimal knowledge. Views are created in a simple declarative syntax\n\n\nPowerful and extensible: complex and powerful graphing is also powerful.\n\n\nReuse: leverage the power of existing specs like [Vega][] (and tools like Vega and Plotly)\n\n\nTransform your data prior to visualization\n\n\nComposable: the views spec is independent but composable with other data package specs (and even usable on its own)\n\n\n\n\nHow it works\n\n\nYou can add a view for your data simply by describing your view in a simple declarative syntax (in JSON) and then adding to the \ndatapackage.json\n for your dataset.\n\n\nFor example, suppose you have data in a csv that looks like this:\n\n\n\n\n\n\n\n\nx\n\n\ny\n\n\nz\n\n\n\n\n\n\n\n\n\n\n1\n\n\n8\n\n\n5\n\n\n\n\n\n\n2\n\n\n9\n\n\n7\n\n\n\n\n\n\n\n\nThen you could describe your view like this:\n\n\n{\n  \ntype\n: \nline\n,\n  \ngroup\n: \nx\n,\n  \nseries\n: [ \ny\n, \nz\n ]\n}\n\n\n\nFinally, you need to connect your view with the underlying data source in the \ndatapackage.json\n\n\n...\n\nresources\n: [{\n    \nname\n: \nmydata\n\n    \npath\n: \nmydata.csv\n,\n    \nschema\n: ... schema for the \n}]\nviews: [{\n    \nname\n: \ngraph-1\n,\n    \ntitle\n: \nMy awesome view\n\n\n    // the data to connect to this view\n    \nresources\n: [\nmydata\n]\n\n    // specType here is optional as simple is the default\n    \nspecType\n: \nsimple\n\n    \nspec\n: {\n        \ntype\n: \nline\n,\n        \ngroup\n: \nx\n,\n        \nseries\n: [ \ny\n, \nz\n ]\n    }\n}}\n\n\n\nTo learn more see the examples live examples below.\n\n\nExamples\n\n\nIn this section, examples of using Data Package views are provided. Each example has a README section with small tutorial.\n\n\nSimple graph spec\n\n\nSimple graph spec is the easiest and quickest way to specify a view in a Data Package. Using simple graph spec publishers can generate graphs, e.g., line and bar charts.\n\n\n\n\nSimple Graph Spec Tutorial\n\n\n\n\nVega graphs\n\n\nPublishers can also describe graphs using Vega specifications:\n\n\n\n\nVega Graph Spec Tutorial - Yields of Barley\n\n\nVega Graph Spec Tutorial - US presidents\n\n\nVega Graph Spec Tutorial - US Airports\n\n\n\n\nMaps\n\n\nAt the moment, we only support \n.geojson\n format:\n\n\n\n\nGeoJSON Tutorial\n\n\n\n\nTables and Transforms\n\n\nIn the following examples, we demonstrate how transforms can be used in Data Package views. Transformed data will be displayed as table views.\n\n\n\n\nFilter \n Formula\n\n\nSample\n\n\nAggregate\n\n\n\n\nMore Information\n\n\n\n\nDetails of how the tooling works are in Views Developer guide\n\n\nViews spec (draft)", 
            "title": "Views"
        }, 
        {
            "location": "/publishers/views/#views", 
            "text": "Views  Introduction  Features  How it works  Examples  Simple graph spec  Vega graphs  Maps  Tables and Transforms      More Information", 
            "title": "Views"
        }, 
        {
            "location": "/publishers/views/#introduction", 
            "text": "Producers and consumers of data want to have data presented in tables and graphs    views  on the data. They want this for a range of reasons, from simple eyeballing to drawing out key insights,  \ngraph LR\n  data[Your Data] --> table[Table]\n  data --> grap[Graph]\n  data --> geo[Map]  This guide shows you how you can create beautiful views for your data on the DataHub.", 
            "title": "Introduction"
        }, 
        {
            "location": "/publishers/views/#features", 
            "text": "Simple things are simple: adding a bar chart or line chart is fast and easy   seconds to do and requiring minimal knowledge. Views are created in a simple declarative syntax  Powerful and extensible: complex and powerful graphing is also powerful.  Reuse: leverage the power of existing specs like [Vega][] (and tools like Vega and Plotly)  Transform your data prior to visualization  Composable: the views spec is independent but composable with other data package specs (and even usable on its own)", 
            "title": "Features"
        }, 
        {
            "location": "/publishers/views/#how-it-works", 
            "text": "You can add a view for your data simply by describing your view in a simple declarative syntax (in JSON) and then adding to the  datapackage.json  for your dataset.  For example, suppose you have data in a csv that looks like this:     x  y  z      1  8  5    2  9  7     Then you could describe your view like this:  {\n   type :  line ,\n   group :  x ,\n   series : [  y ,  z  ]\n}  Finally, you need to connect your view with the underlying data source in the  datapackage.json  ... resources : [{\n     name :  mydata \n     path :  mydata.csv ,\n     schema : ... schema for the \n}]\nviews: [{\n     name :  graph-1 ,\n     title :  My awesome view \n\n    // the data to connect to this view\n     resources : [ mydata ]\n\n    // specType here is optional as simple is the default\n     specType :  simple \n     spec : {\n         type :  line ,\n         group :  x ,\n         series : [  y ,  z  ]\n    }\n}}  To learn more see the examples live examples below.", 
            "title": "How it works"
        }, 
        {
            "location": "/publishers/views/#examples", 
            "text": "In this section, examples of using Data Package views are provided. Each example has a README section with small tutorial.", 
            "title": "Examples"
        }, 
        {
            "location": "/publishers/views/#simple-graph-spec", 
            "text": "Simple graph spec is the easiest and quickest way to specify a view in a Data Package. Using simple graph spec publishers can generate graphs, e.g., line and bar charts.   Simple Graph Spec Tutorial", 
            "title": "Simple graph spec"
        }, 
        {
            "location": "/publishers/views/#vega-graphs", 
            "text": "Publishers can also describe graphs using Vega specifications:   Vega Graph Spec Tutorial - Yields of Barley  Vega Graph Spec Tutorial - US presidents  Vega Graph Spec Tutorial - US Airports", 
            "title": "Vega graphs"
        }, 
        {
            "location": "/publishers/views/#maps", 
            "text": "At the moment, we only support  .geojson  format:   GeoJSON Tutorial", 
            "title": "Maps"
        }, 
        {
            "location": "/publishers/views/#tables-and-transforms", 
            "text": "In the following examples, we demonstrate how transforms can be used in Data Package views. Transformed data will be displayed as table views.   Filter   Formula  Sample  Aggregate", 
            "title": "Tables and Transforms"
        }, 
        {
            "location": "/publishers/views/#more-information", 
            "text": "Details of how the tooling works are in Views Developer guide  Views spec (draft)", 
            "title": "More Information"
        }
    ]
}