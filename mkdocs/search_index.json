{
    "docs": [
        {
            "location": "/", 
            "text": "DataHub Documentation\n\n\nWelcome to the DataHub documentation. Choose the appropriate section and dive right in!\n\n\nSections\n\n\n\n\nFor developers\n: \n3 Python, JavaScript and data pipelines? Start here!\n\n\nFor publishers\n: Want to store your data on DataHub? Start here!", 
            "title": "Home"
        }, 
        {
            "location": "/#datahub-documentation", 
            "text": "Welcome to the DataHub documentation. Choose the appropriate section and dive right in!", 
            "title": "DataHub Documentation"
        }, 
        {
            "location": "/#sections", 
            "text": "For developers :  3 Python, JavaScript and data pipelines? Start here!  For publishers : Want to store your data on DataHub? Start here!", 
            "title": "Sections"
        }, 
        {
            "location": "/developers/", 
            "text": "Developers\n\n\nThis section of the DataHub documentation is for developers. Here you can learn about the design of the platform and how to get DataHub running locally or on your own servers, and the process for contributing enhancements and bug fixes to the code.\n\n\n\n\nWe use following repositories on GitHub and GitLab for our platform:\n\n\n\n\nDPR API\n - API and web-application\n\n\nDPR DEPLOY\n - Automated deployment of application\n\n\nDPR JS\n - Visualizations and front-end JS\n\n\nDPR DOCS\n - Documentations\n\n\n\n\n\ngraph LR\n\nsubgraph Repos\n  dprapi[DPR API]\n  dprjs[DPR JS]\n  dprdeploy[DPR Deploy]\n  dprdocs[DPR Docs]\n  dprjs --submodule--> dprapi\nend\n\nsubgraph Sites\n  dhio[xxx.datapackaged.com]\n  dhdocs[docs.datapackaged.com]\n  dprdocs --> dhdocs\nend\n\ndeploy((Deploy))\ndprapi --> deploy\ndeploy --> dhio\ndprdeploy --> deploy\n\n\n\n\nInstall\n\n\nTo install the platform locally for development purposes, please follow the instructions here:\n\n\nhttps://github.com/frictionlessdata/dpr-api\n\n\nDeploy\n\n\nFor deployment of the application in a production environment, please see \nthe deploy page\n.\n\n\nAuthorization\n\n\nThe authorization set up enables system to restricts user permission to execute.\n\n\nAuthorization docs\n\n\nAuthentication\n\n\nSome DataHub API methods require client to provide user identity. API Client can use JWT token to perform authenticated requests.\n\n\nAuthentication docs\n\n\nCLI\n\n\nThe DataHub CLI (dpm) is a Python lib and command line interface to interact with an DataHub instance.\n\n\nCLI code", 
            "title": "Getting started"
        }, 
        {
            "location": "/developers/#developers", 
            "text": "This section of the DataHub documentation is for developers. Here you can learn about the design of the platform and how to get DataHub running locally or on your own servers, and the process for contributing enhancements and bug fixes to the code.   We use following repositories on GitHub and GitLab for our platform:   DPR API  - API and web-application  DPR DEPLOY  - Automated deployment of application  DPR JS  - Visualizations and front-end JS  DPR DOCS  - Documentations   \ngraph LR\n\nsubgraph Repos\n  dprapi[DPR API]\n  dprjs[DPR JS]\n  dprdeploy[DPR Deploy]\n  dprdocs[DPR Docs]\n  dprjs --submodule--> dprapi\nend\n\nsubgraph Sites\n  dhio[xxx.datapackaged.com]\n  dhdocs[docs.datapackaged.com]\n  dprdocs --> dhdocs\nend\n\ndeploy((Deploy))\ndprapi --> deploy\ndeploy --> dhio\ndprdeploy --> deploy", 
            "title": "Developers"
        }, 
        {
            "location": "/developers/#install", 
            "text": "To install the platform locally for development purposes, please follow the instructions here:  https://github.com/frictionlessdata/dpr-api", 
            "title": "Install"
        }, 
        {
            "location": "/developers/#deploy", 
            "text": "For deployment of the application in a production environment, please see  the deploy page .", 
            "title": "Deploy"
        }, 
        {
            "location": "/developers/#authorization", 
            "text": "The authorization set up enables system to restricts user permission to execute.  Authorization docs", 
            "title": "Authorization"
        }, 
        {
            "location": "/developers/#authentication", 
            "text": "Some DataHub API methods require client to provide user identity. API Client can use JWT token to perform authenticated requests.  Authentication docs", 
            "title": "Authentication"
        }, 
        {
            "location": "/developers/#cli", 
            "text": "The DataHub CLI (dpm) is a Python lib and command line interface to interact with an DataHub instance.  CLI code", 
            "title": "CLI"
        }, 
        {
            "location": "/developers/platform/", 
            "text": "Platform\n\n\nThe DataHub platform has been designed as a set of loosely coupled components, each performing distinct functions related to the platform as a whole.\n\n\n\n\nArchitecture\n\n\nDomain Model\n\n\nProfile\n\n\nPackage\n\n\n\n\n\n\n\n\nArchitecture\n\n\n\ngraph TD\n\nsubgraph Web Frontend\n  frontend[Frontend Webapp]\n  browse[Browse \n&\n Search]\n  login[Login \n&\n Signup]\n  view[Views Renderer]\n  frontend --> browse\n  frontend --> login\nend\n\nsubgraph Users and Permissions\n  user[User]\n  permissions[Permissions]\n  authapi[Auth API]\n  authzapi[Authorization API]\n  login --> authapi\n  authapi --> user\n  authzapi --> permissions\nend\n\nsubgraph BitStore\n  bitstore[\"BitStore (S3)\"]\n  bitstoreapi[BitStore API\nput,get]\n  bitstoreapi --> bitstore\n  browse --> bitstoreapi\nend\n\nsubgraph MetaStore\n  metastore[\"MetaStore (RDS)\"]\n  metaapi[MetaStore API\nread,search,import]\n  metaapi --> metastore\n  browse --> metaapi\nend\n\nsubgraph CLI\n  cli[CLI]\nend\n\n\n\n\n\n\nCLI\n - Command Line Interface for publishing \nData Packages\n\n\nFront-end Web Application\n - Core part of platform - API, Login \n Sign-Up and Browse \n Search (page not yet implemented)\n\n\nViews and Renderer\n - JS Library responsible for visualization and views on platform\n\n\n\n\nBitStore\n\n\nWe are preserving the data byte by byte.\n\n\n\n\nWe use AWS S3 instance for storing data\n\n\nWe use the following URL structure on S3: \nbits.{base-domain}.metadata/{publisher}/{data-package}/_v/{version}/data/{resource-name}.csv\n\n\n\n\nMetaStore\n\n\nThe MetaStore stores Data Package meta-data along with other management information like publishers, users and permissions.\n\n\nWe use AWS RDS Postgresql database for storing meta-data.\n\n\nUsers and Permissions\n\n\n\n\nWe are using GitHub auth API for authenticating users on our platform. See more information on \nauthentication page\n\n\nWe have a standard access control matrix with 3 axes for authorization. See more information on\n\nauthorization page\n\n\n\n\nDomain model\n\n\nThere are two main concepts to understand in DataHub domain model - \nProfile\n and \nPackage\n\n\n\ngraph TD\n\npkg[Data Package]\nresource[Resource]\nfile[File]\nversion[Version]\nuser[User]\npublisher[Publisher]\n\nsubgraph Package\n  pkg --0..*--> resource\n  resource --1..*--> file\n  pkg --> version\nend\n\nsubgraph Profile\n  publisher --1..*--> user\n  publisher --0..*--> pkg\nend\n\n\n\n\nProfile\n\n\nSet of an authenticated and authorized entities like publishers and users. They are responsible for publishing, deleting or maintaining data on platform.\n\n\nImportant:\n Users do not have Data Packages, Publishers do. Users are \nmembers\n of Publishers.\n\n\nPublisher\n\n\nPublisher is an organization which \"owns\" Data Packages. Publisher may have zero or more Data Packages. Publisher may also have one or more user.\n\n\nUser\n\n\nUser is an authenticated entity, that is member of Publisher organization, that can read, edit, create or delete data packages depending on their permissions.\n\n\nPackage\n\n\nSet of Data Packages published under publisher name.\n\n\nData Package\n\n\nA Data Package is a simple way of \u201cpackaging\u201d up and describing data so that it can be easily shared and used. You can imagine as collection of data and and it's meta-data (\ndatapackage.json\n), usually covering some concrete topic Eg: \n\"Gold Prices\"\n or \n\"Population Growth Rate In My country\"\n etc.\n\n\nEach Data Package may have zero or more resources and one or more versions.\n\n\nResources\n - think like \"tables\" - Each can map to one or more physical files (usually just one). Think of a data table split into multiple CSV files on disk.\n\n\nVersion of a Data Package\n - similar to git commits and tags. People can mean different things by a \"Version\":\n\n\n\n\nTag - Same as label or version - a nice human usable label e.g. \n\"v0.3\"\n, \n\"master\"\n, \n\"2013\"\n\n\nCommit/Hash - Corresponds to the hash of datapackage.json, with that datapackage.json including all hashes of all data files\n\n\n\n\nWe interpret Version as \n\"Tag\"\n concept. \n\"Commit/Hash\"\n is not supported", 
            "title": "Platform"
        }, 
        {
            "location": "/developers/platform/#platform", 
            "text": "The DataHub platform has been designed as a set of loosely coupled components, each performing distinct functions related to the platform as a whole.   Architecture  Domain Model  Profile  Package", 
            "title": "Platform"
        }, 
        {
            "location": "/developers/platform/#architecture", 
            "text": "graph TD\n\nsubgraph Web Frontend\n  frontend[Frontend Webapp]\n  browse[Browse  &  Search]\n  login[Login  &  Signup]\n  view[Views Renderer]\n  frontend --> browse\n  frontend --> login\nend\n\nsubgraph Users and Permissions\n  user[User]\n  permissions[Permissions]\n  authapi[Auth API]\n  authzapi[Authorization API]\n  login --> authapi\n  authapi --> user\n  authzapi --> permissions\nend\n\nsubgraph BitStore\n  bitstore[\"BitStore (S3)\"]\n  bitstoreapi[BitStore API put,get]\n  bitstoreapi --> bitstore\n  browse --> bitstoreapi\nend\n\nsubgraph MetaStore\n  metastore[\"MetaStore (RDS)\"]\n  metaapi[MetaStore API read,search,import]\n  metaapi --> metastore\n  browse --> metaapi\nend\n\nsubgraph CLI\n  cli[CLI]\nend   CLI  - Command Line Interface for publishing  Data Packages  Front-end Web Application  - Core part of platform - API, Login   Sign-Up and Browse   Search (page not yet implemented)  Views and Renderer  - JS Library responsible for visualization and views on platform", 
            "title": "Architecture"
        }, 
        {
            "location": "/developers/platform/#bitstore", 
            "text": "We are preserving the data byte by byte.   We use AWS S3 instance for storing data  We use the following URL structure on S3:  bits.{base-domain}.metadata/{publisher}/{data-package}/_v/{version}/data/{resource-name}.csv", 
            "title": "BitStore"
        }, 
        {
            "location": "/developers/platform/#metastore", 
            "text": "The MetaStore stores Data Package meta-data along with other management information like publishers, users and permissions.  We use AWS RDS Postgresql database for storing meta-data.", 
            "title": "MetaStore"
        }, 
        {
            "location": "/developers/platform/#users-and-permissions", 
            "text": "We are using GitHub auth API for authenticating users on our platform. See more information on  authentication page  We have a standard access control matrix with 3 axes for authorization. See more information on authorization page", 
            "title": "Users and Permissions"
        }, 
        {
            "location": "/developers/platform/#domain-model", 
            "text": "There are two main concepts to understand in DataHub domain model -  Profile  and  Package  \ngraph TD\n\npkg[Data Package]\nresource[Resource]\nfile[File]\nversion[Version]\nuser[User]\npublisher[Publisher]\n\nsubgraph Package\n  pkg --0..*--> resource\n  resource --1..*--> file\n  pkg --> version\nend\n\nsubgraph Profile\n  publisher --1..*--> user\n  publisher --0..*--> pkg\nend", 
            "title": "Domain model"
        }, 
        {
            "location": "/developers/platform/#profile", 
            "text": "Set of an authenticated and authorized entities like publishers and users. They are responsible for publishing, deleting or maintaining data on platform.  Important:  Users do not have Data Packages, Publishers do. Users are  members  of Publishers.", 
            "title": "Profile"
        }, 
        {
            "location": "/developers/platform/#publisher", 
            "text": "Publisher is an organization which \"owns\" Data Packages. Publisher may have zero or more Data Packages. Publisher may also have one or more user.", 
            "title": "Publisher"
        }, 
        {
            "location": "/developers/platform/#user", 
            "text": "User is an authenticated entity, that is member of Publisher organization, that can read, edit, create or delete data packages depending on their permissions.", 
            "title": "User"
        }, 
        {
            "location": "/developers/platform/#package", 
            "text": "Set of Data Packages published under publisher name.", 
            "title": "Package"
        }, 
        {
            "location": "/developers/platform/#data-package", 
            "text": "A Data Package is a simple way of \u201cpackaging\u201d up and describing data so that it can be easily shared and used. You can imagine as collection of data and and it's meta-data ( datapackage.json ), usually covering some concrete topic Eg:  \"Gold Prices\"  or  \"Population Growth Rate In My country\"  etc.  Each Data Package may have zero or more resources and one or more versions.  Resources  - think like \"tables\" - Each can map to one or more physical files (usually just one). Think of a data table split into multiple CSV files on disk.  Version of a Data Package  - similar to git commits and tags. People can mean different things by a \"Version\":   Tag - Same as label or version - a nice human usable label e.g.  \"v0.3\" ,  \"master\" ,  \"2013\"  Commit/Hash - Corresponds to the hash of datapackage.json, with that datapackage.json including all hashes of all data files   We interpret Version as  \"Tag\"  concept.  \"Commit/Hash\"  is not supported", 
            "title": "Data Package"
        }, 
        {
            "location": "/developers/deploy/", 
            "text": "DevOps - Production Deployment\n\n\nWe use various cloud services for the platform, for example AWS S3 and RDS for storing data and metadata, and the application runs on Heroku.\n\n\nWe have fully automated the deployment of the platform including the setup of all necessary services so that it is one commmand to deploy. Code and instructions here:\n\n\nhttps://gitlab.com/datopian/datahub-deploy\n\n\nBelow we provide a conceptual outline.\n\n\nOutline - Conceptually\n\n\n\ngraph TD\n\n  user[fa:fa-user User] --> frontend[Frontend]\n  frontend --> db[Database - RDS]\n  frontend --> bits[BitStore - S3]\n\n\n\n\nCurrent Structure\n\n\nThis diagram shows the current deployment architecture.\n\n\n\ngraph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> heroku\n  cloudflare --> bits\n  heroku[Heroku - Flask] --> rds[RDS Database]\n  heroku --> bits\n\n\n\n\nAWS - Old Structure\n\n\nWe are no longer using AWS in this way. However, we have kept this for historical purposes and in case we return to AWS\n\n\n\ngraph TD\n\n  user[fa:fa-user User] --> cloudfront[Cloudfront]\n  cloudfront --> apigateway[API Gateway]\n  apigateway --> lambda[AWS Lambda - Flask via Zappa]\n  cloudfront --> s3assets[S3 Assets]\n  lambda --> rds[RDS Database]\n  lambda --> bits[BitStore]\n  cloudfront --> bits", 
            "title": "Deploy"
        }, 
        {
            "location": "/developers/deploy/#devops-production-deployment", 
            "text": "We use various cloud services for the platform, for example AWS S3 and RDS for storing data and metadata, and the application runs on Heroku.  We have fully automated the deployment of the platform including the setup of all necessary services so that it is one commmand to deploy. Code and instructions here:  https://gitlab.com/datopian/datahub-deploy  Below we provide a conceptual outline.", 
            "title": "DevOps - Production Deployment"
        }, 
        {
            "location": "/developers/deploy/#outline-conceptually", 
            "text": "graph TD\n\n  user[fa:fa-user User] --> frontend[Frontend]\n  frontend --> db[Database - RDS]\n  frontend --> bits[BitStore - S3]", 
            "title": "Outline - Conceptually"
        }, 
        {
            "location": "/developers/deploy/#current-structure", 
            "text": "This diagram shows the current deployment architecture.  \ngraph TD\n\n  user[fa:fa-user User]\n  bits[BitStore]\n  cloudflare[Cloudflare]\n\n  user --> cloudflare\n  cloudflare --> heroku\n  cloudflare --> bits\n  heroku[Heroku - Flask] --> rds[RDS Database]\n  heroku --> bits", 
            "title": "Current Structure"
        }, 
        {
            "location": "/developers/deploy/#aws-old-structure", 
            "text": "We are no longer using AWS in this way. However, we have kept this for historical purposes and in case we return to AWS  \ngraph TD\n\n  user[fa:fa-user User] --> cloudfront[Cloudfront]\n  cloudfront --> apigateway[API Gateway]\n  apigateway --> lambda[AWS Lambda - Flask via Zappa]\n  cloudfront --> s3assets[S3 Assets]\n  lambda --> rds[RDS Database]\n  lambda --> bits[BitStore]\n  cloudfront --> bits", 
            "title": "AWS - Old Structure"
        }, 
        {
            "location": "/developers/api/", 
            "text": "DataHub API\n\n\nThe API provides a range of endpoints to query and manage the platform.\n\n\nYou can explore the API via the docs below.\n\n\n\n\n\n \n\n\n\n \n\n\n\n\nwindow.onload = function() {\n  // Build a system\n  const ui = SwaggerUIBundle({\n    url: \"https://staging.datapackaged.com/api/swagger.json\",\n    dom_id: '#swagger-ui',\n    validatorUrl : null,\n    presets: [\n      SwaggerUIBundle.presets.apis,\n      SwaggerUIStandalonePreset\n    ],\n    plugins: [\n      SwaggerUIBundle.plugins.DownloadUrl\n    ],\n    layout: \"StandaloneLayout\"\n  })\n\n  window.ui = ui\n}", 
            "title": "API"
        }, 
        {
            "location": "/developers/api/#datahub-api", 
            "text": "The API provides a range of endpoints to query and manage the platform.  You can explore the API via the docs below.         \nwindow.onload = function() {\n  // Build a system\n  const ui = SwaggerUIBundle({\n    url: \"https://staging.datapackaged.com/api/swagger.json\",\n    dom_id: '#swagger-ui',\n    validatorUrl : null,\n    presets: [\n      SwaggerUIBundle.presets.apis,\n      SwaggerUIStandalonePreset\n    ],\n    plugins: [\n      SwaggerUIBundle.plugins.DownloadUrl\n    ],\n    layout: \"StandaloneLayout\"\n  })\n\n  window.ui = ui\n}", 
            "title": "DataHub API"
        }, 
        {
            "location": "/developers/publish/", 
            "text": "Publish\n\n\nExplanation of DataHub publishing flow from client and back-end perspectives.\n\n\nClient Perspective\n\n\nPublishing flow takes the following steps and processes to communicate with DataHub API:\n\n\n\nsequenceDiagram\nUpload Agent CLI->>Upload Agent CLI: Check Data Package valid\nUpload Agent CLI-->>Auth(SSO): Get Session Token (Sends base auth)\nAuth(SSO)-->>Upload Agent CLI: session token\nUpload Agent CLI->>BitStore Upload Auth: Get BitStore Upload token [send session token]\nBitStore Upload Auth->>Auth(SSO): Check key / token\nAuth(SSO)->>BitStore Upload Auth: OK / Not OK\nBitStore Upload Auth->>Upload Agent CLI: S3 auth Token\nUpload Agent CLI->>Data Storage (S3 Raw): Send file plus token\nData Storage (S3 Raw)->>Upload Agent CLI: OK / Not OK\nUpload Agent CLI->>MetaData Storage API: Finalize (After all data uploaded)\nMetaData Storage API->>Upload Agent CLI: OK / Not OK\n\n\n\n\n\n\n\n\nUpload API - see \nPOST /api/package/upload\n in \npackage\n section of \nAPI\n\n\nAuthentication API - see \nPOST /api/auth/token\n in \nauth\n section of \nAPI\n. Read more \nabout authentication\n\n\n[Authorization API][authz] - see \nPOST /api/datastore/authorize\n in \npackage\n section of \nAPI\n. Read more \nabout authorization\n\n\n\n\nSee example \ncode snippet in dpm-py\n\n\n\n\nBack-end perspective\n\n\nDataHub Metadata and Data Flow\n\n\n\n\nPink = service we build\n\n\nBlue = external service\n\n\nDark gray = not yet implemented\n\n\n\n\n\ngraph TD\n\nuser[Publisher fa:fa-user]\nupload-api[\"Upload API (S3 API)\"]\nbitstore(Bitstore S3)\nmetaingestor[Meta Ingestor]\ndataingestor[Data Ingestor]\nmetastore(\"Metastore (RDS)\")\nreadapi[Read API]\ndataproxy[\"DataProxy (convert raw data to json on the fly)\"]\ndatastore[\"Datastore (RDS)\"]\ns3readapi[S3 Get API]\nreaduser[Consumer fa:fa-user]\n\nuser --s3 signed upload url--> upload-api\nupload-api --> bitstore\nbitstore --> metaingestor\nmetaingestor --> metastore\nmetastore --> readapi\nbitstore -.-> dataproxy\nbitstore -.-> dataingestor\ndataingestor -.-> datastore\ndatastore -.-> readapi\nbitstore --> s3readapi\ns3readapi --> readuser\ndataproxy -.-> readuser\nreadapi --> readuser\n\n  classDef extservice fill:lightblue,stroke:#333,stroke-width:4px;\n  classDef notimplemented fill:darkgrey,stroke:#bbb,stroke-width:1px;\n  classDef service fill:pink,stroke:#333,stroke-width:4px;\n  class datastore,dataingestor,dataproxy notimplemented;\n  class bitstore,metastore,s3readapi extservice;\n  class readapi service;\n\n\n\n\n\n\nAuthentication\n\n\nAuthorization\n\n\nMetastore\n\n\nBitStore", 
            "title": "Publish"
        }, 
        {
            "location": "/developers/publish/#publish", 
            "text": "Explanation of DataHub publishing flow from client and back-end perspectives.", 
            "title": "Publish"
        }, 
        {
            "location": "/developers/publish/#client-perspective", 
            "text": "Publishing flow takes the following steps and processes to communicate with DataHub API:  \nsequenceDiagram\nUpload Agent CLI->>Upload Agent CLI: Check Data Package valid\nUpload Agent CLI-->>Auth(SSO): Get Session Token (Sends base auth)\nAuth(SSO)-->>Upload Agent CLI: session token\nUpload Agent CLI->>BitStore Upload Auth: Get BitStore Upload token [send session token]\nBitStore Upload Auth->>Auth(SSO): Check key / token\nAuth(SSO)->>BitStore Upload Auth: OK / Not OK\nBitStore Upload Auth->>Upload Agent CLI: S3 auth Token\nUpload Agent CLI->>Data Storage (S3 Raw): Send file plus token\nData Storage (S3 Raw)->>Upload Agent CLI: OK / Not OK\nUpload Agent CLI->>MetaData Storage API: Finalize (After all data uploaded)\nMetaData Storage API->>Upload Agent CLI: OK / Not OK    Upload API - see  POST /api/package/upload  in  package  section of  API  Authentication API - see  POST /api/auth/token  in  auth  section of  API . Read more  about authentication  [Authorization API][authz] - see  POST /api/datastore/authorize  in  package  section of  API . Read more  about authorization   See example  code snippet in dpm-py", 
            "title": "Client Perspective"
        }, 
        {
            "location": "/developers/publish/#back-end-perspective", 
            "text": "DataHub Metadata and Data Flow   Pink = service we build  Blue = external service  Dark gray = not yet implemented   \ngraph TD\n\nuser[Publisher fa:fa-user]\nupload-api[\"Upload API (S3 API)\"]\nbitstore(Bitstore S3)\nmetaingestor[Meta Ingestor]\ndataingestor[Data Ingestor]\nmetastore(\"Metastore (RDS)\")\nreadapi[Read API]\ndataproxy[\"DataProxy (convert raw data to json on the fly)\"]\ndatastore[\"Datastore (RDS)\"]\ns3readapi[S3 Get API]\nreaduser[Consumer fa:fa-user]\n\nuser --s3 signed upload url--> upload-api\nupload-api --> bitstore\nbitstore --> metaingestor\nmetaingestor --> metastore\nmetastore --> readapi\nbitstore -.-> dataproxy\nbitstore -.-> dataingestor\ndataingestor -.-> datastore\ndatastore -.-> readapi\nbitstore --> s3readapi\ns3readapi --> readuser\ndataproxy -.-> readuser\nreadapi --> readuser\n\n  classDef extservice fill:lightblue,stroke:#333,stroke-width:4px;\n  classDef notimplemented fill:darkgrey,stroke:#bbb,stroke-width:1px;\n  classDef service fill:pink,stroke:#333,stroke-width:4px;\n  class datastore,dataingestor,dataproxy notimplemented;\n  class bitstore,metastore,s3readapi extservice;\n  class readapi service;   Authentication  Authorization  Metastore  BitStore", 
            "title": "Back-end perspective"
        }, 
        {
            "location": "/developers/views/", 
            "text": "Views\n\n\nTo render Data Packages in browsers we use DataHub views written in JavaScript. The module implemented in ReactJS framework and it can render tables, maps and various graphs using third-party libraries.\n\n\n\n  graph TD\n\n  url[\"metadata URL passed from back-end\"]\n  dp-js[datapackage-js]\n  dprender[datapackage-render-js]\n  table[\"table view\"]\n  chart[\"graph view\"]\n  hot[HandsOnTable]\n  map[LeafletMap]\n  vega[Vega]\n  plotly[Plotly]\n  browser[Browser]\n\n  url --> dp-js\n  dp-js --fetched dp--> dprender\n  dprender --spec--> table\n  table --1..n--> hot\n  dprender --geojson--> map\n  dprender --spec--> chart\n  chart --0..n--> vega\n  chart --0..n--> plotly\n  hot --table--> browser\n  map --map--> browser\n  vega --graph--> browser\n  plotly --graph--> browser\n\n\n\n\nNotice that DataHub views render a table view per tabular resource. If GeoJSON resource is given, it renders a map. Graph views should be specified in \nviews\n property of a Data Package.\n\n\nLinks\n\n\n\n\ndpr-js repo\n\n\ndatapackage-render-js\n\n\nviews specification and analysis", 
            "title": "Views"
        }, 
        {
            "location": "/developers/views/#views", 
            "text": "To render Data Packages in browsers we use DataHub views written in JavaScript. The module implemented in ReactJS framework and it can render tables, maps and various graphs using third-party libraries.  \n  graph TD\n\n  url[\"metadata URL passed from back-end\"]\n  dp-js[datapackage-js]\n  dprender[datapackage-render-js]\n  table[\"table view\"]\n  chart[\"graph view\"]\n  hot[HandsOnTable]\n  map[LeafletMap]\n  vega[Vega]\n  plotly[Plotly]\n  browser[Browser]\n\n  url --> dp-js\n  dp-js --fetched dp--> dprender\n  dprender --spec--> table\n  table --1..n--> hot\n  dprender --geojson--> map\n  dprender --spec--> chart\n  chart --0..n--> vega\n  chart --0..n--> plotly\n  hot --table--> browser\n  map --map--> browser\n  vega --graph--> browser\n  plotly --graph--> browser  Notice that DataHub views render a table view per tabular resource. If GeoJSON resource is given, it renders a map. Graph views should be specified in  views  property of a Data Package.", 
            "title": "Views"
        }, 
        {
            "location": "/developers/views/#links", 
            "text": "dpr-js repo  datapackage-render-js  views specification and analysis", 
            "title": "Links"
        }, 
        {
            "location": "/developers/authorization/", 
            "text": "Authorization Set up\n\n\nAuthorization is the process of giving someone permission to do or have something. In multi-user systems, a system administrator defines for the system which users are allowed access to the system and what privileges of use.\n\n\nWe have a standard access control matrix with 3 axes:\n\n\n\n\nActions: CREATE, READ, WRITE, DELETE, PURGE etc. these can vary among different entities\n\n\nEntities (object): User, Publisher, Package, Package Resource, \u2026\n\n\nUsers: a user or type of user\n\n\n\n\nPermission is a tuple of \n(Users, Entities, Actions)\n\n\nIntroducing Roles\n\n\nIt can be tiresome and inefficient to list for every object all the users permitted to perform a given action. For example:\n\n\n\n\nMany users in an organization get same set of privileges because of their position in the organization.\n\n\nWe want to change the permissions associated with a certain level in the organization and to have those permissions changed for all people in that level\n\n\nA user may change level frequently (ex. user may get promoted)\n\n\n\n\nSo we create roles\n\n\n\n\nPer object roles e.g. Package Owner\n\n\nPer system roles e.g. System Administrator\n\n\nA list or algorithm for assigning Users =\n Roles\n\n\n\n\nAccess control algorithm:\n\nis_allowed(user, entity, action)\n\n\nFor this user: what roles do they have related to this entity and the system?\nGiven those roles: what actions do they have: UNIONrole\n\n\nNote: it would get more complex if some roles deny access. E.g. Role: Spammer might mean you are denied action to posting etc. Right now we don\u2019t have that issue.\n\n\nIs the desired action in that set?\n\n\nRoles\n\n\nThe example roles are given below.\n\n\n\n\nPackage\n\n\nOwner  =\n all actions\n\n\nEditor\n\n\nRead\n\n\nCreate\n\n\nDelete\n\n\nUndelete\n\n\nUpdate\n\n\nTag\n\n\n\n\n\n\nViewer  =\n Only read\n\n\n\n\n\n\nPublisher\n\n\nOwner =\n all actions on Publisher\n\n\nEditor\n\n\nViewMemberList\n\n\nAddMember\n\n\nRemoveMember\n\n\nRead\n\n\n\n\n\n\nViewer =\n Only Read\n\n\n\n\n\n\nSystem\n\n\nLoggedIn\n\n\nPackage::Create\n\n\nPublisher::Create\n\n\n\n\n\n\nAll =\n Package::Read on public packages\n\n\nSysadmin =\n all actions\n\n\n\n\n\n\n\n\nThis\n contains the current roles.\n\n\nBusiness roles\n\n\n\n\nPublisher Owner\n\n\nPublisher::Owner\n\n\n\n\n\n\nPublisher Member\n\n\nPublisher::Editor\n\n\n\n\n\n\n(Logged in) User\n\n\nSystem::LoggedIn\n\n\n\n\n\n\nSys Admin\n\n\nSystem::Sysadmin\n\n\n\n\n\n\nVisitor\n\n\nSystem::Anonymous\n\n\n\n\n\n\n\n\n\n\nNOTE: business roles and authorization roles are distinct. Of course, in implementing access control we will use the business logic inherent in business roles. However, business roles are not explicitly present in the access control system.\n\n\n\n\nActions\n\n\n\n\nNote: not an exhaustive list. \nThis\n contains the current Actions.\n\n\n\n\n\n\nPackage:\n\n\nPackage::Read\n\n\nPackage::Create\n\n\nPackage::Delete\n\n\nPackage::Undelete\n\n\nPackage::Purge\n\n\nPackage::Update\n\n\nPackage::Tag\n\n\n\n\n\n\nPublisher:\n\n\nPublisher::Create\n\n\nPublisher::AddMember\n\n\nPublisher::RemoveMember\n\n\nPublisher::Read\n\n\nPublisher::Delete\n\n\nPublisher::Update\n\n\nPublisher::ViewMemberList\n\n\n\n\n\n\n\n\nExamples\n\n\nFirst time visitor or not logged in:\n\n\nThe business role will be \nSystem::Anonymous\n. So the user can only has the action permission of \nPackage::Read\n.\nSo the user can only view the public data packages.\n\n\nLogged in user:\n\n\nThe business role will be \nSystem::LoggedIn\n . So the user will have permission of :\n\n\n\n\nPublisher::Create\n : The user can create new publisher.\n\n\nPackage::Create\n : The user can create new data package.\n\n\nPackage::Read\n : Can read public data packages", 
            "title": "Authorization"
        }, 
        {
            "location": "/developers/authorization/#authorization-set-up", 
            "text": "Authorization is the process of giving someone permission to do or have something. In multi-user systems, a system administrator defines for the system which users are allowed access to the system and what privileges of use.  We have a standard access control matrix with 3 axes:   Actions: CREATE, READ, WRITE, DELETE, PURGE etc. these can vary among different entities  Entities (object): User, Publisher, Package, Package Resource, \u2026  Users: a user or type of user   Permission is a tuple of  (Users, Entities, Actions)", 
            "title": "Authorization Set up"
        }, 
        {
            "location": "/developers/authorization/#introducing-roles", 
            "text": "It can be tiresome and inefficient to list for every object all the users permitted to perform a given action. For example:   Many users in an organization get same set of privileges because of their position in the organization.  We want to change the permissions associated with a certain level in the organization and to have those permissions changed for all people in that level  A user may change level frequently (ex. user may get promoted)   So we create roles   Per object roles e.g. Package Owner  Per system roles e.g. System Administrator  A list or algorithm for assigning Users =  Roles   Access control algorithm: is_allowed(user, entity, action)  For this user: what roles do they have related to this entity and the system?\nGiven those roles: what actions do they have: UNIONrole  Note: it would get more complex if some roles deny access. E.g. Role: Spammer might mean you are denied action to posting etc. Right now we don\u2019t have that issue.  Is the desired action in that set?", 
            "title": "Introducing Roles"
        }, 
        {
            "location": "/developers/authorization/#roles", 
            "text": "The example roles are given below.   Package  Owner  =  all actions  Editor  Read  Create  Delete  Undelete  Update  Tag    Viewer  =  Only read    Publisher  Owner =  all actions on Publisher  Editor  ViewMemberList  AddMember  RemoveMember  Read    Viewer =  Only Read    System  LoggedIn  Package::Create  Publisher::Create    All =  Package::Read on public packages  Sysadmin =  all actions     This  contains the current roles.", 
            "title": "Roles"
        }, 
        {
            "location": "/developers/authorization/#business-roles", 
            "text": "Publisher Owner  Publisher::Owner    Publisher Member  Publisher::Editor    (Logged in) User  System::LoggedIn    Sys Admin  System::Sysadmin    Visitor  System::Anonymous      NOTE: business roles and authorization roles are distinct. Of course, in implementing access control we will use the business logic inherent in business roles. However, business roles are not explicitly present in the access control system.", 
            "title": "Business roles"
        }, 
        {
            "location": "/developers/authorization/#actions", 
            "text": "Note: not an exhaustive list.  This  contains the current Actions.    Package:  Package::Read  Package::Create  Package::Delete  Package::Undelete  Package::Purge  Package::Update  Package::Tag    Publisher:  Publisher::Create  Publisher::AddMember  Publisher::RemoveMember  Publisher::Read  Publisher::Delete  Publisher::Update  Publisher::ViewMemberList", 
            "title": "Actions"
        }, 
        {
            "location": "/developers/authorization/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/developers/authorization/#first-time-visitor-or-not-logged-in", 
            "text": "The business role will be  System::Anonymous . So the user can only has the action permission of  Package::Read .\nSo the user can only view the public data packages.", 
            "title": "First time visitor or not logged in:"
        }, 
        {
            "location": "/developers/authorization/#logged-in-user", 
            "text": "The business role will be  System::LoggedIn  . So the user will have permission of :   Publisher::Create  : The user can create new publisher.  Package::Create  : The user can create new data package.  Package::Read  : Can read public data packages", 
            "title": "Logged in user:"
        }, 
        {
            "location": "/developers/authentication/", 
            "text": "DataHub Authentication\n\n\nThis page describes authentication of DataHub users. The details provided can be used by developers, willing to contribute to the existing \ndpm\n API client or implement custom client for The DataHub API.\n\n\nThe DataHub Frontend allows users to be registered via \ngithub\n using the web browser. After a successful registration, user will be given unique API-KEY to authenticate with DataHub API server.\n\n\nAPI authentication\n\n\nSome DataHub API methods require client to provide identity of a registered user. To prove its identity, client first has to obtain temporal JWT token, providing permanent API-KEY of a registered user. After that client can pass this token in the header of a request to the API.\n\n\nTo obtain a temporal JWT token, client should send POST request to \n/api/auth/token\n. Request should have json-encoded body with 'username' and 'secret' keys, where 'secret' is an API-KEY of the user:\n\n\nresponse = requests.post(\n        url='https://datapackaged.com/api/auth/token',\n        {'username': 'my_username', 'secret': '1dd5f984bc'}))\n\n\n\nIf the username and API-KEY are valid, server will return json response with JWT token: \n{'token': 'a6d8b887'}\n\n\nauth_token = response.json().get('token')\n\n\n\nThis token should be temporarily stored by the client. To access any API method with authentication, client should include this token in the \"Authorization\" header.\n\n\nrequests.post(api_url, headers={'Authorization', 'Bearer %s' % auth_token})", 
            "title": "Authentication"
        }, 
        {
            "location": "/developers/authentication/#datahub-authentication", 
            "text": "This page describes authentication of DataHub users. The details provided can be used by developers, willing to contribute to the existing  dpm  API client or implement custom client for The DataHub API.  The DataHub Frontend allows users to be registered via  github  using the web browser. After a successful registration, user will be given unique API-KEY to authenticate with DataHub API server.", 
            "title": "DataHub Authentication"
        }, 
        {
            "location": "/developers/authentication/#api-authentication", 
            "text": "Some DataHub API methods require client to provide identity of a registered user. To prove its identity, client first has to obtain temporal JWT token, providing permanent API-KEY of a registered user. After that client can pass this token in the header of a request to the API.  To obtain a temporal JWT token, client should send POST request to  /api/auth/token . Request should have json-encoded body with 'username' and 'secret' keys, where 'secret' is an API-KEY of the user:  response = requests.post(\n        url='https://datapackaged.com/api/auth/token',\n        {'username': 'my_username', 'secret': '1dd5f984bc'}))  If the username and API-KEY are valid, server will return json response with JWT token:  {'token': 'a6d8b887'}  auth_token = response.json().get('token')  This token should be temporarily stored by the client. To access any API method with authentication, client should include this token in the \"Authorization\" header.  requests.post(api_url, headers={'Authorization', 'Bearer %s' % auth_token})", 
            "title": "API authentication"
        }, 
        {
            "location": "/publishers/", 
            "text": "Publishers\n\n\nThis section of the DataHub documentation is for data publishers. Here you can learn about getting your data ready for loading into DataHub, and how you can interact with your data once it is loaded.\n\n\n\n\nPublishing a Data Package\n\n\nSign up \n get a secret key\n\n\nInstall command line tool\n\n\nConfigure\n\n\nPublish a dataset\n\n\nView it online\n\n\n\n\n\n\n\n\nPublishing a Data Package\n\n\nSign up \n get a secret key\n\n\nYou can sign up using your GitHub account. Once you are signed in, you will be redirected to a dashboard, where you can find your secret key (access token).\n\n\nInstall command line tool\n\n\nNext you need to install \ndpm\n - the data package manager command line tool:\n\n\n$ [sudo] pip install git+https://github.com/frictionlessdata/dpm-py.git\n\n\n\nConfigure\n\n\nYou will need the secret key (access token) to set your configurations:\n\n\n$ dpm configure\n\n\n Username:  \n your user name \n\n\n Your access_token:  \n you secret key \n\n\n Server URL: https://www.datapackaged.com\n\n\n\nNote: server URL may vary depending on application development stage\n\n\nPublish a dataset\n\n\nWe assume you know what a \nData Package\n is.\n\n\nGo to a directory where your data package is located and publish it:\n\n\n$ cd your-data-package-directory/\n$ dpm publish\n\n\n\nView it online\n\n\nOnce your data package is successfully published, you will get an URL to your dataset on the website. Open the URL in your favourite browser and explore it.", 
            "title": "Getting started"
        }, 
        {
            "location": "/publishers/#publishers", 
            "text": "This section of the DataHub documentation is for data publishers. Here you can learn about getting your data ready for loading into DataHub, and how you can interact with your data once it is loaded.   Publishing a Data Package  Sign up   get a secret key  Install command line tool  Configure  Publish a dataset  View it online", 
            "title": "Publishers"
        }, 
        {
            "location": "/publishers/#publishing-a-data-package", 
            "text": "", 
            "title": "Publishing a Data Package"
        }, 
        {
            "location": "/publishers/#sign-up-get-a-secret-key", 
            "text": "You can sign up using your GitHub account. Once you are signed in, you will be redirected to a dashboard, where you can find your secret key (access token).", 
            "title": "Sign up &amp; get a secret key"
        }, 
        {
            "location": "/publishers/#install-command-line-tool", 
            "text": "Next you need to install  dpm  - the data package manager command line tool:  $ [sudo] pip install git+https://github.com/frictionlessdata/dpm-py.git", 
            "title": "Install command line tool"
        }, 
        {
            "location": "/publishers/#configure", 
            "text": "You will need the secret key (access token) to set your configurations:  $ dpm configure  Username:    your user name    Your access_token:    you secret key    Server URL: https://www.datapackaged.com  Note: server URL may vary depending on application development stage", 
            "title": "Configure"
        }, 
        {
            "location": "/publishers/#publish-a-dataset", 
            "text": "We assume you know what a  Data Package  is.  Go to a directory where your data package is located and publish it:  $ cd your-data-package-directory/\n$ dpm publish", 
            "title": "Publish a dataset"
        }, 
        {
            "location": "/publishers/#view-it-online", 
            "text": "Once your data package is successfully published, you will get an URL to your dataset on the website. Open the URL in your favourite browser and explore it.", 
            "title": "View it online"
        }, 
        {
            "location": "/publishers/core-datasets/", 
            "text": "Core Datasets\n\n\nCore Datasets\n - important, commonly used datasets in high quality, easy-to-use \n open form like GDP or ISO-codes. \n\n\nThe main characteristics of core datasets:\n\n\n\n\n\n\nHigh Quality \n Reliable\n - sourcing, normalizing and quality checking a set of \nkey reference and indicator datasets such as country codes, currencies, GDP and population\n\n\n\n\n\n\nStandardized \n Bulk\n - all datasets provided in a \nstandardized\n form and can be accessed in \nbulk as CSV\n together with a simple \nJSON schema\n\n\n\n\n\n\nVersioned \n Packaged\n - all data is in \ndata packages\n and is \nversioned\n using git so all changes are visible and data can be \ncollaboratively maintained\n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n\n\nContribute - Become a Core Data Curator\n\nWe need help suggesting, preparing and maintaining a set of \"core\" datasets as Data Packages. Note that:\n\n\n\n\nWe package data rather than create it \u2013 our focus is to take source data and ensure it is of high quality and in a standard form\n\n\nWe preserve a clean separation between the data source, the data package and this registry \u2013 for example, data packages are stored in git repos hosted separately (preferably github)\n\n\n\n\nBecome a  \nCore Data Curator\n\n\nTake a look at the Core Data Curators guide:", 
            "title": "Core Datasets"
        }, 
        {
            "location": "/publishers/core-datasets/#core-datasets", 
            "text": "Core Datasets  - important, commonly used datasets in high quality, easy-to-use   open form like GDP or ISO-codes.   The main characteristics of core datasets:    High Quality   Reliable  - sourcing, normalizing and quality checking a set of  key reference and indicator datasets such as country codes, currencies, GDP and population    Standardized   Bulk  - all datasets provided in a  standardized  form and can be accessed in  bulk as CSV  together with a simple  JSON schema    Versioned   Packaged  - all data is in  data packages  and is  versioned  using git so all changes are visible and data can be  collaboratively maintained    \n   \n     \n       Contribute - Become a Core Data Curator \nWe need help suggesting, preparing and maintaining a set of \"core\" datasets as Data Packages. Note that:   We package data rather than create it \u2013 our focus is to take source data and ensure it is of high quality and in a standard form  We preserve a clean separation between the data source, the data package and this registry \u2013 for example, data packages are stored in git repos hosted separately (preferably github)   Become a   Core Data Curator  Take a look at the Core Data Curators guide:", 
            "title": "Core Datasets"
        }, 
        {
            "location": "/publishers/core-data-curators/", 
            "text": "Core Data Curators Guide\n\n\nData Curators's job is to collect and maintain important and commonly-used (\u201ccore\u201d) datasets in high-quality, standardized and easy-to-use form - in particular, as up-to-date, well-structured \nData Packages\n.\n\n\nTake a look to the following Core Data Curators guide:\n\n\nQuick Links\n\n\n\n\nDiscussion forum\n - discussion takes place here by default\n\n\nThis is the place to ask questions, get help etc - just open a new topic\n\n\nIntroduction to Core Datasets Project\n\n\nJoin the Team Post\n\n\nPackaging Queue (GitHub Issues Tracker)\n\n\nPublish Data Packages Documentation on Frictionless Data Site\n\n\n\n\nQuick Start\n\n\n\n\nPlease take 2m to introduce yourself in the \ndiscussion forum\n so that other team members can get to know you\n\n\nRead the contributing guide below so you:\n\n\nunderstand the details of the curator workflow\n\n\ncan work out where you'd like to contribute\n\n\nStop: have you read the contributing guide? The next items only make sense if you have!\n\n\n\n\nNow you can dive in with one or both of:\n\n\n\n\n\n\nResearching: start reviewing the \ncurrent queue\n - add new items, comment on existing ones etc\n\n\n\n\nPackaging:  check out the \n\u201cReady to Package\u201d\n section of the queue and assign yourself (drop a comment in the issue claiming it)\n\n\n\n\nContributor Guide\n\n\n\n\n*Fig 1: Overview of the Curation Workflow \n\n\nThere are 2 areas of activity:\n\n\n\n\nPreparing datasets as Core Data Packages - finding them, cleaning them, data-packaging them\n\n\nMaintaining Core Data Packages - keeping them up to date with the source dataset, handling changes, responding to user queries\n\n\n\n\nEach of these has sub-steps which we detail below and you can contribute in any and all of these. [In fact given how many of us there are you will almost end up doing several of these at once!]\n\n\nPreparing Datasets as Core Data Packages\n\n\nThere are different areas where people can contribute:\n\n\n\n\nResearch\n\n\nPackaging up data\n\n\nQuality assurance\n\n\nFinal Publication into the official core datasets list\n\n\n\n\nOften you will contribute in all 4 by taking a dataset all the way from a suggestion to a fully packaged data package published online.\n\n\n1. Research\n\n\nThis involves researching and selecting datasets as core datasets and adding them to the queue for packaging - no coding or data wrangling skill is needed for this\n\n\n\n\nTo propose a dataset for addition you \nopen an issue in the Registry\n with the details of the proposed dataset.\n\n\nIdentify relevant source or sources for the dataset\n\n\nTo propose a dataset you do not have to know where to get the data from (e.g. you could suggest \u201cUS GDP\u201d as a core dataset without yet knowing where to get the data from)\n\n\nDiscuss with Queue Manager(s) (they will spot your submission and start commenting in the GitHub issue)\n\n\nIf good =\n Shortlist for Packaging - add \nLabel \u201cStatus: Ready to Package\u201d\n\n\n\n\n2. Packaging up data\n\n\nOnce we have a suggested dataset marked as \"ready to package\" we can move to packaging it up.\n\n\nHow to package up data is covered in the \ngeneral publishing guide\n.\n\n\n3. Quality Assurance\n\n\nThis involves validating and checking packaged datasets to ensure they are of high quality and ready to publish.\n\n\n\n\nValidate\n the Data Package and \nreview\n the data in the Data Package.\n\n\nIn the review phase, you should be looking at a table with the data you have input before. That will ensure your data package is working without any issues and that it follows the same quality standards that any other package.\n\n\nPost a validation link and a view link in the comments for the issue in the Registry related to your Data Package.\n\n\n\n\n4. Publishing\n\n\nWe have a few extra specific requirements:\n\n\n\n\nAll Data Packages must (ultimately) be stored in a public GitHub repo\n\n\nFirst publish to your own repository\n\n\nThen arrange a move the repository to \ngithub.com/datasets/ organization\n - as the owner of a repository you can initiate a transfer request to github.com/datasets/ which can then be approved\n\n\nAdd to the \ncatalog list\n \nand\n the \ncore list\n \nand\n the associated csv files: \ncatalog-list.csv\n and \ncore-list.csv\n.\n\n\nReload \nhttp://data.okfn.org/data/\n by visiting \nhttp://data.okfn.org/admin/reload/\n\n\nIf you have access, tweet from the @OKFNLabs account a link to the \nhttp://data.okfn.org/data/\n page for the dataset.\n\n\n\n\nMaintaining Data Packages\n\n\nMany data packages package data that changes over time - for example, many time series get updated monthly or daily.\n\n\nWe need people to become the \"maintainer\" for a given dataset and keep it up to date by regularly adding in the new data.\n\n\nList of datasets needing a maintainer\n\n\nCore Data Assessment Criteria\n\n\nFor a dataset to be designated as \"core\" it should meet the following criteria:\n\n\n\n\nQuality - the dataset must be well structured\n\n\nRelevance and importance - the focus at present is on indicators and reference data\n\n\nOngoing support - it should have a maintainer\n\n\nOpenness - data should be \nopen data\n and openly licensed in accordance with the \nOpen Definition\n\n\n\n\n\n\nGuide for Managing Curators\n\n\nIntro Email for New Joiners\n\n\nYou are being added to the Core Data Curators mailing list as you indicated your interest in the project through the online form.\n\n\nThis list is announce-only and will be used rarely. General discussion takes place in the public forum:\n\n\nhttp://discuss.okfn.org/category/open-knowledge-labs/core-datasets\n\n\nGetting Started\n\n\nTo kick-off your core data curatorship we encourage you to:\n\n\n\n\n\n\nIntroduce yourself in forum here: \nhttp://discuss.okfn.org/t/core-data-curators-introductions/145/24\n\n\n\n\n\n\nTake a look at the Core Data Curators guide: \nhttp://docs.datapackaged.com/publishers/core-data-curators", 
            "title": "Core Data Curators"
        }, 
        {
            "location": "/publishers/core-data-curators/#core-data-curators-guide", 
            "text": "Data Curators's job is to collect and maintain important and commonly-used (\u201ccore\u201d) datasets in high-quality, standardized and easy-to-use form - in particular, as up-to-date, well-structured  Data Packages .  Take a look to the following Core Data Curators guide:", 
            "title": "Core Data Curators Guide"
        }, 
        {
            "location": "/publishers/core-data-curators/#quick-links", 
            "text": "Discussion forum  - discussion takes place here by default  This is the place to ask questions, get help etc - just open a new topic  Introduction to Core Datasets Project  Join the Team Post  Packaging Queue (GitHub Issues Tracker)  Publish Data Packages Documentation on Frictionless Data Site", 
            "title": "Quick Links"
        }, 
        {
            "location": "/publishers/core-data-curators/#quick-start", 
            "text": "Please take 2m to introduce yourself in the  discussion forum  so that other team members can get to know you  Read the contributing guide below so you:  understand the details of the curator workflow  can work out where you'd like to contribute  Stop: have you read the contributing guide? The next items only make sense if you have!   Now you can dive in with one or both of:    Researching: start reviewing the  current queue  - add new items, comment on existing ones etc   Packaging:  check out the  \u201cReady to Package\u201d  section of the queue and assign yourself (drop a comment in the issue claiming it)", 
            "title": "Quick Start"
        }, 
        {
            "location": "/publishers/core-data-curators/#contributor-guide", 
            "text": "*Fig 1: Overview of the Curation Workflow   There are 2 areas of activity:   Preparing datasets as Core Data Packages - finding them, cleaning them, data-packaging them  Maintaining Core Data Packages - keeping them up to date with the source dataset, handling changes, responding to user queries   Each of these has sub-steps which we detail below and you can contribute in any and all of these. [In fact given how many of us there are you will almost end up doing several of these at once!]", 
            "title": "Contributor Guide"
        }, 
        {
            "location": "/publishers/core-data-curators/#preparing-datasets-as-core-data-packages", 
            "text": "There are different areas where people can contribute:   Research  Packaging up data  Quality assurance  Final Publication into the official core datasets list   Often you will contribute in all 4 by taking a dataset all the way from a suggestion to a fully packaged data package published online.", 
            "title": "Preparing Datasets as Core Data Packages"
        }, 
        {
            "location": "/publishers/core-data-curators/#1-research", 
            "text": "This involves researching and selecting datasets as core datasets and adding them to the queue for packaging - no coding or data wrangling skill is needed for this   To propose a dataset for addition you  open an issue in the Registry  with the details of the proposed dataset.  Identify relevant source or sources for the dataset  To propose a dataset you do not have to know where to get the data from (e.g. you could suggest \u201cUS GDP\u201d as a core dataset without yet knowing where to get the data from)  Discuss with Queue Manager(s) (they will spot your submission and start commenting in the GitHub issue)  If good =  Shortlist for Packaging - add  Label \u201cStatus: Ready to Package\u201d", 
            "title": "1. Research"
        }, 
        {
            "location": "/publishers/core-data-curators/#2-packaging-up-data", 
            "text": "Once we have a suggested dataset marked as \"ready to package\" we can move to packaging it up.  How to package up data is covered in the  general publishing guide .", 
            "title": "2. Packaging up data"
        }, 
        {
            "location": "/publishers/core-data-curators/#3-quality-assurance", 
            "text": "This involves validating and checking packaged datasets to ensure they are of high quality and ready to publish.   Validate  the Data Package and  review  the data in the Data Package.  In the review phase, you should be looking at a table with the data you have input before. That will ensure your data package is working without any issues and that it follows the same quality standards that any other package.  Post a validation link and a view link in the comments for the issue in the Registry related to your Data Package.", 
            "title": "3. Quality Assurance"
        }, 
        {
            "location": "/publishers/core-data-curators/#4-publishing", 
            "text": "We have a few extra specific requirements:   All Data Packages must (ultimately) be stored in a public GitHub repo  First publish to your own repository  Then arrange a move the repository to  github.com/datasets/ organization  - as the owner of a repository you can initiate a transfer request to github.com/datasets/ which can then be approved  Add to the  catalog list   and  the  core list   and  the associated csv files:  catalog-list.csv  and  core-list.csv .  Reload  http://data.okfn.org/data/  by visiting  http://data.okfn.org/admin/reload/  If you have access, tweet from the @OKFNLabs account a link to the  http://data.okfn.org/data/  page for the dataset.", 
            "title": "4. Publishing"
        }, 
        {
            "location": "/publishers/core-data-curators/#maintaining-data-packages", 
            "text": "Many data packages package data that changes over time - for example, many time series get updated monthly or daily.  We need people to become the \"maintainer\" for a given dataset and keep it up to date by regularly adding in the new data.  List of datasets needing a maintainer", 
            "title": "Maintaining Data Packages"
        }, 
        {
            "location": "/publishers/core-data-curators/#core-data-assessment-criteria", 
            "text": "For a dataset to be designated as \"core\" it should meet the following criteria:   Quality - the dataset must be well structured  Relevance and importance - the focus at present is on indicators and reference data  Ongoing support - it should have a maintainer  Openness - data should be  open data  and openly licensed in accordance with the  Open Definition", 
            "title": "Core Data Assessment Criteria"
        }, 
        {
            "location": "/publishers/core-data-curators/#guide-for-managing-curators", 
            "text": "", 
            "title": "Guide for Managing Curators"
        }, 
        {
            "location": "/publishers/core-data-curators/#intro-email-for-new-joiners", 
            "text": "You are being added to the Core Data Curators mailing list as you indicated your interest in the project through the online form.  This list is announce-only and will be used rarely. General discussion takes place in the public forum:  http://discuss.okfn.org/category/open-knowledge-labs/core-datasets  Getting Started  To kick-off your core data curatorship we encourage you to:    Introduce yourself in forum here:  http://discuss.okfn.org/t/core-data-curators-introductions/145/24    Take a look at the Core Data Curators guide:  http://docs.datapackaged.com/publishers/core-data-curators", 
            "title": "Intro Email for New Joiners"
        }, 
        {
            "location": "/publishers/cli/", 
            "text": "DPM: The Data Package Manager CLI\n\n\n\n\nGetting started\n\n\nInstallation\n\n\nCommands\n\n\nConfiguration\n\n\nUsage\n\n\nPublish\n\n\nTag\n\n\nDelete\n\n\nUndelete\n\n\n\n\n\n\nLinks\n\n\n\n\nGetting started\n\n\nThe dpm is a command-line tool aimed to help publishers to prepare and upload data to the DataHub. With dpm you will be able to:\n\n\n\n\nValidate your data to ensure its quality\n\n\nPublish Data Package\n\n\nTag uploaded Data Package to create historical snapshot\n\n\nRemove uploaded Data Package that is no longer needed\n\n\n\n\nInstallation\n\n\nYou can install unstable version directly from the code repository:\n\n\npip install git+https://github.com/frictionlessdata/dpm-py.git\n\n\n\nCommands\n\n\nYou can see the latest commands and get help by doing:\n\n\ndpm --help\n\n\n\nYou will see output like this:\n\n\nUsage: dpm [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --version      Show the version and exit.\n  --config TEXT  Use custom config file. Default /home/u1/.dpm/config\n  --debug        Show debug messages\n  --help         Show this message and exit.\n\nCommands:\n  configure     Update configuration options.\n  datavalidate  Validate csv file data, given its path.\n  delete        Delete Data Package from the registry server.\n  publish       Publish Data Package to the registry server.\n  purge         Purge Data Package from the registry server.\n  tag           Tag Data Package on the server.\n  undelete      Undelete Data Package from the registry...\n  validate      Validate Data Package in the current dir.\n\n\n\nConfiguration\n\n\nDpm can be configured using \ndpm configure\n command. It will ask you\nto provide username, access_token and server address of DataHub.\n\n\nThe config is stored in \n~/.dpm/config\n, you can edit it with text editor.\nSimple example config file can look like this:\n\n\nusername = myname\naccess_token = mykey\nserver = server URL for publishing Eg: https://www.datapackaged.com\n\n\n\nUsage\n\n\nPublish\n\n\nTo publish a Data Package, go to the Data Package directory (with \ndatapackage.json\n) and\nrun:\n\n\ndpm publish\n\n\n\nIf your configured \nusername\n and \naccess_token\n are correct, dpm will\nupload datapackage.json and all relevant resources to the registry server.\n\n\nTag\n\n\nTo create historical snapshot of your data, you can tag previously uploaded datapackage on the server. Use \ndpm tag\n command:\n\n\ncd datapackage-dir\ndpm tag v1.1\n\nThis will copy the latest version of the Data Package to a separate location in the BitStore. This way you will be able keep a copy of your Data Package at this particular point in time.\n\n\nDelete\n\n\nYou have two choices: delete Data Package completely from the server (\npurge\n) or make the datapackage invisible to everyone except you (\ndelete\n). You can use \ndpm purge\n and \ndpm delete\n accordingly:\n\n\ncd datapackage-dir\ndpm delete\n# or purge it completely\ndpm purge\n\n\n\nUndelete\n\n\nYou can restore your Data Package using \nundelete\n command.\n\n\ndpm undelete\n\nNote that this only works on packages with soft delete (\ndpm delete\n), you can not undelete ones with hard delete (\ndpm purge\n)\n\n\nLinks\n\n\n\n\nCode repo", 
            "title": "CLI"
        }, 
        {
            "location": "/publishers/cli/#dpm-the-data-package-manager-cli", 
            "text": "Getting started  Installation  Commands  Configuration  Usage  Publish  Tag  Delete  Undelete    Links", 
            "title": "DPM: The Data Package Manager CLI"
        }, 
        {
            "location": "/publishers/cli/#getting-started", 
            "text": "The dpm is a command-line tool aimed to help publishers to prepare and upload data to the DataHub. With dpm you will be able to:   Validate your data to ensure its quality  Publish Data Package  Tag uploaded Data Package to create historical snapshot  Remove uploaded Data Package that is no longer needed", 
            "title": "Getting started"
        }, 
        {
            "location": "/publishers/cli/#installation", 
            "text": "You can install unstable version directly from the code repository:  pip install git+https://github.com/frictionlessdata/dpm-py.git", 
            "title": "Installation"
        }, 
        {
            "location": "/publishers/cli/#commands", 
            "text": "You can see the latest commands and get help by doing:  dpm --help  You will see output like this:  Usage: dpm [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --version      Show the version and exit.\n  --config TEXT  Use custom config file. Default /home/u1/.dpm/config\n  --debug        Show debug messages\n  --help         Show this message and exit.\n\nCommands:\n  configure     Update configuration options.\n  datavalidate  Validate csv file data, given its path.\n  delete        Delete Data Package from the registry server.\n  publish       Publish Data Package to the registry server.\n  purge         Purge Data Package from the registry server.\n  tag           Tag Data Package on the server.\n  undelete      Undelete Data Package from the registry...\n  validate      Validate Data Package in the current dir.", 
            "title": "Commands"
        }, 
        {
            "location": "/publishers/cli/#configuration", 
            "text": "Dpm can be configured using  dpm configure  command. It will ask you\nto provide username, access_token and server address of DataHub.  The config is stored in  ~/.dpm/config , you can edit it with text editor.\nSimple example config file can look like this:  username = myname\naccess_token = mykey\nserver = server URL for publishing Eg: https://www.datapackaged.com", 
            "title": "Configuration"
        }, 
        {
            "location": "/publishers/cli/#usage", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/publishers/cli/#publish", 
            "text": "To publish a Data Package, go to the Data Package directory (with  datapackage.json ) and\nrun:  dpm publish  If your configured  username  and  access_token  are correct, dpm will\nupload datapackage.json and all relevant resources to the registry server.", 
            "title": "Publish"
        }, 
        {
            "location": "/publishers/cli/#tag", 
            "text": "To create historical snapshot of your data, you can tag previously uploaded datapackage on the server. Use  dpm tag  command:  cd datapackage-dir\ndpm tag v1.1 \nThis will copy the latest version of the Data Package to a separate location in the BitStore. This way you will be able keep a copy of your Data Package at this particular point in time.", 
            "title": "Tag"
        }, 
        {
            "location": "/publishers/cli/#delete", 
            "text": "You have two choices: delete Data Package completely from the server ( purge ) or make the datapackage invisible to everyone except you ( delete ). You can use  dpm purge  and  dpm delete  accordingly:  cd datapackage-dir\ndpm delete\n# or purge it completely\ndpm purge", 
            "title": "Delete"
        }, 
        {
            "location": "/publishers/cli/#undelete", 
            "text": "You can restore your Data Package using  undelete  command.  dpm undelete \nNote that this only works on packages with soft delete ( dpm delete ), you can not undelete ones with hard delete ( dpm purge )", 
            "title": "Undelete"
        }, 
        {
            "location": "/publishers/cli/#links", 
            "text": "Code repo", 
            "title": "Links"
        }
    ]
}